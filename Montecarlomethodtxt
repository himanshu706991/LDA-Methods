import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.

    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year

# Group by year, business line, and event type
grouped = df.groupby(['Year', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group
for (year, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Show the results
print(result_df)
    Year      Business Line       Event Type  Total Net Loss  \
0   2020  Corporate Banking       Compliance      1178440.66   
1   2020  Corporate Banking            Fraud      1044400.43   
2   2020  Corporate Banking  Physical Damage       720221.69   
3   2020  Corporate Banking   System Failure      1043377.98   
4   2020          Insurance       Compliance      1355431.31   
..   ...                ...              ...             ...   
75  2024     Retail Banking   System Failure      5120518.30   
76  2024  Wealth Management       Compliance      5624138.76   
77  2024  Wealth Management            Fraud      5408901.57   
78  2024  Wealth Management  Physical Damage      5866408.19   
79  2024  Wealth Management   System Failure      5711744.98   

    Total Event Count  VaR Net Loss  VaR Event Count  
0                  44  1.299636e+07           65.000  
1                  43  7.043111e+06           65.000  
2                  25  6.775743e+06           43.000  
3                  40  9.575177e+06           61.001  
4                  51  1.553352e+07           74.000  
..                ...           ...              ...  
75                203  4.254348e+07          246.000  
76                206  6.030275e+07          251.001  
77                217  4.953413e+07          264.001  
78                218  6.108046e+07          263.001  
79                221  5.235720e+07          269.001  

[80 rows x 7 columns]
result_df
Year	Business Line	Event Type	Total Net Loss	Total Event Count	VaR Net Loss	VaR Event Count
0	2020	Corporate Banking	Compliance	1178440.66	44	1.299636e+07	65.000
1	2020	Corporate Banking	Fraud	1044400.43	43	7.043111e+06	65.000
2	2020	Corporate Banking	Physical Damage	720221.69	25	6.775743e+06	43.000
3	2020	Corporate Banking	System Failure	1043377.98	40	9.575177e+06	61.001
4	2020	Insurance	Compliance	1355431.31	51	1.553352e+07	74.000
...	...	...	...	...	...	...	...
75	2024	Retail Banking	System Failure	5120518.30	203	4.254348e+07	246.000
76	2024	Wealth Management	Compliance	5624138.76	206	6.030275e+07	251.001
77	2024	Wealth Management	Fraud	5408901.57	217	4.953413e+07	264.001
78	2024	Wealth Management	Physical Damage	5866408.19	218	6.108046e+07	263.001
79	2024	Wealth Management	System Failure	5711744.98	221	5.235720e+07	269.001
80 rows × 7 columns

# Calculate summary statistics for 'Total Net Loss' and 'VaR Net Loss'
metrics = ['Total Net Loss', 'VaR Net Loss']
outlier_results = []

for metric in metrics:
    # Calculate Q1, Q3, and IQR
    Q1 = result_df[metric].quantile(0.25)
    Q3 = result_df[metric].quantile(0.75)
    IQR = Q3 - Q1
    
    # Define upper and lower bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Flag rows as outliers for the current metric
    result_df[f'{metric} Outlier'] = (result_df[metric] < lower_bound) | (result_df[metric] > upper_bound)
    
    # Append outlier details to the results
    outlier_results.append({
        'Metric': metric,
        'Lower Bound': lower_bound,
        'Upper Bound': upper_bound,
        'Outliers Count': result_df[f'{metric} Outlier'].sum()
    })

# Convert outlier details to a DataFrame for summary
outlier_summary = pd.DataFrame(outlier_results)

# Filter rows that are outliers for any metric
outliers = result_df[result_df['Total Net Loss Outlier'] | result_df['VaR Net Loss Outlier']]

# Display the summary of outliers
print(outlier_summary)

# Display the rows flagged as outliers
print(outliers)
           Metric   Lower Bound   Upper Bound  Outliers Count
0  Total Net Loss  3.602035e+06  8.201126e+06              16
1    VaR Net Loss  2.158199e+07  8.689621e+07              16
    Year      Business Line       Event Type  Total Net Loss  \
0   2020  Corporate Banking       Compliance      1178440.66   
1   2020  Corporate Banking            Fraud      1044400.43   
2   2020  Corporate Banking  Physical Damage       720221.69   
3   2020  Corporate Banking   System Failure      1043377.98   
4   2020          Insurance       Compliance      1355431.31   
5   2020          Insurance            Fraud       839500.25   
6   2020          Insurance  Physical Damage       833369.87   
7   2020          Insurance   System Failure      1164394.40   
8   2020     Retail Banking       Compliance       719252.98   
9   2020     Retail Banking            Fraud      1061984.35   
10  2020     Retail Banking  Physical Damage       732694.73   
11  2020     Retail Banking   System Failure       955736.23   
12  2020  Wealth Management       Compliance       889043.10   
13  2020  Wealth Management            Fraud       822002.36   
14  2020  Wealth Management  Physical Damage       680933.97   
15  2020  Wealth Management   System Failure       942287.36   

    Total Event Count  VaR Net Loss  VaR Event Count  Total Net Loss Outlier  \
0                  44  1.299636e+07           65.000                    True   
1                  43  7.043111e+06           65.000                    True   
2                  25  6.775743e+06           43.000                    True   
3                  40  9.575177e+06           61.001                    True   
4                  51  1.553352e+07           74.000                    True   
5                  30  8.348572e+06           48.001                    True   
6                  33  1.058691e+07           53.001                    True   
7                  41  1.531688e+07           62.000                    True   
8                  36  5.035766e+06           56.000                    True   
9                  42  1.102865e+07           64.000                    True   
10                 33  1.340712e+07           51.000                    True   
11                 38  8.581421e+06           59.000                    True   
12                 37  6.927756e+06           58.000                    True   
13                 32  7.811345e+06           50.000                    True   
14                 27  6.382747e+06           44.000                    True   
15                 38  1.025848e+07           58.000                    True   

    VaR Net Loss Outlier  
0                   True  
1                   True  
2                   True  
3                   True  
4                   True  
5                   True  
6                   True  
7                   True  
8                   True  
9                   True  
10                  True  
11                  True  
12                  True  
13                  True  
14                  True  
15                  True  
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'result_df' is already populated with your data
metrics = ['Total Net Loss', 'VaR Net Loss']
outlier_results = []

# Initialize the plot
plt.figure(figsize=(12, 6))

# Iterate through the metrics to detect outliers and plot them
for metric in metrics:
    # Calculate Q1, Q3, and IQR
    Q1 = result_df[metric].quantile(0.25)
    Q3 = result_df[metric].quantile(0.75)
    IQR = Q3 - Q1
    
    # Define upper and lower bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Flag rows as outliers for the current metric
    result_df[f'{metric} Outlier'] = (result_df[metric] < lower_bound) | (result_df[metric] > upper_bound)
    
    # Append outlier details to the results
    outlier_results.append({
        'Metric': metric,
        'Lower Bound': lower_bound,
        'Upper Bound': upper_bound,
        'Outliers Count': result_df[f'{metric} Outlier'].sum()
    })
    
    # Plotting the data with outliers highlighted
    plt.subplot(1, 2, metrics.index(metric) + 1)
    sns.boxplot(data=result_df, x=metric, color='skyblue')
    sns.scatterplot(data=result_df[result_df[f'{metric} Outlier']], x=metric, 
                    y=[0] * result_df[result_df[f'{metric} Outlier']].shape[0], 
                    color='red', label='Outliers', marker='x')
    plt.title(f'{metric} with Outliers')
    plt.xlabel(metric)

# Convert outlier details to a DataFrame for summary
outlier_summary = pd.DataFrame(outlier_results)

# Filter rows that are outliers for any metric
outliers = result_df[result_df['Total Net Loss Outlier'] | result_df['VaR Net Loss Outlier']]

# Display the summary of outliers
print(outlier_summary)

# Display the rows flagged as outliers
print(outliers)

# Show the plots
plt.tight_layout()
plt.show()
           Metric   Lower Bound   Upper Bound  Outliers Count
0  Total Net Loss  3.602035e+06  8.201126e+06              16
1    VaR Net Loss  2.269857e+07  9.018169e+07              16
    Year      Business Line       Event Type  Total Net Loss  \
0   2020  Corporate Banking       Compliance      1178440.66   
1   2020  Corporate Banking            Fraud      1044400.43   
2   2020  Corporate Banking  Physical Damage       720221.69   
3   2020  Corporate Banking   System Failure      1043377.98   
4   2020          Insurance       Compliance      1355431.31   
5   2020          Insurance            Fraud       839500.25   
6   2020          Insurance  Physical Damage       833369.87   
7   2020          Insurance   System Failure      1164394.40   
8   2020     Retail Banking       Compliance       719252.98   
9   2020     Retail Banking            Fraud      1061984.35   
10  2020     Retail Banking  Physical Damage       732694.73   
11  2020     Retail Banking   System Failure       955736.23   
12  2020  Wealth Management       Compliance       889043.10   
13  2020  Wealth Management            Fraud       822002.36   
14  2020  Wealth Management  Physical Damage       680933.97   
15  2020  Wealth Management   System Failure       942287.36   

    Total Event Count  VaR Net Loss  VaR Event Count  Total Net Loss Outlier  \
0                  44  1.273306e+07           67.000                    True   
1                  43  7.683275e+06           65.000                    True   
2                  25  6.356106e+06           41.000                    True   
3                  40  9.523384e+06           61.000                    True   
4                  51  1.795497e+07           73.000                    True   
5                  30  7.960641e+06           49.000                    True   
6                  33  1.156122e+07           52.000                    True   
7                  41  1.296699e+07           61.000                    True   
8                  36  4.872376e+06           56.000                    True   
9                  42  1.008247e+07           64.000                    True   
10                 33  1.472485e+07           53.000                    True   
11                 38  7.712238e+06           58.000                    True   
12                 37  7.063035e+06           57.000                    True   
13                 32  5.748410e+06           49.001                    True   
14                 27  5.745405e+06           45.000                    True   
15                 38  1.133194e+07           59.001                    True   

    VaR Net Loss Outlier  
0                   True  
1                   True  
2                   True  
3                   True  
4                   True  
5                   True  
6                   True  
7                   True  
8                   True  
9                   True  
10                  True  
11                  True  
12                  True  
13                  True  
14                  True  
15                  True  

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'result_df' is already populated with your data
metrics = ['Total Net Loss', 'VaR Net Loss']
outlier_results = []

# Initialize the plot
plt.figure(figsize=(12, 6))

# Iterate through the metrics to detect outliers and plot them
for metric in metrics:
    # Calculate Q1, Q3, and IQR
    Q1 = result_df[metric].quantile(0.25)
    Q3 = result_df[metric].quantile(0.75)
    IQR = Q3 - Q1
    
    # Define upper and lower bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Flag rows as outliers for the current metric
    result_df[f'{metric} Outlier'] = (result_df[metric] < lower_bound) | (result_df[metric] > upper_bound)
    
    # Append outlier details to the results
    outlier_results.append({
        'Metric': metric,
        'Lower Bound': lower_bound,
        'Upper Bound': upper_bound,
        'Outliers Count': result_df[f'{metric} Outlier'].sum()
    })
    
    # Plotting the data with outliers highlighted
    plt.subplot(1, 2, metrics.index(metric) + 1)
    sns.boxplot(data=result_df, x=metric, color='skyblue')
    
    # Highlight outliers
    sns.scatterplot(data=result_df[result_df[f'{metric} Outlier']], x=metric, 
                    y=[0] * result_df[result_df[f'{metric} Outlier']].shape[0], 
                    color='red', label='Outliers', marker='x')
    
    # Annotate the lower and upper bounds
    plt.axvline(lower_bound, color='orange', linestyle='--', label='Lower Bound')
    plt.axvline(upper_bound, color='orange', linestyle='--', label='Upper Bound')
    
    # Title and labels
    plt.title(f'{metric} with Outliers')
    plt.xlabel(metric)
    plt.legend()

# Convert outlier details to a DataFrame for summary
outlier_summary = pd.DataFrame(outlier_results)

# Filter rows that are outliers for any metric
outliers = result_df[result_df['Total Net Loss Outlier'] | result_df['VaR Net Loss Outlier']]

# Display the summary of outliers
print(outlier_summary)

# Display the rows flagged as outliers
print(outliers)

# Show the plots
plt.tight_layout()
plt.show()
           Metric   Lower Bound   Upper Bound  Outliers Count
0  Total Net Loss  3.602035e+06  8.201126e+06              16
1    VaR Net Loss  2.269857e+07  9.018169e+07              16
    Year      Business Line       Event Type  Total Net Loss  \
0   2020  Corporate Banking       Compliance      1178440.66   
1   2020  Corporate Banking            Fraud      1044400.43   
2   2020  Corporate Banking  Physical Damage       720221.69   
3   2020  Corporate Banking   System Failure      1043377.98   
4   2020          Insurance       Compliance      1355431.31   
5   2020          Insurance            Fraud       839500.25   
6   2020          Insurance  Physical Damage       833369.87   
7   2020          Insurance   System Failure      1164394.40   
8   2020     Retail Banking       Compliance       719252.98   
9   2020     Retail Banking            Fraud      1061984.35   
10  2020     Retail Banking  Physical Damage       732694.73   
11  2020     Retail Banking   System Failure       955736.23   
12  2020  Wealth Management       Compliance       889043.10   
13  2020  Wealth Management            Fraud       822002.36   
14  2020  Wealth Management  Physical Damage       680933.97   
15  2020  Wealth Management   System Failure       942287.36   

    Total Event Count  VaR Net Loss  VaR Event Count  Total Net Loss Outlier  \
0                  44  1.273306e+07           67.000                    True   
1                  43  7.683275e+06           65.000                    True   
2                  25  6.356106e+06           41.000                    True   
3                  40  9.523384e+06           61.000                    True   
4                  51  1.795497e+07           73.000                    True   
5                  30  7.960641e+06           49.000                    True   
6                  33  1.156122e+07           52.000                    True   
7                  41  1.296699e+07           61.000                    True   
8                  36  4.872376e+06           56.000                    True   
9                  42  1.008247e+07           64.000                    True   
10                 33  1.472485e+07           53.000                    True   
11                 38  7.712238e+06           58.000                    True   
12                 37  7.063035e+06           57.000                    True   
13                 32  5.748410e+06           49.001                    True   
14                 27  5.745405e+06           45.000                    True   
15                 38  1.133194e+07           59.001                    True   

    VaR Net Loss Outlier  
0                   True  
1                   True  
2                   True  
3                   True  
4                   True  
5                   True  
6                   True  
7                   True  
8                   True  
9                   True  
10                  True  
11                  True  
12                  True  
13                  True  
14                  True  
15                  True  

outliers
Year	Business Line	Event Type	Total Net Loss	Total Event Count	VaR Net Loss	VaR Event Count	Total Net Loss Outlier	VaR Net Loss Outlier
0	2020	Corporate Banking	Compliance	1178440.66	44	1.299636e+07	65.000	True	True
1	2020	Corporate Banking	Fraud	1044400.43	43	7.043111e+06	65.000	True	True
2	2020	Corporate Banking	Physical Damage	720221.69	25	6.775743e+06	43.000	True	True
3	2020	Corporate Banking	System Failure	1043377.98	40	9.575177e+06	61.001	True	True
4	2020	Insurance	Compliance	1355431.31	51	1.553352e+07	74.000	True	True
5	2020	Insurance	Fraud	839500.25	30	8.348572e+06	48.001	True	True
6	2020	Insurance	Physical Damage	833369.87	33	1.058691e+07	53.001	True	True
7	2020	Insurance	System Failure	1164394.40	41	1.531688e+07	62.000	True	True
8	2020	Retail Banking	Compliance	719252.98	36	5.035766e+06	56.000	True	True
9	2020	Retail Banking	Fraud	1061984.35	42	1.102865e+07	64.000	True	True
10	2020	Retail Banking	Physical Damage	732694.73	33	1.340712e+07	51.000	True	True
11	2020	Retail Banking	System Failure	955736.23	38	8.581421e+06	59.000	True	True
12	2020	Wealth Management	Compliance	889043.10	37	6.927756e+06	58.000	True	True
13	2020	Wealth Management	Fraud	822002.36	32	7.811345e+06	50.000	True	True
14	2020	Wealth Management	Physical Damage	680933.97	27	6.382747e+06	44.000	True	True
15	2020	Wealth Management	System Failure	942287.36	38	1.025848e+07	58.000	True	True
import numpy as np

# Example: Calculate e^VaR_Net_Loss
var_net_loss = 10  # Example VaR Net Loss value

# Calculate exponential of VaR Net Loss
exp_var_net_loss = np.exp(var_net_loss)

print(f"Exponential of VaR Net Loss: {exp_var_net_loss}")
Exponential of VaR Net Loss: 22026.465794806718
# Convert the 'VaR Net Loss' column from scientific notation to regular numbers
result_df['VaR Net Loss Regular'] = result_df['VaR Net Loss'].apply(lambda x: f"{x:.0f}")

# Display the updated DataFrame
print(result_df[['Business Line', 'Event Type', 'Year', 'VaR Net Loss', 'VaR Net Loss Regular']])
        Business Line       Event Type  Year  VaR Net Loss  \
0   Corporate Banking       Compliance  2020  1.299636e+07   
1   Corporate Banking            Fraud  2020  7.043111e+06   
2   Corporate Banking  Physical Damage  2020  6.775743e+06   
3   Corporate Banking   System Failure  2020  9.575177e+06   
4           Insurance       Compliance  2020  1.553352e+07   
..                ...              ...   ...           ...   
75     Retail Banking   System Failure  2024  4.254348e+07   
76  Wealth Management       Compliance  2024  6.030275e+07   
77  Wealth Management            Fraud  2024  4.953413e+07   
78  Wealth Management  Physical Damage  2024  6.108046e+07   
79  Wealth Management   System Failure  2024  5.235720e+07   

   VaR Net Loss Regular  
0              12996363  
1               7043111  
2               6775743  
3               9575177  
4              15533518  
..                  ...  
75             42543481  
76             60302751  
77             49534134  
78             61080456  
79             52357196  

[80 rows x 5 columns]
result_df
Year	Business Line	Event Type	Total Net Loss	Total Event Count	VaR Net Loss	VaR Event Count	Total Net Loss Outlier	VaR Net Loss Outlier	Exp VaR Net Loss	VaR Net Loss Regular
0	2020	Corporate Banking	Compliance	1178440.66	44	1.299636e+07	65.000	True	True	inf	12996363
1	2020	Corporate Banking	Fraud	1044400.43	43	7.043111e+06	65.000	True	True	inf	7043111
2	2020	Corporate Banking	Physical Damage	720221.69	25	6.775743e+06	43.000	True	True	inf	6775743
3	2020	Corporate Banking	System Failure	1043377.98	40	9.575177e+06	61.001	True	True	inf	9575177
4	2020	Insurance	Compliance	1355431.31	51	1.553352e+07	74.000	True	True	inf	15533518
...	...	...	...	...	...	...	...	...	...	...	...
75	2024	Retail Banking	System Failure	5120518.30	203	4.254348e+07	246.000	False	False	inf	42543481
76	2024	Wealth Management	Compliance	5624138.76	206	6.030275e+07	251.001	False	False	inf	60302751
77	2024	Wealth Management	Fraud	5408901.57	217	4.953413e+07	264.001	False	False	inf	49534134
78	2024	Wealth Management	Physical Damage	5866408.19	218	6.108046e+07	263.001	False	False	inf	61080456
79	2024	Wealth Management	System Failure	5711744.98	221	5.235720e+07	269.001	False	False	inf	52357196
80 rows × 11 columns

import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year

# Group by year, business line, and event type
grouped = df.groupby(['Year', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group
for (year, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count for this year
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Predict next year's VaR (for example, 2022) based on historical trends
predicted_results = []

# Loop through the historical data to predict next year's VaR
for (business_line, event_type), group in result_df.groupby(['Business Line', 'Event Type']):
    # For each business line and event type, calculate the mean and standard deviation of the event count and net loss
    mean_event_count = group['Total Event Count'].mean()
    std_event_count = group['Total Event Count'].std()
    mean_net_loss = group['Total Net Loss'].mean()
    std_net_loss = group['Total Net Loss'].std()
    
    # Simulate next year's event count and net loss (assuming same distribution behavior)
    simulated_event_counts = poisson.rvs(mu=mean_event_count, size=10000)
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=10000)
    
    # Predict VaR for next year
    var_net_loss_next_year = np.percentile(simulated_event_counts * simulated_loss_amounts, 99.9)
    var_event_count_next_year = np.percentile(simulated_event_counts, 99.9)
    
    predicted_results.append({
        'Year': 2022,  # Predicting for 2022
        'Business Line': business_line,
        'Event Type': event_type,
        'Predicted VaR Net Loss': var_net_loss_next_year,
        'Predicted VaR Event Count': var_event_count_next_year
    })

# Convert the predicted results to a DataFrame
predicted_df = pd.DataFrame(predicted_results)

# Show the predicted results
print(predicted_df)
    Year      Business Line       Event Type  Predicted VaR Net Loss  \
0   2022  Corporate Banking       Compliance            5.647299e+07   
1   2022  Corporate Banking            Fraud            5.504514e+07   
2   2022  Corporate Banking  Physical Damage            4.837054e+07   
3   2022  Corporate Banking   System Failure            5.650924e+07   
4   2022          Insurance       Compliance            5.141347e+07   
5   2022          Insurance            Fraud            4.918959e+07   
6   2022          Insurance  Physical Damage            4.643252e+07   
7   2022          Insurance   System Failure            4.967677e+07   
8   2022     Retail Banking       Compliance            4.664370e+07   
9   2022     Retail Banking            Fraud            4.382973e+07   
10  2022     Retail Banking  Physical Damage            5.244796e+07   
11  2022     Retail Banking   System Failure            5.357562e+07   
12  2022  Wealth Management       Compliance            6.166539e+07   
13  2022  Wealth Management            Fraud            5.141349e+07   
14  2022  Wealth Management  Physical Damage            4.828907e+07   
15  2022  Wealth Management   System Failure            4.455117e+07   

    Predicted VaR Event Count  
0                     244.001  
1                     244.000  
2                     245.000  
3                     252.002  
4                     238.000  
5                     235.001  
6                     247.000  
7                     246.001  
8                     238.001  
9                     248.000  
10                    247.001  
11                    252.000  
12                    253.000  
13                    241.001  
14                    242.000  
15                    250.000  
predicted_df
Year	Business Line	Event Type	Predicted VaR Net Loss	Predicted VaR Event Count
0	2022	Corporate Banking	Compliance	5.647299e+07	244.001
1	2022	Corporate Banking	Fraud	5.504514e+07	244.000
2	2022	Corporate Banking	Physical Damage	4.837054e+07	245.000
3	2022	Corporate Banking	System Failure	5.650924e+07	252.002
4	2022	Insurance	Compliance	5.141347e+07	238.000
5	2022	Insurance	Fraud	4.918959e+07	235.001
6	2022	Insurance	Physical Damage	4.643252e+07	247.000
7	2022	Insurance	System Failure	4.967677e+07	246.001
8	2022	Retail Banking	Compliance	4.664370e+07	238.001
9	2022	Retail Banking	Fraud	4.382973e+07	248.000
10	2022	Retail Banking	Physical Damage	5.244796e+07	247.001
11	2022	Retail Banking	System Failure	5.357562e+07	252.000
12	2022	Wealth Management	Compliance	6.166539e+07	253.000
13	2022	Wealth Management	Fraud	5.141349e+07	241.001
14	2022	Wealth Management	Physical Damage	4.828907e+07	242.000
15	2022	Wealth Management	System Failure	4.455117e+07	250.000
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year

# Filter the data for years 2021 to 2024 (historical data)
df_historical = df[(df['Year'] >= 2021) & (df['Year'] <= 2024)]

# Group by year, business line, and event type
grouped = df_historical.groupby(['Year', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group and calculate the VaR for historical data (2021-2024)
for (year, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count for this year
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
historical_results_df = pd.DataFrame(results)

# Predict next year's VaR (for example, 2025) based on historical trends (2021-2024)
predicted_results = []

# Loop through the historical data to predict next year's VaR (2025)
for (business_line, event_type), group in historical_results_df.groupby(['Business Line', 'Event Type']):
    # For each business line and event type, calculate the mean and standard deviation of the event count and net loss
    mean_event_count = group['Total Event Count'].mean()
    std_event_count = group['Total Event Count'].std()
    mean_net_loss = group['Total Net Loss'].mean()
    std_net_loss = group['Total Net Loss'].std()
    
    # Simulate next year's event count and net loss (assuming same distribution behavior)
    simulated_event_counts = poisson.rvs(mu=mean_event_count, size=10000)
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=10000)
    
    # Predict VaR for next year (2025)
    var_net_loss_next_year = np.percentile(simulated_event_counts * simulated_loss_amounts, 99.9)
    var_event_count_next_year = np.percentile(simulated_event_counts, 99.9)
    
    predicted_results.append({
        'Year': 2025,  # Predicting for 2025
        'Business Line': business_line,
        'Event Type': event_type,
        'Predicted VaR Net Loss': var_net_loss_next_year,
        'Predicted VaR Event Count': var_event_count_next_year
    })

# Convert the predicted results to a DataFrame
predicted_df_2025 = pd.DataFrame(predicted_results)

# Show the predicted results for 2025
print(predicted_df_2025)
    Year      Business Line       Event Type  Predicted VaR Net Loss  \
0   2025  Corporate Banking       Compliance            5.322507e+07   
1   2025  Corporate Banking            Fraud            5.844785e+07   
2   2025  Corporate Banking  Physical Damage            7.063874e+07   
3   2025  Corporate Banking   System Failure            6.255592e+07   
4   2025          Insurance       Compliance            6.016074e+07   
5   2025          Insurance            Fraud            6.198984e+07   
6   2025          Insurance  Physical Damage            6.248438e+07   
7   2025          Insurance   System Failure            6.955471e+07   
8   2025     Retail Banking       Compliance            5.295911e+07   
9   2025     Retail Banking            Fraud            6.364447e+07   
10  2025     Retail Banking  Physical Damage            7.540085e+07   
11  2025     Retail Banking   System Failure            6.145338e+07   
12  2025  Wealth Management       Compliance            6.220766e+07   
13  2025  Wealth Management            Fraud            6.017784e+07   
14  2025  Wealth Management  Physical Damage            6.896084e+07   
15  2025  Wealth Management   System Failure            6.968656e+07   

    Predicted VaR Event Count  
0                     288.001  
1                     289.000  
2                     296.001  
3                     295.000  
4                     275.000  
5                     280.001  
6                     293.000  
7                     292.001  
8                     280.000  
9                     294.001  
10                    293.000  
11                    294.000  
12                    300.000  
13                    285.002  
14                    288.001  
15                    295.000  
predicted_df_2025
Year	Business Line	Event Type	Predicted VaR Net Loss	Predicted VaR Event Count
0	2025	Corporate Banking	Compliance	5.322507e+07	288.001
1	2025	Corporate Banking	Fraud	5.844785e+07	289.000
2	2025	Corporate Banking	Physical Damage	7.063874e+07	296.001
3	2025	Corporate Banking	System Failure	6.255592e+07	295.000
4	2025	Insurance	Compliance	6.016074e+07	275.000
5	2025	Insurance	Fraud	6.198984e+07	280.001
6	2025	Insurance	Physical Damage	6.248438e+07	293.000
7	2025	Insurance	System Failure	6.955471e+07	292.001
8	2025	Retail Banking	Compliance	5.295911e+07	280.000
9	2025	Retail Banking	Fraud	6.364447e+07	294.001
10	2025	Retail Banking	Physical Damage	7.540085e+07	293.000
11	2025	Retail Banking	System Failure	6.145338e+07	294.000
12	2025	Wealth Management	Compliance	6.220766e+07	300.000
13	2025	Wealth Management	Fraud	6.017784e+07	285.002
14	2025	Wealth Management	Physical Damage	6.896084e+07	288.001
15	2025	Wealth Management	System Failure	6.968656e+07	295.000
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year and quarter
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Filter the data for years 2021-2023 (for fitting the model)
historical_data = df[df['Year'].between(2021, 2023)]

# Group by year, business line, and event type
grouped = historical_data.groupby(['Year', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group and calculate VaR for each
for (year, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Prediction for 2024 Q4: use the same parameters from 2021-2023
future_results = []

for (business_line, event_type), group in historical_data.groupby(['Business Line', 'Event Type']):
    # Estimate parameters for the severity distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Estimate the mean frequency (lambda) from historical data (2021–2023)
    total_event_count = group.shape[0]  # Total number of events for this business line/event type
    frequency_lambda = total_event_count / 3  # Dividing by 3 for three years (2021–2023)
    
    # Simulate for Q4 2024 using the fitted distribution parameters
    var_net_loss = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=frequency_lambda, size=10000), 99.9)
    
    # Append the prediction for 2024 Q4
    future_results.append({
        'Year': 2024,
        'Business Line': business_line,
        'Event Type': event_type,
        'Predicted VaR Net Loss': var_net_loss,
        'Predicted VaR Event Count': var_event_count
    })

# Convert prediction results to DataFrame
future_results_df = pd.DataFrame(future_results)

# Show predictions for 2024 Q4
print(future_results_df)
    Year      Business Line       Event Type  Predicted VaR Net Loss  \
0   2024  Corporate Banking       Compliance            5.948475e+07   
1   2024  Corporate Banking            Fraud            6.463672e+07   
2   2024  Corporate Banking  Physical Damage            5.844187e+07   
3   2024  Corporate Banking   System Failure            6.090070e+07   
4   2024          Insurance       Compliance            6.760091e+07   
5   2024          Insurance            Fraud            5.963561e+07   
6   2024          Insurance  Physical Damage            6.156219e+07   
7   2024          Insurance   System Failure            5.456166e+07   
8   2024     Retail Banking       Compliance            5.372402e+07   
9   2024     Retail Banking            Fraud            5.621072e+07   
10  2024     Retail Banking  Physical Damage            6.585911e+07   
11  2024     Retail Banking   System Failure            5.762123e+07   
12  2024  Wealth Management       Compliance            6.916307e+07   
13  2024  Wealth Management            Fraud            5.353153e+07   
14  2024  Wealth Management  Physical Damage            6.289477e+07   
15  2024  Wealth Management   System Failure            5.941253e+07   

    Predicted VaR Event Count  
0                     293.000  
1                     297.000  
2                     311.001  
3                     309.000  
4                     286.000  
5                     295.001  
6                     303.000  
7                     305.000  
8                     288.001  
9                     303.000  
10                    297.001  
11                    306.000  
12                    319.000  
13                    295.000  
14                    295.001  
15                    305.000  
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year and quarter
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Filter the data for years 2021-2023 (for fitting the model)
historical_data = df[df['Year'].between(2021, 2023)]

# Group by year, business line, and event type
grouped = historical_data.groupby(['Year', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group and calculate VaR for each
for (year, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Prediction for 2024 Q4: use the same parameters from 2021-2023
future_results = []

for (business_line, event_type), group in historical_data.groupby(['Business Line', 'Event Type']):
    # Estimate parameters for the severity distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Estimate the mean frequency (lambda) from historical data (2021–2023)
    total_event_count = group.shape[0]  # Total number of events for this business line/event type
    frequency_lambda = total_event_count / 3  # Dividing by 3 for three years (2021–2023)
    
    # Simulate for Q4 2024 using the fitted distribution parameters
    var_net_loss = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=frequency_lambda, size=10000), 99.9)
    
    # Append the prediction for 2024 Q4 with the quarter information
    future_results.append({
        'Year': 2024,
        'Quarter': 4,  # Setting the quarter to Q4 for prediction
        'Business Line': business_line,
        'Event Type': event_type,
        'Predicted VaR Net Loss': var_net_loss,
        'Predicted VaR Event Count': var_event_count
    })

# Convert prediction results to DataFrame
future_results_df = pd.DataFrame(future_results)

# Show predictions for 2024 Q4
print(future_results_df)
    Year  Quarter      Business Line       Event Type  Predicted VaR Net Loss  \
0   2024        4  Corporate Banking       Compliance            5.295189e+07   
1   2024        4  Corporate Banking            Fraud            5.999021e+07   
2   2024        4  Corporate Banking  Physical Damage            5.847584e+07   
3   2024        4  Corporate Banking   System Failure            5.570793e+07   
4   2024        4          Insurance       Compliance            6.671533e+07   
5   2024        4          Insurance            Fraud            6.336127e+07   
6   2024        4          Insurance  Physical Damage            5.170754e+07   
7   2024        4          Insurance   System Failure            6.230087e+07   
8   2024        4     Retail Banking       Compliance            5.280140e+07   
9   2024        4     Retail Banking            Fraud            6.644406e+07   
10  2024        4     Retail Banking  Physical Damage            6.877179e+07   
11  2024        4     Retail Banking   System Failure            5.642726e+07   
12  2024        4  Wealth Management       Compliance            6.273145e+07   
13  2024        4  Wealth Management            Fraud            5.122476e+07   
14  2024        4  Wealth Management  Physical Damage            5.716008e+07   
15  2024        4  Wealth Management   System Failure            5.831445e+07   

    Predicted VaR Event Count  
0                     292.002  
1                     297.000  
2                     313.000  
3                     313.002  
4                     284.001  
5                     297.000  
6                     301.000  
7                     304.000  
8                     286.001  
9                     303.000  
10                    300.001  
11                    308.000  
12                    318.001  
13                    296.002  
14                    296.000  
15                    305.001  
import pandas as pd
import numpy as np
import uuid

# Define random seed for reproducibility
np.random.seed(42)

# Create a date range from 2021-01-01 to 2024-12-31
date_range = pd.date_range(start='2021-01-01', end='2024-12-31', freq='D')

# Define possible event types and business lines
event_types = ['Fraud', 'System Failure', 'Human Error', 'Compliance', 'External Event']
business_lines = ['Retail Banking', 'Insurance', 'Investment', 'Corporate Banking']

# Generate random data for the dataset
num_records = len(date_range)

# Randomly select values for the columns
data = {
    'Event ID': [uuid.uuid4() for _ in range(num_records)],  # Generate unique UUIDs for each event
    'Date': np.random.choice(date_range, size=num_records),
    'Event Type': np.random.choice(event_types, size=num_records),
    'Business Line': np.random.choice(business_lines, size=num_records),
    'Net Loss Amount': np.random.lognormal(mean=5, sigma=2, size=num_records)  # Log-normal distribution for loss
}

# Create the DataFrame
df = pd.DataFrame(data)

# Show the first few rows of the dataset
print(df.head())

# Save the DataFrame to a CSV file
df.to_csv('synthetic_operational_risk_with_id.csv', index=False)
                               Event ID       Date      Event Type  \
0  587a5aa0-d188-4e6c-9c06-8614e707e83f 2024-02-01  System Failure   
1  d030f4e5-b180-4d2f-a01e-a9207760078b 2024-12-30           Fraud   
2  f97cb11a-6e3a-4b20-aacf-47a6398129a7 2023-05-11  System Failure   
3  c3a6b80c-d508-4524-aa92-ca22bb11a097 2024-07-18  External Event   
4  7835aedf-801e-4c5c-9ee3-510ec9368991 2024-02-05  System Failure   

       Business Line  Net Loss Amount  
0     Retail Banking        54.143630  
1  Corporate Banking       126.084795  
2          Insurance       297.481776  
3          Insurance        56.010707  
4         Investment        38.420218  
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.

    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the synthetic operational risk dataset
df = pd.read_csv('synthetic_operational_risk_with_id.csv')

# Ensure 'Date' is in datetime format and extract the year
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year

# Group by year, business line, and event type
grouped = df.groupby(['Year', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group
for (year, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Show the results
print(result_df)

# Optionally, save the result_df to a CSV file
result_df.to_csv('var_summary_2021_to_2024.csv', index=False)
    Year      Business Line      Event Type  Total Net Loss  \
0   2021  Corporate Banking      Compliance    99309.269625   
1   2021  Corporate Banking  External Event     8011.137068   
2   2021  Corporate Banking           Fraud    11272.509870   
3   2021  Corporate Banking     Human Error     7664.912015   
4   2021  Corporate Banking  System Failure    19193.151939   
..   ...                ...             ...             ...   
75  2024     Retail Banking      Compliance    17089.739489   
76  2024     Retail Banking  External Event    24484.997462   
77  2024     Retail Banking           Fraud    24251.541786   
78  2024     Retail Banking     Human Error    59982.012248   
79  2024     Retail Banking  System Failure    16104.526417   

    Total Event Count  VaR Net Loss  VaR Event Count  
0                  19  1.076998e+07           34.001  
1                   9  5.056306e+05           19.001  
2                  14  4.533106e+05           27.000  
3                  19  7.047646e+05           34.000  
4                  17  2.028314e+06           31.000  
..                ...           ...              ...  
75                 21  1.703874e+06           37.000  
76                 24  2.295159e+06           40.000  
77                 20  1.816202e+06           34.001  
78                 17  1.663521e+06           30.001  
79                 17  7.629048e+05           31.001  

[80 rows x 7 columns]
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Define the calculate_var function
def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.

    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value

# Load the synthetic operational risk dataset
df = pd.read_csv('synthetic_operational_risk_with_id.csv')

# Ensure 'Date' is in datetime format and extract the year and quarter
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Group by year, quarter, business line, and event type
grouped = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type'])

# Initialize a list to hold the results
results = []

# Iterate over each group
for (year, quarter, business_line, event_type), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Calculate VaR for net loss and event count
    var_net_loss = calculate_var(total_event_count, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    var_event_count = np.percentile(poisson.rvs(mu=total_event_count, size=10000), 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Quarter': quarter,
        'Business Line': business_line,
        'Event Type': event_type,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Show the results
print(result_df)

# Optionally, save the result_df to a CSV file
result_df.to_csv('var_summary_with_quarter.csv', index=False)
     Year  Quarter      Business Line      Event Type  Total Net Loss  \
0    2021        1  Corporate Banking      Compliance     6219.883751   
1    2021        1  Corporate Banking  External Event      389.515003   
2    2021        1  Corporate Banking           Fraud    10594.193132   
3    2021        1  Corporate Banking     Human Error      272.612137   
4    2021        1  Corporate Banking  System Failure     1683.231678   
..    ...      ...                ...             ...             ...   
313  2024        4     Retail Banking      Compliance     2082.059343   
314  2024        4     Retail Banking  External Event     9258.630893   
315  2024        4     Retail Banking           Fraud     3337.753017   
316  2024        4     Retail Banking     Human Error    54307.469295   
317  2024        4     Retail Banking  System Failure    13016.364232   

     Total Event Count  VaR Net Loss  VaR Event Count  
0                    8  3.395371e+05             18.0  
1                    2  7.306666e+03              8.0  
2                    3  1.358951e+07             10.0  
3                    2  5.602876e+03              8.0  
4                    7  1.033527e+05             16.0  
..                 ...           ...              ...  
313                  6  7.000318e+04             15.0  
314                  4  1.157800e+05             11.0  
315                  5  9.054192e+04             13.0  
316                  3  1.051956e+08             10.0  
317                  6  9.981127e+05             15.0  

[318 rows x 8 columns]
result_df
Year	Quarter	Business Line	Event Type	Total Net Loss	Total Event Count	VaR Net Loss	VaR Event Count
0	2021	1	Corporate Banking	Compliance	6219.883751	8	3.395371e+05	18.0
1	2021	1	Corporate Banking	External Event	389.515003	2	7.306666e+03	8.0
2	2021	1	Corporate Banking	Fraud	10594.193132	3	1.358951e+07	10.0
3	2021	1	Corporate Banking	Human Error	272.612137	2	5.602876e+03	8.0
4	2021	1	Corporate Banking	System Failure	1683.231678	7	1.033527e+05	16.0
...	...	...	...	...	...	...	...	...
313	2024	4	Retail Banking	Compliance	2082.059343	6	7.000318e+04	15.0
314	2024	4	Retail Banking	External Event	9258.630893	4	1.157800e+05	11.0
315	2024	4	Retail Banking	Fraud	3337.753017	5	9.054192e+04	13.0
316	2024	4	Retail Banking	Human Error	54307.469295	3	1.051956e+08	10.0
317	2024	4	Retail Banking	System Failure	13016.364232	6	9.981127e+05	15.0
318 rows × 8 columns

import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('var_summary_with_quarter.csv')

# Ensure that 'Year' and 'Quarter' are integers
df['Year'] = df['Year'].astype(int)
df['Quarter'] = df['Quarter'].astype(int)

# We need to focus on Business Line-specific predictions, so we'll filter by Business Line
business_lines = df['Business Line'].unique()

# Function to forecast VaR Net Loss and VaR Event Count for each Business Line
def forecast_var(df, business_line, target_column, forecast_period=1):
    # Filter data for the given business line
    data = df[df['Business Line'] == business_line]
    
    # Group by year and quarter to create a time series for each quarter
    data_grouped = data.groupby(['Year', 'Quarter']).agg({target_column: 'mean'}).reset_index()

    # Create a date column for ARIMA (combined Year and Quarter as time index)
    data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))

    # Set 'Date' as the index
    data_grouped.set_index('Date', inplace=True)

    # Train ARIMA model
    model = ARIMA(data_grouped[target_column], order=(1, 1, 1))  # ARIMA(p, d, q)
    model_fit = model.fit()

    # Forecast the next period (Q4 2024)
    forecast = model_fit.forecast(steps=forecast_period)

    return forecast

# Make predictions for Q4 2024 for each Business Line
predictions = []

for business_line in business_lines:
    var_net_loss_forecast = forecast_var(df, business_line, 'VaR Net Loss', forecast_period=1)
    var_event_count_forecast = forecast_var(df, business_line, 'VaR Event Count', forecast_period=1)
    
    predictions.append({
        'Business Line': business_line,
        'Predicted VaR Net Loss Q4 2024': var_net_loss_forecast[0],
        'Predicted VaR Event Count Q4 2024': var_event_count_forecast[0]
    })

# Convert the predictions to a DataFrame
predictions_df = pd.DataFrame(predictions)

# Show predictions for Q4 2024
print(predictions_df)

# Optionally, you can save the predictions to a CSV file
predictions_df.to_csv('predictions_q4_2024_var.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\base\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  warnings.warn("Maximum Likelihood optimization failed to "
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:48: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Net Loss Q4 2024': var_net_loss_forecast[0],
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Event Count Q4 2024': var_event_count_forecast[0]
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.
  warn('Non-stationary starting autoregressive parameters'
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.
  warn('Non-stationary starting autoregressive parameters'
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:48: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Net Loss Q4 2024': var_net_loss_forecast[0],
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Event Count Q4 2024': var_event_count_forecast[0]
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.
  warn('Non-invertible starting MA parameters found.'
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.
  warn('Non-invertible starting MA parameters found.'
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:48: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Net Loss Q4 2024': var_net_loss_forecast[0],
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Event Count Q4 2024': var_event_count_forecast[0]
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
  data_grouped['Date'] = pd.to_datetime(data_grouped['Year'].astype(str) + 'Q' + data_grouped['Quarter'].astype(str))
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QS-OCT will be used.
  self._init_dates(dates, freq)
       Business Line  Predicted VaR Net Loss Q4 2024  \
0  Corporate Banking                    8.364926e+05   
1          Insurance                    9.246781e+05   
2         Investment                    1.436512e+06   
3     Retail Banking                    1.267117e+06   

   Predicted VaR Event Count Q4 2024  
0                          16.133568  
1                           9.979375  
2                          12.166885  
3                          12.768083  
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:48: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Net Loss Q4 2024': var_net_loss_forecast[0],
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_15404\118908729.py:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  'Predicted VaR Event Count Q4 2024': var_event_count_forecast[0]
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Function to calculate VaR using Monte Carlo simulation
def calculate_var_mc(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.
    
    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value, simulated_event_counts, simulated_loss_amounts

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year and quarter
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Filter data for 2021-2024
df_filtered = df[(df['Year'] >= 2021) & (df['Year'] <= 2024)]

# Group by year, quarter, and business line
grouped = df_filtered.groupby(['Year', 'Quarter', 'Business Line'])

# Initialize a list to hold results
results = []

# Iterate over each group (Business Line)
for (year, quarter, business_line), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Estimate lambda for Poisson distribution (average number of events)
    frequency_lambda = total_event_count / len(group['Year'].unique())
    
    # Perform Monte Carlo Simulation to calculate VaR
    var_net_loss, simulated_event_counts, simulated_loss_amounts = calculate_var_mc(
        frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    
    var_event_count = np.percentile(simulated_event_counts, 99.9)  # Poisson VaR for event count

    # Append the results for Q4 2024 prediction
    results.append({
        'Year': year,
        'Quarter': quarter,
        'Business Line': business_line,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Filter the results to show Q4 2024 predictions
q4_2024_predictions = result_df[(result_df['Year'] == 2024) & (result_df['Quarter'] == 4)]

# Show the results for Q4 2024 predictions
print(q4_2024_predictions)

# Optionally, save the results for future reference
q4_2024_predictions.to_csv('q4_2024_predictions_var.csv', index=False)
    Year  Quarter      Business Line  Total Net Loss  Total Event Count  \
60  2024        4  Corporate Banking      2623854.43                 94   
61  2024        4          Insurance      1780907.63                 77   
62  2024        4     Retail Banking      2957827.59                106   
63  2024        4  Wealth Management      2279028.74                 82   

    VaR Net Loss  VaR Event Count  
60  2.536039e+07          125.000  
61  1.844371e+07          105.001  
62  2.901253e+07          137.001  
63  2.253389e+07          110.001  
q4_2024_predictions
Year	Quarter	Business Line	Total Net Loss	Total Event Count	VaR Net Loss	VaR Event Count
60	2024	4	Corporate Banking	2623854.43	94	2.536039e+07	125.000
61	2024	4	Insurance	1780907.63	77	1.844371e+07	105.001
62	2024	4	Retail Banking	2957827.59	106	2.901253e+07	137.001
63	2024	4	Wealth Management	2279028.74	82	2.253389e+07	110.001
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Function to calculate VaR using Monte Carlo simulation
def calculate_var_mc(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.

    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value, simulated_event_counts, simulated_loss_amounts

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year and quarter
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Filter data for 2021-2024
df_filtered = df[(df['Year'] >= 2021) & (df['Year'] <= 2024)]

# Group by year, quarter, and business line
grouped = df_filtered.groupby(['Year', 'Quarter', 'Business Line'])

# Initialize a list to hold results
results = []

# Iterate over each group (Business Line)
for (year, quarter, business_line), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Estimate lambda for Poisson distribution (average number of events)
    frequency_lambda = total_event_count / len(group['Year'].unique())
    
    # Perform Monte Carlo Simulation to calculate VaR
    var_net_loss, simulated_event_counts, simulated_loss_amounts = calculate_var_mc(
        frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    
    var_event_count = np.percentile(simulated_event_counts, 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Quarter': quarter,
        'Business Line': business_line,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Filter the results to show only the data for 2021-2024
historical_data = result_df[(result_df['Year'] >= 2021) & (result_df['Year'] <= 2024)]

# Now predict for Q1 2025 for each Business Line (using the parameters from 2021-2024)
predictions = []

for business_line in historical_data['Business Line'].unique():
    # Filter the data for each business line
    business_line_data = historical_data[historical_data['Business Line'] == business_line]
    
    # Get the mean of VaR Net Loss and VaR Event Count from historical data (2021-2024)
    mean_var_net_loss = business_line_data['VaR Net Loss'].mean()
    mean_var_event_count = business_line_data['VaR Event Count'].mean()
    
    # Use these mean values to predict for Q1 2025 (simple assumption of continuity)
    predictions.append({
        'Year': 2025,
        'Quarter': 1,
        'Business Line': business_line,
        'Predicted VaR Net Loss': mean_var_net_loss,
        'Predicted VaR Event Count': mean_var_event_count
    })

# Convert the predictions to a DataFrame
predicted_q1_2025_df = pd.DataFrame(predictions)

# Show the predicted results for Q1 2025
print(predicted_q1_2025_df)

# Optionally, save the results for future reference
predicted_q1_2025_df.to_csv('q1_2025_predictions_var.csv', index=False)
   Year  Quarter      Business Line  Predicted VaR Net Loss  \
0  2025        1  Corporate Banking            5.986222e+07   
1  2025        1          Insurance            5.954235e+07   
2  2025        1     Retail Banking            5.742031e+07   
3  2025        1  Wealth Management            5.905922e+07   

   Predicted VaR Event Count  
0                 291.312875  
1                 285.125375  
2                 290.063000  
3                 292.188125  
predicted_q1_2025_df
Year	Quarter	Business Line	Predicted VaR Net Loss	Predicted VaR Event Count
0	2025	1	Corporate Banking	5.986222e+07	291.312875
1	2025	1	Insurance	5.954235e+07	285.125375
2	2025	1	Retail Banking	5.742031e+07	290.063000
3	2025	1	Wealth Management	5.905922e+07	292.188125
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Function to calculate VaR using Monte Carlo simulation
def calculate_var_mc(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.

    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value, simulated_event_counts, simulated_loss_amounts

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year and quarter
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Filter data for 2021-2024
df_filtered = df[(df['Year'] >= 2021) & (df['Year'] <= 2024)]

# Group by year, quarter, and business line
grouped = df_filtered.groupby(['Year', 'Quarter', 'Business Line'])

# Initialize a list to hold results
results = []

# Iterate over each group (Business Line)
for (year, quarter, business_line), group in grouped:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Estimate lambda for Poisson distribution (average number of events)
    frequency_lambda = total_event_count / len(group['Year'].unique())
    
    # Perform Monte Carlo Simulation to calculate VaR
    var_net_loss, simulated_event_counts, simulated_loss_amounts = calculate_var_mc(
        frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    
    var_event_count = np.percentile(simulated_event_counts, 99.9)  # Poisson VaR for event count

    # Append the results
    results.append({
        'Year': year,
        'Quarter': quarter,
        'Business Line': business_line,
        'Total Net Loss': total_net_loss,
        'Total Event Count': total_event_count,
        'VaR Net Loss': var_net_loss,
        'VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame
result_df = pd.DataFrame(results)

# Filter the results to show only the data for 2021-2024
historical_data = result_df[(result_df['Year'] >= 2021) & (result_df['Year'] <= 2024)]

# Now predict for Q2, Q3, and Q4 2025 for each Business Line (using the parameters from 2021-2024)
predictions = []

for business_line in historical_data['Business Line'].unique():
    # Filter the data for each business line
    business_line_data = historical_data[historical_data['Business Line'] == business_line]
    
    # Get the mean of VaR Net Loss and VaR Event Count from historical data (2021-2024)
    mean_var_net_loss = business_line_data['VaR Net Loss'].mean()
    mean_var_event_count = business_line_data['VaR Event Count'].mean()
    
    # Predict for Q2, Q3, Q4 2025 (simple assumption of continuity)
    for quarter in range(2, 5):  # For Q2, Q3, and Q4
        predictions.append({
            'Year': 2025,
            'Quarter': quarter,
            'Business Line': business_line,
            'Predicted VaR Net Loss': mean_var_net_loss,
            'Predicted VaR Event Count': mean_var_event_count
        })

# Convert the predictions to a DataFrame
predicted_2025_df = pd.DataFrame(predictions)

# Show the predicted results for Q2, Q3, and Q4 2025
print(predicted_2025_df)

# Optionally, save the results for future reference
predicted_2025_df.to_csv('q2_q3_q4_2025_predictions_var.csv', index=False)
    Year  Quarter      Business Line  Predicted VaR Net Loss  \
0   2025        2  Corporate Banking            5.876780e+07   
1   2025        3  Corporate Banking            5.876780e+07   
2   2025        4  Corporate Banking            5.876780e+07   
3   2025        2          Insurance            6.173071e+07   
4   2025        3          Insurance            6.173071e+07   
5   2025        4          Insurance            6.173071e+07   
6   2025        2     Retail Banking            5.582832e+07   
7   2025        3     Retail Banking            5.582832e+07   
8   2025        4     Retail Banking            5.582832e+07   
9   2025        2  Wealth Management            6.020497e+07   
10  2025        3  Wealth Management            6.020497e+07   
11  2025        4  Wealth Management            6.020497e+07   

    Predicted VaR Event Count  
0                  291.438063  
1                  291.438063  
2                  291.438063  
3                  285.875625  
4                  285.875625  
5                  285.875625  
6                  289.812688  
7                  289.812688  
8                  289.812688  
9                  291.937875  
10                 291.937875  
11                 291.937875  
predicted_2025_df
Year	Quarter	Business Line	Predicted VaR Net Loss	Predicted VaR Event Count
0	2025	2	Corporate Banking	5.876780e+07	291.438063
1	2025	3	Corporate Banking	5.876780e+07	291.438063
2	2025	4	Corporate Banking	5.876780e+07	291.438063
3	2025	2	Insurance	6.173071e+07	285.875625
4	2025	3	Insurance	6.173071e+07	285.875625
5	2025	4	Insurance	6.173071e+07	285.875625
6	2025	2	Retail Banking	5.582832e+07	289.812688
7	2025	3	Retail Banking	5.582832e+07	289.812688
8	2025	4	Retail Banking	5.582832e+07	289.812688
9	2025	2	Wealth Management	6.020497e+07	291.937875
10	2025	3	Wealth Management	6.020497e+07	291.937875
11	2025	4	Wealth Management	6.020497e+07	291.937875
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm

# Function to calculate VaR using Monte Carlo simulation
def calculate_var_mc(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):
    """
    Calculate the Value at Risk (VaR) using Monte Carlo simulation.

    Parameters:
    - frequency_lambda (float): The mean rate of events (λ) for Poisson distribution.
    - severity_shape (float): The shape parameter for log-normal distribution.
    - severity_scale (float): The scale parameter for log-normal distribution.
    - severity_loc (float): The location parameter for log-normal distribution.
    - confidence_level (float): Confidence level for VaR (default is 0.999).
    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).

    Returns:
    - float: The calculated VaR at the specified confidence level.
    """
    # Simulate event counts based on Poisson
    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)
    
    # Simulate severity amounts based on Log-Normal
    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)
    
    # Calculate total loss for each simulation
    total_losses = simulated_event_counts * simulated_loss_amounts
    
    # Calculate the VaR at the specified confidence level
    var_value = np.percentile(total_losses, confidence_level * 100)
    
    return var_value, simulated_event_counts, simulated_loss_amounts

# Load the operational risk dataset
df = pd.read_csv('operational_risk_dataset.csv')

# Ensure 'Date' is in datetime format and extract the year, quarter, and business line
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.quarter

# Filter data for Q4 2024
df_q4_2024 = df[(df['Year'] == 2024) & (df['Quarter'] == 4)]

# Group by business line for Q4 2024
grouped_q4_2024 = df_q4_2024.groupby('Business Line')

# Initialize a list to hold results
q4_2024_results = []

# Iterate over each business line group for Q4 2024
for business_line, group in grouped_q4_2024:
    # Calculate total net loss and event count for the group
    total_net_loss = group['Net Loss Amount'].sum()
    total_event_count = group.shape[0]
    
    # Estimate parameters for the severity (log-normal) distribution
    severity_shape, severity_loc, severity_scale = lognorm.fit(group['Net Loss Amount'], floc=0)
    
    # Estimate lambda for Poisson distribution (average number of events)
    frequency_lambda = total_event_count / len(group['Year'].unique())
    
    # Perform Monte Carlo Simulation to calculate VaR
    var_net_loss, simulated_event_counts, simulated_loss_amounts = calculate_var_mc(
        frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000)
    
    var_event_count = np.percentile(simulated_event_counts, 99.9)  # Poisson VaR for event count

    # Append the results for comparison
    q4_2024_results.append({
        'Year': 2024,
        'Quarter': 4,
        'Business Line': business_line,
        'Actual Net Loss': total_net_loss,
        'Actual Event Count': total_event_count,
        'Predicted VaR Net Loss': var_net_loss,
        'Predicted VaR Event Count': var_event_count
    })

# Convert the results to a DataFrame for easy comparison
q4_2024_comparison_df = pd.DataFrame(q4_2024_results)

# Show the comparison for Q4 2024
print(q4_2024_comparison_df)

# Optionally, save the comparison results to a CSV file
q4_2024_comparison_df.to_csv('q4_2024_comparison_var.csv', index=False)
   Year  Quarter      Business Line  Actual Net Loss  Actual Event Count  \
0  2024        4  Corporate Banking       2623854.43                  94   
1  2024        4          Insurance       1780907.63                  77   
2  2024        4     Retail Banking       2957827.59                 106   
3  2024        4  Wealth Management       2279028.74                  82   

   Predicted VaR Net Loss  Predicted VaR Event Count  
0            2.357513e+07                    125.001  
1            1.770501e+07                    106.000  
2            3.157256e+07                    138.000  
3            1.875255e+07                    111.001  
q4_2024_comparison_df
Year	Quarter	Business Line	Actual Net Loss	Actual Event Count	Predicted VaR Net Loss	Predicted VaR Event Count
0	2024	4	Corporate Banking	2623854.43	94	2.357513e+07	125.001
1	2024	4	Insurance	1780907.63	77	1.770501e+07	106.000
2	2024	4	Retail Banking	2957827.59	106	3.157256e+07	138.000
3	2024	4	Wealth Management	2279028.74	82	1.875255e+07	111.001
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson, lognorm

# Assuming the previous code to calculate Var and get q4_2024_comparison_df

# Bar Plot for Comparison of Actual vs Predicted Net Loss
plt.figure(figsize=(12, 6))
plt.bar(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Actual Net Loss'], width=0.4, label='Actual Net Loss', align='center')
plt.bar(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Predicted VaR Net Loss'], width=0.4, label='Predicted VaR Net Loss', align='edge')
plt.xlabel('Business Line')
plt.ylabel('Amount (in Million)')
plt.title('Q4 2024: Actual vs Predicted Net Loss by Business Line')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()

# Bar Plot for Comparison of Actual vs Predicted Event Count
plt.figure(figsize=(12, 6))
plt.bar(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Actual Event Count'], width=0.4, label='Actual Event Count', align='center')
plt.bar(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Predicted VaR Event Count'], width=0.4, label='Predicted VaR Event Count', align='edge')
plt.xlabel('Business Line')
plt.ylabel('Event Count')
plt.title('Q4 2024: Actual vs Predicted Event Count by Business Line')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming q4_2024_comparison_df already contains the necessary data for Actual vs Predicted

# Line Plot for Comparison of Actual vs Predicted Net Loss
plt.figure(figsize=(12, 6))
plt.plot(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Actual Net Loss'], marker='o', label='Actual Net Loss', color='b', linestyle='-', linewidth=2)
plt.plot(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Predicted VaR Net Loss'], marker='o', label='Predicted VaR Net Loss', color='r', linestyle='--', linewidth=2)
plt.xlabel('Business Line')
plt.ylabel('Amount (in Million)')
plt.title('Q4 2024: Actual vs Predicted Net Loss by Business Line')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()

# Line Plot for Comparison of Actual vs Predicted Event Count
plt.figure(figsize=(12, 6))
plt.plot(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Actual Event Count'], marker='o', label='Actual Event Count', color='g', linestyle='-', linewidth=2)
plt.plot(q4_2024_comparison_df['Business Line'], q4_2024_comparison_df['Predicted VaR Event Count'], marker='o', label='Predicted VaR Event Count', color='orange', linestyle='--', linewidth=2)
plt.xlabel('Business Line')
plt.ylabel('Event Count')
plt.title('Q4 2024: Actual vs Predicted Event Count by Business Line')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()

######
 4o mini

Log in
You said:
What is the use of performing the value at risk in Monte Carlo simulations on historical data of net loss because that historical data what is the point of value at least is already over 
ChatGPT said:
ChatGPT
Performing Value at Risk (VaR) in Monte Carlo simulations on historical data of net loss serves a few important purposes, even though the historical data itself represents past events. Here's why it can be valuable:

1. Stress Testing and Risk Assessment:
Historical Data as a Starting Point: Historical data of net loss offers a foundation to model the future risk of a portfolio or investment. By simulating future scenarios based on this data, you can assess how extreme events might impact the current position. Monte Carlo simulations use this data to generate a range of potential future outcomes that are consistent with observed historical patterns, helping to evaluate potential future risks.
2. Estimation of Potential Future Losses:
Even though historical data is already "over," Monte Carlo simulations allow you to take that data and run numerous hypothetical future scenarios to estimate the probability of losses exceeding a certain threshold (e.g., the VaR). This allows the firm to assess the risk of extreme but plausible events based on how losses have behaved in the past.
3. Incorporating Stochastic Processes and Variability:
Monte Carlo Simulations Consider Randomness: Monte Carlo simulations don't just apply a deterministic model of loss but also incorporate randomness and potential future volatility. While historical data shows past outcomes, it doesn't fully account for the inherent randomness and variability that could occur going forward. Monte Carlo simulations explore this by generating a wide range of potential future outcomes (taking into account the statistical properties of the data).
4. Risk Distribution and Tail Events:
Evaluating Tail Risk: Historical data might reveal the magnitude of net losses that occurred, but it doesn't necessarily highlight the tail risks (extreme losses). Monte Carlo simulations help model the full distribution of potential outcomes and give insights into the probability of extreme losses that may not have occurred in the past but could in the future. This is particularly useful in the context of VaR, which is concerned with the worst-case losses under a given confidence level.
5. Forward-Looking Risk Assessment:
Beyond Historical Experience: While historical data provides useful insights, it's not sufficient for predicting future events, especially in highly volatile or non-stationary environments. Monte Carlo simulations allow you to extrapolate based on past data while introducing variability in the future outcomes. This helps estimate the future risk of a position or portfolio even if extreme events haven’t been observed recently.
6. Scenario Generation for Decision-Making:
Simulating Different Scenarios: The simulation can help in generating multiple potential future scenarios that involve different market conditions, risk factors, or events that may not have been represented in the historical dataset. It provides a broader picture of risk beyond just looking at past occurrences.
In summary, while historical data is retrospective, using it with Monte Carlo simulations allows you to project future risk more comprehensively. It enables you to understand the potential distribution of losses, including the tail risks that may be important for risk management, despite the data being "historical." The purpose is to quantify and assess potential future financial risks based on past loss behavior, thus supporting proactive risk management and decision-making.



You said:
So you mean that whatever the value that I have calculated value at response historical data that is the only advantage that I can use it for future prediction for next quarter or year that is the only thing that is my understanding to calculate the value at this corn historical data is it understanding is correct
ChatGPT said:
ChatGPT
Your understanding is partially correct, but there are a few nuances to clarify.

When you calculate Value at Risk (VaR) using historical data, the advantage is not just to predict specific future values (like the exact loss for the next quarter or year). Rather, the purpose is to understand and measure the potential future risks—how much loss you could face in the future under normal or extreme market conditions.

Here’s a more refined explanation of what you're doing with historical data in VaR calculations:

1. Historical Data as a Reference Point:
You use historical data (like past net losses) to estimate the potential risk of future losses. The idea is that past data reflects how risks have behaved in the past, and using that, you can calculate the distribution of losses that might occur in the future. This doesn't guarantee that future outcomes will be exactly the same, but it gives you a reasonable framework for estimating risk.
2. Monte Carlo Simulations and Historical Data:
Monte Carlo simulations help model the future by using the statistical properties (like the mean, standard deviation, and other distribution characteristics) of your historical data to generate a large number of possible future scenarios. This approach simulates what might happen in the future, given that the conditions in the future might be similar to the past but with added randomness.
3. Risk Forecasting for Future Periods (like Next Quarter or Year):
The goal isn't to predict the exact losses for the next quarter or year, but rather to measure the risk you might face over those periods. By using historical data, you can estimate the potential range of losses you could experience, which helps you plan for the worst-case scenario. This helps with capital allocation, stress testing, and risk management decisions. So, even though the data is from the past, it can help inform your risk strategy for the future.
4. VaR and Time Periods:
VaR is typically calculated over a specific time horizon, such as a day, week, month, quarter, or year. By simulating various future outcomes using Monte Carlo, you are estimating the likelihood of losses exceeding a certain threshold (for example, the 1% worst-case loss) over your chosen time horizon. You can do this for the next quarter or year, even if you only have historical data up until the present.
5. Limitations of Historical Data:
While historical data can give you a good indication of the risk, it does have limitations. Past events don't always predict future ones, especially in volatile or changing market conditions. Monte Carlo simulations attempt to account for this by introducing randomness into the future scenarios and not just relying on a simple historical average or past distribution.
In Summary:
Yes, the historical data is the starting point for your VaR calculation. You use it to estimate the potential range of future risks (like for the next quarter or year).
No, the goal is not to predict the exact loss for the future but to estimate the risk and understand the worst-case scenario you might face, based on historical trends and variability.
So, you're using historical data to help quantify potential future risks, but remember that the value you're calculating (VaR) is a risk estimate, not a concrete prediction of exact future losses.



You said:
But my point is that when in the past already net losses already done then what is the point to calculate the value address even though if I am getting higher value but that is already in the pass that is already done then what is the point of to calculate the value at risk
ChatGPT said:
ChatGPT
I see the distinction you're raising, and I understand where the confusion comes from. You’re asking why calculate Value at Risk (VaR) using past losses when those losses are already incurred—essentially, what value does it add if the losses have already happened?

The core idea behind calculating VaR on historical data, even if the losses are already "done," lies in risk management and proactive decision-making for the future. Let's break it down:

1. VaR is Forward-Looking, Not Retrospective:
VaR is a measure of potential future risk, not a record of past losses. When you calculate VaR using historical data (e.g., past net losses), you’re trying to estimate the future potential loss based on patterns and behaviors observed in the past. It’s not about the losses you’ve already experienced, but about assessing how likely it is to experience similar losses in the future.

Even though the losses have occurred, understanding them through VaR helps you quantify what could happen again in future periods. For example, if your portfolio has seen a significant drawdown in the past, VaR helps estimate how likely it is that you could face similar, or even worse, losses again. It helps to measure risk exposure moving forward.

2. Risk Management and Capital Allocation:
Even though historical losses are "done," knowing the extent of past losses helps you prepare for potential future risks. For instance:
Risk Limits: If you know that a certain level of loss has been experienced historically, VaR can help you set appropriate risk limits for the future. If your portfolio has seen a 10% loss during extreme conditions in the past, you might want to ensure you have enough capital or hedging strategies in place to handle a similar situation in the future.
Capital Reserves: VaR helps determine how much capital you might need to hold to absorb potential future losses. If historical data shows high volatility, the VaR calculation gives you an estimate of how much capital you may need to prevent insolvency or financial distress.
3. Understanding Risk in Changing Conditions:
Historical Data Can Reveal Risk Patterns: Even though past events are finished, their patterns can reveal how risk behaves under different market conditions. For example, if there was a crisis or a sudden market shock, the net losses during that period provide insights into how your portfolio reacted. This is useful to predict how it might behave in similar, albeit not identical, situations in the future.
Stress Testing: By using past losses in simulations, you are essentially stress-testing your portfolio for future events, simulating what could happen under different scenarios. Even though the past losses are "done," the risk model can show you whether a similar market shock could occur again in the future.
4. Monte Carlo Simulations & Variability:
Monte Carlo simulations based on historical data allow you to explore a wide variety of future scenarios, even ones that didn’t happen in the past but are plausible based on the historical data. It's not just about the losses you've already incurred, but about generating a broad distribution of potential future outcomes. These simulations are designed to give you an idea of the worst-case scenarios that might not have happened historically but could potentially happen in the future.
5. Decision Making Based on Past Data:
Setting Risk Limits and Policies: VaR helps firms set risk policies, make decisions about portfolio diversification, and determine appropriate hedging strategies. Even if the loss has already occurred, knowing the VaR helps in making future decisions about how much risk you are willing to take on and how much protection you may need.
In Summary:
While it’s true that historical net losses are "already done," calculating VaR using past data helps you understand the risk profile for the future. It provides insight into potential future losses (even if they didn’t occur in the past) and helps in managing and mitigating risks going forward.

In essence, you're not just calculating what has happened, but preparing for the possibility that similar or worse losses might occur again under certain conditions, and making sure you're ready for them.



You said:
Then why at value at risk with respect to the net loss I am getting very very high value but in actually the net loss amount is less similarly it is happening sometime your actual net losses very high but the value address calculation is low  less amount I need to explain to the business actually
ChatGPT said:
ChatGPT
The discrepancy you're observing, where the Value at Risk (VaR) is much higher than actual net losses in some cases and lower than actual losses in other cases, often arises from the nature of the VaR calculation and the methods used to estimate risk. Let's explore why this happens and how you can explain it to the business:

1. VaR Measures Potential Risk, Not Actual Losses:
VaR is a forward-looking measure of the potential maximum loss over a specified period at a certain confidence level (e.g., 95% or 99%). It represents the worst-case loss within a given confidence interval, but it doesn't necessarily reflect the actual realized loss over a historical period.
For example, if you're calculating VaR for a one-day or one-month horizon at the 99% confidence level, VaR tells you the maximum loss you might expect to see 1% of the time (in the worst 1% of scenarios). However, the actual net loss for that period could be lower, especially if the market conditions don't align with the tail of the loss distribution.
2. High VaR Despite Low Actual Losses:
Historical Data & Tail Risk: VaR accounts for extreme losses, or tail risks, which may not occur frequently but are still possible. If you've observed a period where losses were relatively small but your VaR is still high, it's likely because your model is accounting for rare but extreme scenarios (such as market crashes, major economic shocks, etc.), even though such extreme events didn’t actually happen in the observed period.
If your VaR calculation is based on historical data or Monte Carlo simulations, it could produce a high VaR because it considers the potential for large losses based on rare historical events, even though the actual losses during the period were small. For example, if there was a market crash a few years ago, the VaR calculation might give a high value, reflecting the risk of another crash, even if there was no significant loss recently.
3. Low VaR Despite High Actual Losses:
On the other hand, if you observe high actual losses but your VaR is low, it could be due to VaR not capturing certain risks that were unique to that specific event. For example:
Market Conditions: The specific market conditions or events that led to the high loss might not be adequately represented in the VaR model. If your VaR is based on historical data and a significant, unexpected event happens (such as a geopolitical crisis or an unexpected regulatory change), it could result in losses that were not anticipated by the VaR model.
Low Confidence Interval: VaR is often calculated at a specific confidence level, such as 95% or 99%. If actual losses occurred in the tail (beyond the 99th percentile), the VaR calculation wouldn't capture those losses because VaR only estimates the maximum loss at a certain confidence level, not the worst-case outcome.
Model Assumptions: The VaR calculation might assume normal market conditions and not fully account for extreme volatility, skewed distributions, or non-linear behaviors of risk factors.
4. Key Factors Contributing to the Discrepancy:
Confidence Level Selection: VaR at a 99% confidence level means it calculates the worst loss you'd expect to experience 1% of the time. In practice, actual losses could exceed this 1% threshold, especially if markets behave in unexpected ways (like during a crisis).
Risk Factors Considered: VaR depends on the factors it incorporates into the calculation (e.g., interest rates, asset prices, market volatility). If your model only considers a limited set of factors, it may fail to capture risks that led to high actual losses but were not part of the model's assumptions.
Time Horizon: VaR is calculated over a specific time horizon (e.g., one day, one week, one month). A significant loss over a longer horizon (like a quarter or a year) might be underestimated in a shorter-term VaR calculation.
5. Explaining the Discrepancy to the Business:
When explaining to the business why VaR is high despite low actual losses, or low despite high actual losses, you can emphasize the following points:

VaR is about potential risk, not actual losses: VaR quantifies the maximum loss you'd expect to encounter within a given confidence interval (e.g., the worst 1% of outcomes). It's a risk measure, not a reflection of every scenario.

VaR accounts for extreme, low-probability events: Even though those events haven’t occurred recently, VaR considers the possibility of rare but extreme scenarios. High VaR reflects the risk of such tail events, even if they didn’t happen during the period you're observing.

VaR doesn’t capture everything: It’s a simplified model that doesn’t capture every possible risk (such as black swan events or market shocks that don’t follow historical patterns). Actual losses might occur because of something outside the scope of VaR modeling, such as a one-off event that your VaR calculation doesn’t consider.

Risk Measures are Estimates, Not Certainties: VaR provides an estimate of potential risk based on certain assumptions (confidence level, historical data, etc.), but it cannot predict the exact future losses. Actual losses might be more or less than the VaR estimate, depending on how the market behaves.

Summary:
VaR is an estimate of potential risk based on historical data and statistical models, not a measure of actual loss.
High VaR despite low actual losses occurs because VaR considers extreme, low-probability events, and market conditions might not have triggered those extreme losses.
Low VaR despite high actual losses happens when real-world events fall outside the scope of the VaR model, or the VaR is based on specific assumptions that didn’t align with the actual event.
By communicating these points, you can help the business understand that VaR is a risk management tool designed to assess potential future losses based on probabilities, not a guarantee of actual future outcomes.



You said:
But this is possible like net losses in million and the VR net loss I am getting in billion viceversa for some cade
ChatGPT said:
ChatGPT
Yes, it's certainly possible to encounter a situation where the Value at Risk (VaR) is vastly higher or lower than the actual net losses (e.g., net losses in millions while VaR is in billions, or the opposite), and there are several factors that can explain such discrepancies. Let’s go deeper into why this might happen:

1. VaR Measures a Tail Loss Distribution
VaR calculates the potential loss at a specific confidence level (e.g., 99%) over a set time horizon (e.g., daily, monthly). It represents the worst-case scenario within that confidence level. For example, if you calculate VaR at the 99% confidence level, it tells you the maximum loss you'd expect 1% of the time.

In other words, VaR is designed to estimate the loss that could occur in extreme conditions (the tail of the loss distribution), even if these extreme events are rare or haven’t occurred recently. This could lead to VaR being very high, as it considers catastrophic scenarios that have a very low probability but could result in large losses.

2. Why VaR Can Be Much Higher Than Actual Losses:
Extreme Market Movements Not Captured in Historical Data: If you’ve had a period with relatively mild market movements, but your VaR model includes extreme, but plausible, events from the past (e.g., market crashes or financial crises), it might show a much larger potential loss than what was actually observed in the given period.
Risk Factor Volatility: If you have a portfolio with assets that are highly sensitive to certain risk factors (like exchange rates, interest rates, or commodity prices), VaR might account for extreme movements in these factors. Even though the actual market conditions didn’t trigge
