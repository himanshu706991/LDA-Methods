import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Business Line', 'Event Type']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Business Line', 'Event Type', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Business Line', 'Event Type'], how='left')

# Display the final results
print(final_result)

# Optionally, save results to a CSV
final_result.to_csv('lda_var_results_with_totals_99.9.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: invalid value encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6423: RuntimeWarning: invalid value encountered in log
  lambda x, s: (-np.log(x)**2 / (2 * s**2)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6424: RuntimeWarning: invalid value encountered in log
  - np.log(s * x * np.sqrt(2 * np.pi))),
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: divide by zero encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
     Year      Business Line            Event Type  Total_Loss_Amount  \
0    2020   Asset Management            Compliance         969.926136   
1    2020   Asset Management          Cyber Attack       -8319.899337   
2    2020   Asset Management                 Fraud        4302.214796   
3    2020   Asset Management           Market Risk      -15516.939481   
4    2020   Asset Management      Natural Disaster       11654.835942   
..    ...                ...                   ...                ...   
490  2024  Wealth Management     Operational Error        2172.553384   
491  2024  Wealth Management  Regulatory Violation      -42855.414922   
492  2024  Wealth Management        System Failure       -7810.265618   
493  2024  Wealth Management                 Theft       19192.523010   
494  2024  Wealth Management           Vendor Risk       -8335.665734   

     Event_Count   VaR (99.9%)  
0              2  7.682002e+03  
1              4  1.815066e+21  
2              3  2.617006e+03  
3              3  3.442513e+21  
4              2           NaN  
..           ...           ...  
490           17  1.038305e+17  
491           18  1.393179e+16  
492           22  3.645383e+17  
493           17  7.663892e+20  
494           43  2.528491e+04  

[495 rows x 6 columns]
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\3977692581.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Business Line', 'Event Type']).apply(
final_result
Year	Business Line	Event Type	Total_Loss_Amount	Event_Count	VaR (99.9%)
0	2020	Asset Management	Compliance	969.926136	2	7.682002e+03
1	2020	Asset Management	Cyber Attack	-8319.899337	4	1.815066e+21
2	2020	Asset Management	Fraud	4302.214796	3	2.617006e+03
3	2020	Asset Management	Market Risk	-15516.939481	3	3.442513e+21
4	2020	Asset Management	Natural Disaster	11654.835942	2	NaN
...	...	...	...	...	...	...
490	2024	Wealth Management	Operational Error	2172.553384	17	1.038305e+17
491	2024	Wealth Management	Regulatory Violation	-42855.414922	18	1.393179e+16
492	2024	Wealth Management	System Failure	-7810.265618	22	3.645383e+17
493	2024	Wealth Management	Theft	19192.523010	17	7.663892e+20
494	2024	Wealth Management	Vendor Risk	-8335.665734	43	2.528491e+04
495 rows Ã— 6 columns

import pandas as pd
import numpy as np
from scipy import stats

# Assuming 'df' is the DataFrame already created as provided in the example.

# Step 1: Calculate the average event count per business line and event type from 2020 to 2024
event_count_avg_2020_2024 = df[df['Year'] < 2025].groupby(['Business Line', 'Event Type']).agg(
    Event_Count_Avg=('Unique Event ID', 'count')  # Count the number of Unique Event IDs for each Business Line and Event Type
).reset_index()

# Step 2: Fit a log-normal distribution to the historical losses (negative net loss amounts only)
negative_losses = df[df['Net Loss Amount'] < 0]
shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)

# Step 3: Simulate future losses (for 2025) for each business line using the fitted distribution
n_simulations = 10000  # Simulate 10,000 possible losses for each business line
simulated_losses = {}

for business_line in event_count_avg_2020_2024['Business Line'].unique():
    # Simulate future losses using the fitted log-normal distribution
    simulated_losses[business_line] = stats.lognorm.rvs(s=shape, loc=loc, scale=scale, size=(n_simulations))

# Step 4: Calculate VaR (99.9%) and total predicted losses for 2025
confidence = 0.999  # 99.9% confidence level for VaR
var_results_2025 = {}

for business_line, losses in simulated_losses.items():
    var_99_9 = np.percentile(losses, (1 - confidence) * 100)  # Calculate VaR at 99.9% confidence level
    event_count = event_count_avg_2020_2024[event_count_avg_2020_2024['Business Line'] == business_line]['Event_Count_Avg'].iloc[0]
    total_loss = losses.sum()  # Sum of simulated losses for 2025

    var_results_2025[business_line] = {
        'Year': 2025,
        'Business Line': business_line,
        'Predicted Event Count': event_count,
        'Total Predicted Loss Amount': total_loss,
        'VaR (99.9%)': var_99_9
    }

# Step 5: Convert the results into a DataFrame
var_df_2025 = pd.DataFrame(var_results_2025).T

# Step 6: Display the prediction results for 2025
print(var_df_2025)

# Optionally, save the 2025 results to a CSV file
var_df_2025.to_csv('predicted_var_results_2025.csv', index=False)
                      Year         Business Line Predicted Event Count  \
Asset Management      2025      Asset Management                    96   
Corporate Banking     2025     Corporate Banking                   119   
Credit Card Services  2025  Credit Card Services                    78   
Financial Advisory    2025    Financial Advisory                   108   
Insurance             2025             Insurance                   111   
Investment Banking    2025    Investment Banking                    87   
Mortgage Lending      2025      Mortgage Lending                   115   
Private Banking       2025       Private Banking                    98   
Retail                2025                Retail                   121   
Wealth Management     2025     Wealth Management                   110   

                     Total Predicted Loss Amount  VaR (99.9%)  
Asset Management                 49830631.627872 -3278.274825  
Corporate Banking                49207487.164734 -3154.623919  
Credit Card Services             49245702.225178  -3234.63986  
Financial Advisory               48789534.669639 -3577.220853  
Insurance                        49792812.965663 -2847.876008  
Investment Banking               49219122.922205 -3283.141365  
Mortgage Lending                  48751637.14249 -3052.549329  
Private Banking                  49079752.096169 -3325.929534  
Retail                           49355124.048324 -2921.560566  
Wealth Management                 49356950.38709 -3118.723387  
import pandas as pd
import numpy as np
from scipy import stats

# Assuming 'df' is the DataFrame already created as provided in the example.

# Step 1: Calculate the average event count per business line and event type from 2020 to 2024
event_count_avg_2020_2024 = df[df['Year'] < 2025].groupby(['Business Line', 'Event Type']).agg(
    Event_Count_Avg=('Unique Event ID', 'count')  # Count the number of Unique Event IDs for each Business Line and Event Type
).reset_index()

# Step 2: Fit a log-normal distribution to the historical losses (negative net loss amounts only)
negative_losses = df[df['Net Loss Amount'] < 0]
shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)

# Step 3: Simulate future losses (for 2025) for each business line using the fitted distribution
n_simulations = 10000  # Simulate 10,000 possible losses for each business line
simulated_losses = {}

for business_line in event_count_avg_2020_2024['Business Line'].unique():
    # Simulate future losses using the fitted log-normal distribution
    simulated_losses[business_line] = stats.lognorm.rvs(s=shape, loc=loc, scale=scale, size=(n_simulations))

# Step 4: Calculate VaR (99.9%) and total predicted losses for 2025 Q1 and Q2
confidence = 0.999  # 99.9% confidence level for VaR
var_results_2025_q1_q2 = []

quarters = ['Q1', 'Q2']  # Define the quarters for 2025

for business_line, losses in simulated_losses.items():
    # Get the average event count for the current business line
    event_count = event_count_avg_2020_2024[event_count_avg_2020_2024['Business Line'] == business_line]['Event_Count_Avg'].iloc[0]
    
    # Simulate losses for Q1 and Q2 separately, assuming uniform distribution across quarters
    q1_simulated_losses = losses[:n_simulations // 2]  # First half for Q1
    q2_simulated_losses = losses[n_simulations // 2:]  # Second half for Q2

    # Calculate VaR (99.9%) for Q1 and Q2
    var_q1 = np.percentile(q1_simulated_losses, (1 - confidence) * 100)
    var_q2 = np.percentile(q2_simulated_losses, (1 - confidence) * 100)

    # Calculate total predicted loss amount for Q1 and Q2
    total_loss_q1 = q1_simulated_losses.sum()
    total_loss_q2 = q2_simulated_losses.sum()

    # Append results for both quarters
    var_results_2025_q1_q2.append({
        'Year': 2025,
        'Quarter': 'Q1',
        'Business Line': business_line,
        'Predicted Event Count': event_count,
        'Total Predicted Loss Amount': total_loss_q1,
        'VaR (99.9%)': var_q1
    })
    
    var_results_2025_q1_q2.append({
        'Year': 2025,
        'Quarter': 'Q2',
        'Business Line': business_line,
        'Predicted Event Count': event_count,
        'Total Predicted Loss Amount': total_loss_q2,
        'VaR (99.9%)': var_q2
    })

# Step 5: Convert the results into a DataFrame
var_df_2025_q1_q2 = pd.DataFrame(var_results_2025_q1_q2)

# Step 6: Display the prediction results for 2025 Q1 and Q2
print(var_df_2025_q1_q2)

# Optionally, save the results to a CSV file
var_df_2025_q1_q2.to_csv('predicted_var_results_2025_q1_q2.csv', index=False)
    Year Quarter         Business Line  Predicted Event Count  \
0   2025      Q1      Asset Management                     96   
1   2025      Q2      Asset Management                     96   
2   2025      Q1     Corporate Banking                    119   
3   2025      Q2     Corporate Banking                    119   
4   2025      Q1  Credit Card Services                     78   
5   2025      Q2  Credit Card Services                     78   
6   2025      Q1    Financial Advisory                    108   
7   2025      Q2    Financial Advisory                    108   
8   2025      Q1             Insurance                    111   
9   2025      Q2             Insurance                    111   
10  2025      Q1    Investment Banking                     87   
11  2025      Q2    Investment Banking                     87   
12  2025      Q1      Mortgage Lending                    115   
13  2025      Q2      Mortgage Lending                    115   
14  2025      Q1       Private Banking                     98   
15  2025      Q2       Private Banking                     98   
16  2025      Q1                Retail                    121   
17  2025      Q2                Retail                    121   
18  2025      Q1     Wealth Management                    110   
19  2025      Q2     Wealth Management                    110   

    Total Predicted Loss Amount  VaR (99.9%)  
0                  2.476320e+07 -3498.658616  
1                  2.508496e+07 -3505.790158  
2                  2.458931e+07 -2647.380076  
3                  2.439275e+07 -3265.584940  
4                  2.451737e+07 -2767.412467  
5                  2.466775e+07 -3074.444306  
6                  2.421218e+07 -3096.932404  
7                  2.471678e+07 -3033.374644  
8                  2.428808e+07 -3126.715260  
9                  2.460488e+07 -2500.882284  
10                 2.473775e+07 -3149.350849  
11                 2.471612e+07 -3962.380972  
12                 2.450397e+07 -2840.223349  
13                 2.414675e+07 -2568.759688  
14                 2.473558e+07 -2962.790750  
15                 2.432086e+07 -3485.439787  
16                 2.480335e+07 -3296.111324  
17                 2.468024e+07 -2855.377512  
18                 2.416914e+07 -3729.012209  
19                 2.450441e+07 -3548.697033  
import pandas as pd
import numpy as np
from scipy import stats

# Assuming 'df' is the DataFrame already created as provided in the example.

# Step 1: Calculate the average event count per business line and event type from 2020 to 2024
event_count_avg_2020_2024 = df[df['Year'] < 2025].groupby(['Business Line', 'Event Type']).agg(
    Event_Count_Avg=('Unique Event ID', 'count')  # Count the number of Unique Event IDs for each Business Line and Event Type
).reset_index()

# Step 2: Fit a log-normal distribution to the historical losses (negative net loss amounts only)
negative_losses = df[df['Net Loss Amount'] < 0]
shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)

# Step 3: Simulate future losses (for 2024 Q4 to 2025 Q4) for each business line using the fitted distribution
n_simulations = 10000  # Simulate 10,000 possible losses for each business line
simulated_losses = {}

for business_line in event_count_avg_2020_2024['Business Line'].unique():
    # Simulate future losses using the fitted log-normal distribution
    simulated_losses[business_line] = stats.lognorm.rvs(s=shape, loc=loc, scale=scale, size=(n_simulations))

# Step 4: Calculate VaR (99.9%) and total predicted losses for 2024 Q4 and all quarters of 2025
confidence = 0.999  # 99.9% confidence level for VaR
quarters = ['Q4_2024', 'Q1_2025', 'Q2_2025', 'Q3_2025', 'Q4_2025']  # Define the quarters

var_results_2024_2025 = []

for business_line, losses in simulated_losses.items():
    # Get the average event count for the current business line
    event_count = event_count_avg_2020_2024[event_count_avg_2020_2024['Business Line'] == business_line]['Event_Count_Avg'].iloc[0]
    
    # Simulate losses for each quarter, assuming uniform distribution across quarters
    quarter_simulated_losses = np.array_split(losses, len(quarters))  # Split the simulation into 5 parts (quarters)

    # Calculate VaR (99.9%) for each quarter
    for idx, quarter_losses in enumerate(quarter_simulated_losses):
        var = np.percentile(quarter_losses, (1 - confidence) * 100)
        
        # Calculate total predicted loss amount for the quarter
        total_loss = quarter_losses.sum()

        # Append results for the quarter
        var_results_2024_2025.append({
            'Year': 2024 if idx == 0 else 2025,
            'Quarter': quarters[idx],
            'Business Line': business_line,
            'Predicted Event Count': event_count,
            'Total Predicted Loss Amount': total_loss,
            'VaR (99.9%)': var
        })

# Step 5: Convert the results into a DataFrame
var_df_2024_2025 = pd.DataFrame(var_results_2024_2025)

# Step 6: Display the prediction results for all the quarters (2024 Q4 to 2025 Q4)
print(var_df_2024_2025)

# Optionally, save the results to a CSV file
var_df_2024_2025.to_csv('predicted_var_results_2024_q4_2025.csv', index=False)
    Year  Quarter         Business Line  Predicted Event Count  \
0   2024  Q4_2024      Asset Management                     96   
1   2025  Q1_2025      Asset Management                     96   
2   2025  Q2_2025      Asset Management                     96   
3   2025  Q3_2025      Asset Management                     96   
4   2025  Q4_2025      Asset Management                     96   
5   2024  Q4_2024     Corporate Banking                    119   
6   2025  Q1_2025     Corporate Banking                    119   
7   2025  Q2_2025     Corporate Banking                    119   
8   2025  Q3_2025     Corporate Banking                    119   
9   2025  Q4_2025     Corporate Banking                    119   
10  2024  Q4_2024  Credit Card Services                     78   
11  2025  Q1_2025  Credit Card Services                     78   
12  2025  Q2_2025  Credit Card Services                     78   
13  2025  Q3_2025  Credit Card Services                     78   
14  2025  Q4_2025  Credit Card Services                     78   
15  2024  Q4_2024    Financial Advisory                    108   
16  2025  Q1_2025    Financial Advisory                    108   
17  2025  Q2_2025    Financial Advisory                    108   
18  2025  Q3_2025    Financial Advisory                    108   
19  2025  Q4_2025    Financial Advisory                    108   
20  2024  Q4_2024             Insurance                    111   
21  2025  Q1_2025             Insurance                    111   
22  2025  Q2_2025             Insurance                    111   
23  2025  Q3_2025             Insurance                    111   
24  2025  Q4_2025             Insurance                    111   
25  2024  Q4_2024    Investment Banking                     87   
26  2025  Q1_2025    Investment Banking                     87   
27  2025  Q2_2025    Investment Banking                     87   
28  2025  Q3_2025    Investment Banking                     87   
29  2025  Q4_2025    Investment Banking                     87   
30  2024  Q4_2024      Mortgage Lending                    115   
31  2025  Q1_2025      Mortgage Lending                    115   
32  2025  Q2_2025      Mortgage Lending                    115   
33  2025  Q3_2025      Mortgage Lending                    115   
34  2025  Q4_2025      Mortgage Lending                    115   
35  2024  Q4_2024       Private Banking                     98   
36  2025  Q1_2025       Private Banking                     98   
37  2025  Q2_2025       Private Banking                     98   
38  2025  Q3_2025       Private Banking                     98   
39  2025  Q4_2025       Private Banking                     98   
40  2024  Q4_2024                Retail                    121   
41  2025  Q1_2025                Retail                    121   
42  2025  Q2_2025                Retail                    121   
43  2025  Q3_2025                Retail                    121   
44  2025  Q4_2025                Retail                    121   
45  2024  Q4_2024     Wealth Management                    110   
46  2025  Q1_2025     Wealth Management                    110   
47  2025  Q2_2025     Wealth Management                    110   
48  2025  Q3_2025     Wealth Management                    110   
49  2025  Q4_2025     Wealth Management                    110   

    Total Predicted Loss Amount  VaR (99.9%)  
0                  9.806787e+06 -2966.197168  
1                  9.549439e+06 -2908.761127  
2                  9.864183e+06 -2975.435734  
3                  1.002768e+07 -3041.617555  
4                  9.865946e+06 -2575.675619  
5                  9.671285e+06 -2985.245897  
6                  9.710819e+06 -2949.759614  
7                  1.015287e+07 -2869.897922  
8                  9.713756e+06 -2411.460128  
9                  1.009073e+07 -3350.158923  
10                 9.980706e+06 -3061.832896  
11                 9.702019e+06 -3489.936657  
12                 1.016724e+07 -2318.320386  
13                 9.842224e+06 -3054.777091  
14                 9.869451e+06 -2814.390920  
15                 1.000125e+07 -2849.253933  
16                 1.002692e+07 -2584.343167  
17                 9.705816e+06 -2590.540520  
18                 9.873518e+06 -2351.875332  
19                 9.955819e+06 -2684.878931  
20                 9.762916e+06 -2818.589229  
21                 9.780180e+06 -2551.845075  
22                 9.726269e+06 -3232.872514  
23                 9.608854e+06 -2870.618168  
24                 9.905621e+06 -2214.719850  
25                 9.962136e+06 -3032.196524  
26                 9.740290e+06 -3034.354979  
27                 9.899282e+06 -2941.443795  
28                 9.823958e+06 -3575.367755  
29                 9.984661e+06 -3222.223356  
30                 9.643902e+06 -3464.963232  
31                 9.750518e+06 -2624.409662  
32                 9.896409e+06 -2927.074776  
33                 9.730650e+06 -3058.437858  
34                 9.931135e+06 -2507.240758  
35                 9.926015e+06 -2577.843574  
36                 9.840812e+06 -2958.527809  
37                 9.885305e+06 -2563.839030  
38                 9.763715e+06 -2427.839089  
39                 9.973961e+06 -3014.004929  
40                 1.000437e+07 -4179.352427  
41                 9.866237e+06 -3864.199510  
42                 1.000019e+07 -2662.229591  
43                 9.765219e+06 -3284.310132  
44                 9.775589e+06 -2759.080500  
45                 9.723067e+06 -3854.315471  
46                 9.712092e+06 -3592.529013  
47                 9.797294e+06 -2883.532935  
48                 9.863493e+06 -2730.056335  
49                 9.729901e+06 -4394.929587  
var_df_2024_2025
Year	Quarter	Business Line	Predicted Event Count	Total Predicted Loss Amount	VaR (99.9%)
0	2024	Q4_2024	Asset Management	96	9.806787e+06	-2966.197168
1	2025	Q1_2025	Asset Management	96	9.549439e+06	-2908.761127
2	2025	Q2_2025	Asset Management	96	9.864183e+06	-2975.435734
3	2025	Q3_2025	Asset Management	96	1.002768e+07	-3041.617555
4	2025	Q4_2025	Asset Management	96	9.865946e+06	-2575.675619
5	2024	Q4_2024	Corporate Banking	119	9.671285e+06	-2985.245897
6	2025	Q1_2025	Corporate Banking	119	9.710819e+06	-2949.759614
7	2025	Q2_2025	Corporate Banking	119	1.015287e+07	-2869.897922
8	2025	Q3_2025	Corporate Banking	119	9.713756e+06	-2411.460128
9	2025	Q4_2025	Corporate Banking	119	1.009073e+07	-3350.158923
10	2024	Q4_2024	Credit Card Services	78	9.980706e+06	-3061.832896
11	2025	Q1_2025	Credit Card Services	78	9.702019e+06	-3489.936657
12	2025	Q2_2025	Credit Card Services	78	1.016724e+07	-2318.320386
13	2025	Q3_2025	Credit Card Services	78	9.842224e+06	-3054.777091
14	2025	Q4_2025	Credit Card Services	78	9.869451e+06	-2814.390920
15	2024	Q4_2024	Financial Advisory	108	1.000125e+07	-2849.253933
16	2025	Q1_2025	Financial Advisory	108	1.002692e+07	-2584.343167
17	2025	Q2_2025	Financial Advisory	108	9.705816e+06	-2590.540520
18	2025	Q3_2025	Financial Advisory	108	9.873518e+06	-2351.875332
19	2025	Q4_2025	Financial Advisory	108	9.955819e+06	-2684.878931
20	2024	Q4_2024	Insurance	111	9.762916e+06	-2818.589229
21	2025	Q1_2025	Insurance	111	9.780180e+06	-2551.845075
22	2025	Q2_2025	Insurance	111	9.726269e+06	-3232.872514
23	2025	Q3_2025	Insurance	111	9.608854e+06	-2870.618168
24	2025	Q4_2025	Insurance	111	9.905621e+06	-2214.719850
25	2024	Q4_2024	Investment Banking	87	9.962136e+06	-3032.196524
26	2025	Q1_2025	Investment Banking	87	9.740290e+06	-3034.354979
27	2025	Q2_2025	Investment Banking	87	9.899282e+06	-2941.443795
28	2025	Q3_2025	Investment Banking	87	9.823958e+06	-3575.367755
29	2025	Q4_2025	Investment Banking	87	9.984661e+06	-3222.223356
30	2024	Q4_2024	Mortgage Lending	115	9.643902e+06	-3464.963232
31	2025	Q1_2025	Mortgage Lending	115	9.750518e+06	-2624.409662
32	2025	Q2_2025	Mortgage Lending	115	9.896409e+06	-2927.074776
33	2025	Q3_2025	Mortgage Lending	115	9.730650e+06	-3058.437858
34	2025	Q4_2025	Mortgage Lending	115	9.931135e+06	-2507.240758
35	2024	Q4_2024	Private Banking	98	9.926015e+06	-2577.843574
36	2025	Q1_2025	Private Banking	98	9.840812e+06	-2958.527809
37	2025	Q2_2025	Private Banking	98	9.885305e+06	-2563.839030
38	2025	Q3_2025	Private Banking	98	9.763715e+06	-2427.839089
39	2025	Q4_2025	Private Banking	98	9.973961e+06	-3014.004929
40	2024	Q4_2024	Retail	121	1.000437e+07	-4179.352427
41	2025	Q1_2025	Retail	121	9.866237e+06	-3864.199510
42	2025	Q2_2025	Retail	121	1.000019e+07	-2662.229591
43	2025	Q3_2025	Retail	121	9.765219e+06	-3284.310132
44	2025	Q4_2025	Retail	121	9.775589e+06	-2759.080500
45	2024	Q4_2024	Wealth Management	110	9.723067e+06	-3854.315471
46	2025	Q1_2025	Wealth Management	110	9.712092e+06	-3592.529013
47	2025	Q2_2025	Wealth Management	110	9.797294e+06	-2883.532935
48	2025	Q3_2025	Wealth Management	110	9.863493e+06	-2730.056335
49	2025	Q4_2025	Wealth Management	110	9.729901e+06	-4394.929587
import pandas as pd
import numpy as np
from scipy import stats

# Assuming 'df' is the DataFrame already created as provided in the example.

# Step 1: Calculate the average event count per business line and event type from 2020 to 2024
event_count_avg_2020_2024 = df[df['Year'] < 2025].groupby(['Business Line', 'Event Type']).agg(
    Event_Count_Avg=('Unique Event ID', 'count')  # Count the number of Unique Event IDs for each Business Line and Event Type
).reset_index()

# Step 2: Fit a log-normal distribution to the historical losses (negative net loss amounts only)
negative_losses = df[df['Net Loss Amount'] < 0]
shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)

# Step 3: Simulate future losses (for 2024 Q4 to 2025 Q4) for each business line using the fitted distribution
n_simulations = 10000  # Simulate 10,000 possible losses for each business line
simulated_losses = {}

for business_line in event_count_avg_2020_2024['Business Line'].unique():
    # Simulate future losses using the fitted log-normal distribution
    simulated_losses[business_line] = stats.lognorm.rvs(s=shape, loc=loc, scale=scale, size=(n_simulations))

# Step 4: Calculate VaR (99.9%) and total predicted losses for 2024 Q4 and all quarters of 2025
confidence = 0.999  # 99.9% confidence level for VaR
quarters = ['Q4_2024', 'Q1_2025', 'Q2_2025', 'Q3_2025', 'Q4_2025']  # Define the quarters

var_results_2024_2025 = []

for business_line, losses in simulated_losses.items():
    # Get the average event count for the current business line
    event_count = event_count_avg_2020_2024[event_count_avg_2020_2024['Business Line'] == business_line]['Event_Count_Avg'].iloc[0]
    
    # Simulate losses for each quarter, assuming uniform distribution across quarters
    quarter_simulated_losses = np.array_split(losses, len(quarters))  # Split the simulation into 5 parts (quarters)

    # Calculate VaR (99.9%) for each quarter
    for idx, quarter_losses in enumerate(quarter_simulated_losses):
        var = np.percentile(quarter_losses, (1 - confidence) * 100)
        
        # Calculate total predicted loss amount for the quarter
        total_loss = quarter_losses.sum()

        # Append results for the quarter
        var_results_2024_2025.append({
            'Year': 2024 if idx == 0 else 2025,
            'Quarter': quarters[idx],
            'Business Line': business_line,
            'Predicted Event Count': event_count,
            'Total Predicted Loss Amount': total_loss,
            'VaR (99.9%)': var
        })

# Step 5: Convert the results into a DataFrame
var_df_2024_2025 = pd.DataFrame(var_results_2024_2025)

# Step 6: Sort the results by Year and Quarter
var_df_2024_2025['Quarter_Num'] = var_df_2024_2025['Quarter'].apply(lambda x: int(x.split('_')[1]))
var_df_2024_2025_sorted = var_df_2024_2025.sort_values(by=['Year', 'Quarter_Num']).drop('Quarter_Num', axis=1)

# Step 7: Display the prediction results for all the quarters (2024 Q4 to 2025 Q4)
print(var_df_2024_2025_sorted)

# Optionally, save the results to a CSV file
var_df_2024_2025_sorted.to_csv('predicted_var_results_sorted_2024_q4_2025.csv', index=False)
    Year  Quarter         Business Line  Predicted Event Count  \
0   2024  Q4_2024      Asset Management                     96   
5   2024  Q4_2024     Corporate Banking                    119   
10  2024  Q4_2024  Credit Card Services                     78   
15  2024  Q4_2024    Financial Advisory                    108   
20  2024  Q4_2024             Insurance                    111   
25  2024  Q4_2024    Investment Banking                     87   
30  2024  Q4_2024      Mortgage Lending                    115   
35  2024  Q4_2024       Private Banking                     98   
40  2024  Q4_2024                Retail                    121   
45  2024  Q4_2024     Wealth Management                    110   
1   2025  Q1_2025      Asset Management                     96   
2   2025  Q2_2025      Asset Management                     96   
3   2025  Q3_2025      Asset Management                     96   
4   2025  Q4_2025      Asset Management                     96   
6   2025  Q1_2025     Corporate Banking                    119   
7   2025  Q2_2025     Corporate Banking                    119   
8   2025  Q3_2025     Corporate Banking                    119   
9   2025  Q4_2025     Corporate Banking                    119   
11  2025  Q1_2025  Credit Card Services                     78   
12  2025  Q2_2025  Credit Card Services                     78   
13  2025  Q3_2025  Credit Card Services                     78   
14  2025  Q4_2025  Credit Card Services                     78   
16  2025  Q1_2025    Financial Advisory                    108   
17  2025  Q2_2025    Financial Advisory                    108   
18  2025  Q3_2025    Financial Advisory                    108   
19  2025  Q4_2025    Financial Advisory                    108   
21  2025  Q1_2025             Insurance                    111   
22  2025  Q2_2025             Insurance                    111   
23  2025  Q3_2025             Insurance                    111   
24  2025  Q4_2025             Insurance                    111   
26  2025  Q1_2025    Investment Banking                     87   
27  2025  Q2_2025    Investment Banking                     87   
28  2025  Q3_2025    Investment Banking                     87   
29  2025  Q4_2025    Investment Banking                     87   
31  2025  Q1_2025      Mortgage Lending                    115   
32  2025  Q2_2025      Mortgage Lending                    115   
33  2025  Q3_2025      Mortgage Lending                    115   
34  2025  Q4_2025      Mortgage Lending                    115   
36  2025  Q1_2025       Private Banking                     98   
37  2025  Q2_2025       Private Banking                     98   
38  2025  Q3_2025       Private Banking                     98   
39  2025  Q4_2025       Private Banking                     98   
41  2025  Q1_2025                Retail                    121   
42  2025  Q2_2025                Retail                    121   
43  2025  Q3_2025                Retail                    121   
44  2025  Q4_2025                Retail                    121   
46  2025  Q1_2025     Wealth Management                    110   
47  2025  Q2_2025     Wealth Management                    110   
48  2025  Q3_2025     Wealth Management                    110   
49  2025  Q4_2025     Wealth Management                    110   

    Total Predicted Loss Amount  VaR (99.9%)  
0                  9.968781e+06 -2931.316788  
5                  9.661804e+06 -3161.356329  
10                 9.781407e+06 -2884.712318  
15                 9.891406e+06 -4004.284504  
20                 9.973766e+06 -2492.099361  
25                 9.789269e+06 -2832.483275  
30                 9.823262e+06 -4165.022054  
35                 9.983521e+06 -2231.907435  
40                 9.864185e+06 -3447.531402  
45                 9.705975e+06 -2736.874692  
1                  9.977944e+06 -2302.123849  
2                  9.844992e+06 -2268.235614  
3                  9.750414e+06 -2945.639668  
4                  9.791237e+06 -3073.094606  
6                  9.913954e+06 -3810.507748  
7                  9.976104e+06 -3161.858887  
8                  1.004889e+07 -3549.999913  
9                  9.892165e+06 -2589.866877  
11                 9.935390e+06 -2882.381790  
12                 9.733018e+06 -2693.761311  
13                 1.009328e+07 -2897.856888  
14                 9.899556e+06 -3316.121308  
16                 9.701614e+06 -2845.725547  
17                 9.755883e+06 -2794.522753  
18                 9.723069e+06 -2710.728975  
19                 9.729985e+06 -4370.780868  
21                 9.969451e+06 -2816.156572  
22                 9.877087e+06 -3464.671797  
23                 9.680239e+06 -3286.755073  
24                 9.923737e+06 -2588.733283  
26                 9.725480e+06 -2892.967550  
27                 1.007352e+07 -2231.538906  
28                 9.787649e+06 -2987.112559  
29                 9.682181e+06 -3197.870528  
31                 9.879190e+06 -3971.051759  
32                 9.756155e+06 -2361.296450  
33                 9.656305e+06 -3114.114414  
34                 1.001423e+07 -2881.168686  
36                 9.473306e+06 -3368.836339  
37                 9.910813e+06 -2258.510880  
38                 1.000318e+07 -3103.982485  
39                 9.869266e+06 -2784.829169  
41                 9.761218e+06 -2672.128355  
42                 9.653929e+06 -2695.717707  
43                 9.941633e+06 -2960.898159  
44                 9.713100e+06 -3759.294994  
46                 9.784689e+06 -2819.482920  
47                 9.885638e+06 -2655.965727  
48                 9.768213e+06 -2714.011871  
49                 9.813114e+06 -3068.346768  
##arima
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Example: assume 'final_result' DataFrame has columns 'Year', 'Quarter', 'Business Line', 'Event Count', 'Total Loss Amount', and 'VaR (99.9%)'

# Create a 'Date' column based on Year and Quarter (for time series)
final_result['Date'] = pd.to_datetime(final_result['Year'].astype(str) + '-' + final_result['Quarter'].astype(str) + '-01')

# Group data by Business Line and Quarter for forecasting
grouped = final_result.groupby(['Date', 'Business Line']).agg(
    Total_Event_Count=('Event Count', 'sum'),
    Total_Loss_Amount=('Total Loss Amount', 'sum'),
    Pred_VaR_99_9=('Pred VaR (99.9%)', 'mean')
).reset_index()

# Example: We will use 'Total_Event_Count' for ARIMA forecasting
# Set the time series index for forecasting
ts = grouped.set_index('Date')['Total_Event_Count']

# Check if the time series is stationary (Augmented Dickey-Fuller test)
from statsmodels.tsa.stattools import adfuller
result = adfuller(ts)
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')

# If p-value > 0.05, the series is non-stationary, and we need to make it stationary (e.g., by differencing)

# Apply differencing if needed to make the series stationary
ts_diff = ts.diff().dropna()

# Plot ACF and PACF to find appropriate AR and MA terms
plot_acf(ts_diff)
plot_pacf(ts_diff)
plt.show()

# Fit the ARIMA model (we will use ARIMA(1,1,1) as an example)
model = ARIMA(ts, order=(1, 1, 1))
model_fit = model.fit()

# Print the model summary
print(model_fit.summary())

# Forecast the next 4 quarters (for 2025 Q1, Q2, Q3, Q4)
forecast = model_fit.forecast(steps=4)
forecast_dates = pd.date_range(start=ts.index[-1] + timedelta(days=30), periods=4, freq='Q')

# Create a DataFrame for the forecasted results
forecast_df = pd.DataFrame({
    'Date': forecast_dates,
    'Predicted_Event_Count': forecast
})

# You can merge this with your final result DataFrame to predict for specific Business Lines
# Example of displaying forecasted values
print(forecast_df)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\indexes\base.py:3805, in Index.get_loc(self, key)
   3804 try:
-> 3805     return self._engine.get_loc(casted_key)
   3806 except KeyError as err:

File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()

File index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()

File pandas\\_libs\\hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()

File pandas\\_libs\\hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Quarter'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
Cell In[26], line 13
      8 from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
     10 # Example: assume 'final_result' DataFrame has columns 'Year', 'Quarter', 'Business Line', 'Event Count', 'Total Loss Amount', and 'VaR (99.9%)'
     11 
     12 # Create a 'Date' column based on Year and Quarter (for time series)
---> 13 final_result['Date'] = pd.to_datetime(final_result['Year'].astype(str) + '-' + final_result['Quarter'].astype(str) + '-01')
     15 # Group data by Business Line and Quarter for forecasting
     16 grouped = final_result.groupby(['Date', 'Business Line']).agg(
     17     Total_Event_Count=('Event Count', 'sum'),
     18     Total_Loss_Amount=('Total Loss Amount', 'sum'),
     19     Pred_VaR_99_9=('Pred VaR (99.9%)', 'mean')
     20 ).reset_index()

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\frame.py:4102, in DataFrame.__getitem__(self, key)
   4100 if self.columns.nlevels > 1:
   4101     return self._getitem_multilevel(key)
-> 4102 indexer = self.columns.get_loc(key)
   4103 if is_integer(indexer):
   4104     indexer = [indexer]

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\indexes\base.py:3812, in Index.get_loc(self, key)
   3807     if isinstance(casted_key, slice) or (
   3808         isinstance(casted_key, abc.Iterable)
   3809         and any(isinstance(x, slice) for x in casted_key)
   3810     ):
   3811         raise InvalidIndexError(key)
-> 3812     raise KeyError(key) from err
   3813 except TypeError:
   3814     # If we have a listlike key, _check_indexing_error will raise
   3815     #  InvalidIndexError. Otherwise we fall through and re-raise
   3816     #  the TypeError.
   3817     self._check_indexing_error(key)

KeyError: 'Quarter'
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt

# Set the random seed for reproducibility
np.random.seed(42)

# Assuming 'df' is your DataFrame and contains 'Net Loss Amount' and 'Date'
# Filter for data from Q4 2024 to Q1 2025 (assuming 'Date' is in datetime format)
df['Quarter'] = df['Date'].dt.to_period('Q')

# Filter data for Q4 2024 and Q1 2025
filtered_df = df[(df['Quarter'] >= '2024Q4') & (df['Quarter'] <= '2025Q1')]

# Extract negative losses (for VaR simulation)
negative_losses = filtered_df[filtered_df['Net Loss Amount'] < 0]['Net Loss Amount']

# Monte Carlo simulation parameters
num_simulations = 10000  # Number of simulations
confidence_level = 0.999  # 99.9% confidence level

# Function to run Monte Carlo simulations and calculate VaR
def run_montecarlo_simulation(data, num_simulations=10000, confidence_level=0.999):
    simulated_losses = []
    
    for _ in range(num_simulations):
        sample_losses = np.random.choice(data, size=len(data), replace=True)
        total_loss = np.sum(sample_losses)
        simulated_losses.append(total_loss)
    
    # Convert to numpy array for percentile calculation
    simulated_losses = np.array(simulated_losses)
    
    # Calculate the VaR at the specified confidence level
    VaR = np.percentile(simulated_losses, (1 - confidence_level) * 100)
    return simulated_losses, VaR

# Step 1: Run the Monte Carlo simulation for 2024 Q4 and Q1 2025
simulated_losses, VaR_2024Q4_2025Q1 = run_montecarlo_simulation(negative_losses, num_simulations, confidence_level)

# Step 2: Make predictions for 2025 Q2 to Q4 based on historical simulation
# We will assume that the distribution of losses in Q4 2024 and Q1 2025 holds for the rest of 2025

quarters_2025 = ['2025Q2', '2025Q3', '2025Q4']

# Initialize an empty DataFrame to hold the results
predictions = []

for quarter in quarters_2025:
    # Perform the simulation for each quarter of 2025
    simulated_losses, VaR_value = run_montecarlo_simulation(negative_losses, num_simulations, confidence_level)
    
    # Add the prediction to the results
    predictions.append({
        'Year-Quarter': quarter,
        'Predicted Event Count': len(simulated_losses),  # As a proxy for count
        'Total Predicted Loss Amount': np.sum(simulated_losses),
        'Predicted VaR (99.9%)': VaR_value
    })

# Convert the predictions list into a DataFrame
predictions_df = pd.DataFrame(predictions)

# Display the predictions for 2025
print(predictions_df)

# Optionally, save the predictions to a CSV file
predictions_df.to_csv('predictions_2025_var.csv', index=False)

# Plot the simulated losses for visualization (example for 2025 Q2)
plt.figure(figsize=(10, 6))
plt.hist(simulated_losses, bins=100, edgecolor='black')
plt.axvline(VaR_2024Q4_2025Q1, color='r', linestyle='dashed', linewidth=2, label=f'VaR (99.9%) for Q4 2024 to Q1 2025: ${VaR_2024Q4_2025Q1:,.2f}')
plt.title('Monte Carlo Simulation of Net Losses (2025 Q2 Prediction)')
plt.xlabel('Simulated Total Loss ($)')
plt.ylabel('Frequency')
plt.legend()
plt.show()
  Year-Quarter  Predicted Event Count  Total Predicted Loss Amount  \
0       2025Q2                  10000                -7.709281e+09   
1       2025Q3                  10000                -7.705789e+09   
2       2025Q4                  10000                -7.700807e+09   

   Predicted VaR (99.9%)  
0         -892415.327903  
1         -890137.398345  
2         -884617.057666  

 
import pandas as pd
import numpy as np
from scipy import stats

# Assuming df is already loaded with the relevant columns

# Step 1: Add the Event_Count column by grouping by 'Year' and 'Business Line' (or any other relevant fields)
df['Quarter'] = df['Date'].dt.to_period('Q')
df['Event_Count'] = df.groupby(['Year', 'Business Line', 'Event Type'])['Unique Event ID'].transform('count')

# Step 2: Filter data for Q4 (October - December) of 2024
q4_2024_data = df[df['Quarter'] == '2024Q4']

# Step 3: Fit log-normal distribution for the historical Net Loss Amounts (excluding Q4 2024 data)
historical_losses = df[df['Year'] < 2024]['Net Loss Amount']

# Fit the log-normal distribution to the losses (log-normal fit for severity)
severity_shape, severity_loc, severity_scale = stats.lognorm.fit(historical_losses[historical_losses < 0] * -1)

print(f"Log-normal Distribution Parameters for Severity:")
print(f"Shape: {severity_shape}, Location: {severity_loc}, Scale: {severity_scale}")

# Step 4: Simulate losses for Q4 2024 based on the fitted log-normal distribution
n_simulations = 10000  # Number of Monte Carlo simulations
simulated_loss_amounts = stats.lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)

# Step 5: Forecast event count for Q4 2024 (we assume it to be similar to the average event count for Q4 in past years)
historical_q4_event_counts = df[df['Quarter'].dt.month.isin([10, 11, 12])]['Event_Count']
forecast_event_count_q4_2024 = historical_q4_event_counts.mean()

# Step 6: Simulate event counts for Q4 2024 using Poisson distribution
simulated_event_counts = np.random.poisson(forecast_event_count_q4_2024, n_simulations)

# Step 7: Calculate the total losses for Q4 2024
simulated_total_losses = simulated_event_counts * simulated_loss_amounts

# Step 8: Calculate VaR for Q4 2024 at 99.9% confidence level
var_q4_2024 = np.percentile(simulated_total_losses, 99.9)  # 99.9% quantile for VaR

# Display predicted VaR for Q4 2024
print(f"Predicted VaR for Q4 2024: {var_q4_2024}")
Log-normal Distribution Parameters for Severity:
Shape: 4.2604439243453175e-05, Location: -67108860.8150077, Scale: 67113862.63711381
Predicted VaR for Q4 2024: 386617.1645483713
import pandas as pd
import numpy as np
from scipy import stats

# Assuming df is already loaded with the relevant columns

# Step 1: Add the Event_Count column by grouping by 'Year', 'Business Line', and 'Event Type'
df['Event_Count'] = df.groupby(['Year', 'Business Line', 'Event Type'])['Unique Event ID'].transform('count')

# Step 2: Calculate Total Loss Amount by summing the 'Net Loss Amount' for each group
agg_result = df.groupby(['Year', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Step 3: Fit log-normal distribution to the historical losses (negative losses) for severity calculation
historical_losses = df[df['Year'] < 2024]['Net Loss Amount']
severity_shape, severity_loc, severity_scale = stats.lognorm.fit(historical_losses[historical_losses < 0] * -1)

# Step 4: Calculate VaR for each group (Business Line, Event Type) for the 99.9% confidence level
confidence = 0.999  # Set to 99.9% for VaR

def calculate_var(group, severity_shape, severity_loc, severity_scale, confidence):
    negative_losses = group[group['Net Loss Amount'] < 0]
    if negative_losses.empty:
        return np.nan  # Return NaN if no data for negative losses
    
    # Fit log-normal distribution to negative net loss amounts (severity)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)
    
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    return var

# Step 5: Apply the VaR calculation to each group and add the result to the aggregated DataFrame
agg_result['VaR (99.9%)'] = agg_result.apply(
    lambda x: calculate_var(df[(df['Business Line'] == x['Business Line']) & 
                                (df['Event Type'] == x['Event Type'])], 
                            severity_shape, severity_loc, severity_scale, confidence), axis=1
)

# Display the results
print(agg_result)

# Optionally, save results to a CSV
agg_result.to_csv('lda_var_results_99_9.csv', index=False)
     Year      Business Line            Event Type  Total_Loss_Amount  \
0    2020   Asset Management            Compliance       15924.869542   
1    2020   Asset Management          Cyber Attack       -1139.850460   
2    2020   Asset Management                 Fraud      -18130.537836   
3    2020   Asset Management           Market Risk       -7080.738772   
4    2020   Asset Management      Natural Disaster       -5096.778034   
..    ...                ...                   ...                ...   
486  2024  Wealth Management     Operational Error      -33622.409044   
487  2024  Wealth Management  Regulatory Violation        8040.432316   
488  2024  Wealth Management        System Failure      -11802.255202   
489  2024  Wealth Management                 Theft       34932.355137   
490  2024  Wealth Management           Vendor Risk        2479.497241   

     Event_Count   VaR (99.9%)  
0              7  16365.476390  
1              3  14859.469160  
2              6  14154.764164  
3              2  19314.416272  
4              1  14056.525130  
..           ...           ...  
486           29  13847.949974  
487           24  14526.461347  
488           11  14638.852304  
489           19  14040.489489  
490           18  14111.610245  

[491 rows x 6 columns]
agg_result
Year	Business Line	Event Type	Total_Loss_Amount	Event_Count	VaR (99.9%)
0	2020	Asset Management	Compliance	15924.869542	7	16365.476390
1	2020	Asset Management	Cyber Attack	-1139.850460	3	14859.469160
2	2020	Asset Management	Fraud	-18130.537836	6	14154.764164
3	2020	Asset Management	Market Risk	-7080.738772	2	19314.416272
4	2020	Asset Management	Natural Disaster	-5096.778034	1	14056.525130
...	...	...	...	...	...	...
486	2024	Wealth Management	Operational Error	-33622.409044	29	13847.949974
487	2024	Wealth Management	Regulatory Violation	8040.432316	24	14526.461347
488	2024	Wealth Management	System Failure	-11802.255202	11	14638.852304
489	2024	Wealth Management	Theft	34932.355137	19	14040.489489
490	2024	Wealth Management	Vendor Risk	2479.497241	18	14111.610245
491 rows Ã— 6 columns

import pandas as pd
import numpy as np
from scipy import stats

# Assuming df is already loaded with the relevant columns

# Step 1: Calculate the average event count for Q1 across historical years (e.g., 2020-2024)
q1_data = df[df['Quarter'] == 1]  # Extract Q1 data
q1_event_counts = q1_data.groupby(['Business Line', 'Event Type'])['Unique Event ID'].count()
q1_event_count_avg = q1_event_counts.mean()  # Average count of events for Q1 in historical data

# Step 2: Fit the log-normal distribution to the negative net loss amounts for historical data (loss severity)
historical_losses = df[df['Year'] < 2024]['Net Loss Amount']
severity_shape, severity_loc, severity_scale = stats.lognorm.fit(historical_losses[historical_losses < 0] * -1)

# Step 3: Forecast event counts for Q1 2025 using the historical average
q1_2025_event_counts = q1_event_count_avg  # Assume the same count for Q1 2025

# Step 4: Simulate net loss amounts for Q1 2025 using the log-normal distribution
n_simulations = 10000  # Number of simulations for loss amounts
simulated_loss_amounts = stats.lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)

# Step 5: Calculate the VaR at 99.9% confidence level for Q1 2025
confidence = 0.999  # Set to 99.9% for VaR
simulated_var_99_9 = np.percentile(simulated_loss_amounts, (1 - confidence) * 100)

# Step 6: Combine event count and VaR into the final prediction
predicted_var_q1_2025 = {
    'Year': 2025,
    'Business Line': 'All Business Lines',  # You can adjust this per specific business lines
    'Event Type': 'All Event Types',        # You can adjust this per specific event types
    'Total_Loss_Amount': simulated_loss_amounts.sum(),
    'Event_Count': q1_2025_event_counts,
    'VaR (99.9%)': simulated_var_99_9
}

# Convert the prediction to a DataFrame for easy viewing
predicted_var_df = pd.DataFrame([predicted_var_q1_2025])

# Display the results for Q1 2025 prediction
print(predicted_var_df)
   Year       Business Line       Event Type  Total_Loss_Amount  Event_Count  \
0  2025  All Business Lines  All Event Types       5.034317e+07          NaN   

   VaR (99.9%)  
0 -3784.312823  
import pandas as pd
import numpy as np
from scipy import stats

# Assume `df` is the DataFrame created above with necessary data.

# Step 1: Calculate the average event count for each business line and event type
event_count_avg_2020_2024 = df[df['Year'] < 2025].groupby(['Business Line', 'Event Type']).agg(
    Event_Count_Avg=('Event_Count', 'mean')
).reset_index()

# Step 2: Fit a log-normal distribution to historical loss data (negative values only)
negative_losses = df[df['Net Loss Amount'] < 0]
shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)

# Step 3: Forecast the net loss amounts for 2025 for each business line
n_simulations = 10000  # Simulating 10,000 possible losses for each business line
simulated_losses = {}
for business_line in event_count_avg_2020_2024['Business Line'].unique():
    # Filter data for the business line
    business_line_data = df[df['Business Line'] == business_line]
    # Simulate future losses using the fitted log-normal distribution
    simulated_losses[business_line] = stats.lognorm.rvs(s=shape, loc=loc, scale=scale, size=(n_simulations))

# Step 4: Calculate VaR (99.9% confidence) for each business line in 2025
confidence = 0.999  # 99.9% confidence level for VaR
var_results_2025 = {}

for business_line, losses in simulated_losses.items():
    var_99_9 = np.percentile(losses, (1 - confidence) * 100)
    event_count = event_count_avg_2020_2024[event_count_avg_2020_2024['Business Line'] == business_line]['Event_Count_Avg'].iloc[0]
    total_loss = losses.sum()  # Total loss simulation for 2025

    var_results_2025[business_line] = {
        'Year': 2025,
        'Business Line': business_line,
        'Predicted Event Count': event_count,
        'Total Predicted Loss Amount': total_loss,
        'VaR (99.9%)': var_99_9
    }

# Step 5: Convert the results into a DataFrame
var_df_2025 = pd.DataFrame(var_results_2025).T

# Step 6: Display the prediction results for 2025
print(var_df_2025)

# Optionally, save the 2025 results to a CSV file
var_df_2025.to_csv('predicted_var_results_2025.csv', index=False)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[16], line 8
      3 from scipy import stats
      5 # Assume `df` is the DataFrame created above with necessary data.
      6 
      7 # Step 1: Calculate the average event count for each business line and event type
----> 8 event_count_avg_2020_2024 = df[df['Year'] < 2025].groupby(['Business Line', 'Event Type']).agg(
      9     Event_Count_Avg=('Event_Count', 'mean')
     10 ).reset_index()
     12 # Step 2: Fit a log-normal distribution to historical loss data (negative values only)
     13 negative_losses = df[df['Net Loss Amount'] < 0]

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:1432, in DataFrameGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
   1429     kwargs["engine_kwargs"] = engine_kwargs
   1431 op = GroupByApply(self, func, args=args, kwargs=kwargs)
-> 1432 result = op.agg()
   1433 if not is_dict_like(func) and result is not None:
   1434     # GH #52849
   1435     if not self.as_index and is_list_like(func):

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:190, in Apply.agg(self)
    187     return self.apply_str()
    189 if is_dict_like(func):
--> 190     return self.agg_dict_like()
    191 elif is_list_like(func):
    192     # we require a list, but not a 'str'
    193     return self.agg_list_like()

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:423, in Apply.agg_dict_like(self)
    415 def agg_dict_like(self) -> DataFrame | Series:
    416     """
    417     Compute aggregation in the case of a dict-like argument.
    418 
   (...)
    421     Result of aggregation.
    422     """
--> 423     return self.agg_or_apply_dict_like(op_name="agg")

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:1608, in GroupByApply.agg_or_apply_dict_like(self, op_name)
   1603     kwargs.update({"engine": engine, "engine_kwargs": engine_kwargs})
   1605 with com.temp_setattr(
   1606     obj, "as_index", True, condition=hasattr(obj, "as_index")
   1607 ):
-> 1608     result_index, result_data = self.compute_dict_like(
   1609         op_name, selected_obj, selection, kwargs
   1610     )
   1611 result = self.wrap_results_dict_like(selected_obj, result_index, result_data)
   1612 return result

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:462, in Apply.compute_dict_like(self, op_name, selected_obj, selection, kwargs)
    460 is_groupby = isinstance(obj, (DataFrameGroupBy, SeriesGroupBy))
    461 func = cast(AggFuncTypeDict, self.func)
--> 462 func = self.normalize_dictlike_arg(op_name, selected_obj, func)
    464 is_non_unique_col = (
    465     selected_obj.ndim == 2
    466     and selected_obj.columns.nunique() < len(selected_obj.columns)
    467 )
    469 if selected_obj.ndim == 1:
    470     # key only used for output

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:663, in Apply.normalize_dictlike_arg(self, how, obj, func)
    661     cols = Index(list(func.keys())).difference(obj.columns, sort=True)
    662     if len(cols) > 0:
--> 663         raise KeyError(f"Column(s) {list(cols)} do not exist")
    665 aggregator_types = (list, tuple, dict)
    667 # if we have a dict of any non-scalars
    668 # eg. {'A' : ['mean']}, normalize all to
    669 # be list-likes
    670 # Cannot use func.values() because arg may be a Series

KeyError: "Column(s) ['Event_Count'] do not exist"
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year and Business Line to calculate aggregated statistics and VaR
result = df.groupby(['Year', 'Business Line']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Business Line', 'VaR Net Loss (99.9%)']

# Aggregate data to calculate total net loss and event count
agg_result = df.groupby(['Year', 'Business Line']).agg(
    Total_Net_Loss=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Business Line'], how='left')

# Display the final results
print(final_result)

# Optionally, save results to a CSV
final_result.to_csv('lda_var_results_with_totals_99.9.csv', index=False)
    Year         Business Line  Total_Net_Loss  Event_Count  \
0   2020      Asset Management    28016.241627           28   
1   2020     Corporate Banking   -52313.392566           26   
2   2020  Credit Card Services    51996.021542           32   
3   2020    Financial Advisory   -28678.975147           30   
4   2020             Insurance   -20660.414919           28   
5   2020    Investment Banking    16649.772531           28   
6   2020      Mortgage Lending   -43246.734847           38   
7   2020       Private Banking    27672.903467           34   
8   2020                Retail   -19262.776811           26   
9   2020     Wealth Management    14044.501825           30   
10  2021      Asset Management   -49293.898290          236   
11  2021     Corporate Banking    27676.234802          267   
12  2021  Credit Card Services    71807.350845          263   
13  2021    Financial Advisory  -177748.173811          206   
14  2021             Insurance       84.133321          241   
15  2021    Investment Banking    61961.380330          221   
16  2021      Mortgage Lending   -64710.858406          257   
17  2021       Private Banking  -117517.491332          241   
18  2021                Retail   133008.512303          257   
19  2021     Wealth Management   -72188.619116          265   
20  2022      Asset Management   144867.011713          268   
21  2022     Corporate Banking  -103532.390639          237   
22  2022  Credit Card Services      -11.881135          245   
23  2022    Financial Advisory   101389.568347          247   
24  2022             Insurance    24783.160867          231   
25  2022    Investment Banking      249.343528          283   
26  2022      Mortgage Lending   -31515.831519          233   
27  2022       Private Banking   -84624.948213          238   
28  2022                Retail   108104.220351          293   
29  2022     Wealth Management   136038.334780          248   
30  2023      Asset Management     4003.044231          237   
31  2023     Corporate Banking   -43867.628788          278   
32  2023  Credit Card Services   231429.569005          256   
33  2023    Financial Advisory     1379.972046          245   
34  2023             Insurance   -67062.728680          235   
35  2023    Investment Banking   -11559.602029          257   
36  2023      Mortgage Lending   -62309.416786          223   
37  2023       Private Banking  -122154.116543          260   
38  2023                Retail    40540.769309          248   
39  2023     Wealth Management   -91383.814653          258   
40  2024      Asset Management   -58041.229004          211   
41  2024     Corporate Banking    38215.560873          233   
42  2024  Credit Card Services    58911.278557          276   
43  2024    Financial Advisory   185682.726978          217   
44  2024             Insurance    82516.684761          216   
45  2024    Investment Banking    65759.603460          204   
46  2024      Mortgage Lending   186733.475199          202   
47  2024       Private Banking    45195.851332          240   
48  2024                Retail    53774.234072          218   
49  2024     Wealth Management    20074.269211          209   

    VaR Net Loss (99.9%)  
0           3.891560e+15  
1           1.309960e+04  
2           8.858167e+15  
3           1.326179e+04  
4           1.754848e+14  
5           3.470408e+16  
6           1.494525e+04  
7           2.213233e+04  
8           3.461029e+15  
9           8.253696e+15  
10          1.556046e+04  
11          2.598054e+04  
12          1.426694e+04  
13          1.348671e+04  
14          1.559960e+04  
15          1.472303e+04  
16          1.399159e+04  
17          1.352531e+04  
18          1.401290e+04  
19          1.399899e+04  
20          1.426123e+04  
21          1.887409e+04  
22          1.360603e+04  
23          1.403118e+04  
24          1.749626e+04  
25          1.516821e+04  
26          1.468132e+04  
27          1.390112e+04  
28          1.491655e+04  
29          2.798216e+04  
30          1.613085e+04  
31          1.402248e+04  
32          1.773050e+04  
33          1.441243e+04  
34          1.446189e+04  
35          1.420892e+04  
36          1.406553e+04  
37          1.559071e+04  
38          1.568344e+04  
39          1.407796e+04  
40          1.335648e+04  
41          1.562396e+04  
42          1.428698e+04  
43          1.470406e+04  
44          2.167852e+04  
45          1.489201e+04  
46          2.042332e+04  
47          1.338886e+04  
48          1.392258e+04  
49          1.538710e+04  
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\3195073567.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Business Line']).apply(
final_result
Year	Business Line	Total_Net_Loss	Event_Count	VaR Net Loss (99.9%)
0	2020	Asset Management	28016.241627	28	3.891560e+15
1	2020	Corporate Banking	-52313.392566	26	1.309960e+04
2	2020	Credit Card Services	51996.021542	32	8.858167e+15
3	2020	Financial Advisory	-28678.975147	30	1.326179e+04
4	2020	Insurance	-20660.414919	28	1.754848e+14
5	2020	Investment Banking	16649.772531	28	3.470408e+16
6	2020	Mortgage Lending	-43246.734847	38	1.494525e+04
7	2020	Private Banking	27672.903467	34	2.213233e+04
8	2020	Retail	-19262.776811	26	3.461029e+15
9	2020	Wealth Management	14044.501825	30	8.253696e+15
10	2021	Asset Management	-49293.898290	236	1.556046e+04
11	2021	Corporate Banking	27676.234802	267	2.598054e+04
12	2021	Credit Card Services	71807.350845	263	1.426694e+04
13	2021	Financial Advisory	-177748.173811	206	1.348671e+04
14	2021	Insurance	84.133321	241	1.559960e+04
15	2021	Investment Banking	61961.380330	221	1.472303e+04
16	2021	Mortgage Lending	-64710.858406	257	1.399159e+04
17	2021	Private Banking	-117517.491332	241	1.352531e+04
18	2021	Retail	133008.512303	257	1.401290e+04
19	2021	Wealth Management	-72188.619116	265	1.399899e+04
20	2022	Asset Management	144867.011713	268	1.426123e+04
21	2022	Corporate Banking	-103532.390639	237	1.887409e+04
22	2022	Credit Card Services	-11.881135	245	1.360603e+04
23	2022	Financial Advisory	101389.568347	247	1.403118e+04
24	2022	Insurance	24783.160867	231	1.749626e+04
25	2022	Investment Banking	249.343528	283	1.516821e+04
26	2022	Mortgage Lending	-31515.831519	233	1.468132e+04
27	2022	Private Banking	-84624.948213	238	1.390112e+04
28	2022	Retail	108104.220351	293	1.491655e+04
29	2022	Wealth Management	136038.334780	248	2.798216e+04
30	2023	Asset Management	4003.044231	237	1.613085e+04
31	2023	Corporate Banking	-43867.628788	278	1.402248e+04
32	2023	Credit Card Services	231429.569005	256	1.773050e+04
33	2023	Financial Advisory	1379.972046	245	1.441243e+04
34	2023	Insurance	-67062.728680	235	1.446189e+04
35	2023	Investment Banking	-11559.602029	257	1.420892e+04
36	2023	Mortgage Lending	-62309.416786	223	1.406553e+04
37	2023	Private Banking	-122154.116543	260	1.559071e+04
38	2023	Retail	40540.769309	248	1.568344e+04
39	2023	Wealth Management	-91383.814653	258	1.407796e+04
40	2024	Asset Management	-58041.229004	211	1.335648e+04
41	2024	Corporate Banking	38215.560873	233	1.562396e+04
42	2024	Credit Card Services	58911.278557	276	1.428698e+04
43	2024	Financial Advisory	185682.726978	217	1.470406e+04
44	2024	Insurance	82516.684761	216	2.167852e+04
45	2024	Investment Banking	65759.603460	204	1.489201e+04
46	2024	Mortgage Lending	186733.475199	202	2.042332e+04
47	2024	Private Banking	45195.851332	240	1.338886e+04
48	2024	Retail	53774.234072	218	1.392258e+04
49	2024	Wealth Management	20074.269211	209	1.538710e+04
 
import pandas as pd
import numpy as np
from scipy.stats import poisson, lognorm, expon
from datetime import datetime, timedelta

# Generate dummy operational risk data
def random_dates(start, end, n=10000):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4 * 365)
end_date = datetime.now()

business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate random data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] +
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

df = pd.DataFrame(data)
df['Year'] = df['Date'].dt.year

# Monte Carlo simulation function
def monte_carlo_simulation(data, num_simulations=10000, confidence=0.999):
    results = []
    
    # Group by Year and Business Line
    grouped = data.groupby(['Year', 'Business Line'])
    
    for (year, business_line), group in grouped:
        # Frequency: Fit a Poisson distribution
        event_counts = len(group)
        frequency = poisson(event_counts)
        
        # Severity: Fit a Log-Normal distribution
        losses = group['Net Loss Amount']
        negative_losses = losses[losses < 0] * -1  # Convert losses to positive for fitting
        
        if len(negative_losses) > 0:
            shape, loc, scale = lognorm.fit(negative_losses, floc=0)
        else:
            shape, loc, scale = 1, 0, 1  # Default values if no negative losses
        
        # Monte Carlo simulations
        simulated_losses = []
        for _ in range(num_simulations):
            # Simulate frequency
            num_events = frequency.rvs()
            
            # Simulate severity
            if num_events > 0:
                event_losses = lognorm.rvs(shape, loc=loc, scale=scale, size=num_events)
                simulated_losses.append(np.sum(event_losses))
            else:
                simulated_losses.append(0)
        
        # Calculate VaR at the specified confidence level
        simulated_losses = np.sort(simulated_losses)
        var = simulated_losses[int(confidence * len(simulated_losses)) - 1]
        
        # Aggregate results
        results.append({
            "Year": year,
            "Business Line": business_line,
            "Total Net Loss": losses.sum(),
            "Event Count": event_counts,
            "VaR (99.9%)": var
        })
    
    return pd.DataFrame(results)

# Run the Monte Carlo simulation
results_df = monte_carlo_simulation(df)

# Save results to CSV
results_df.to_csv("monte_carlo_results.csv", index=False)

# Display the results
print(results_df)
    Year         Business Line  Total Net Loss  Event Count   VaR (99.9%)
0   2020      Asset Management    55519.391717           22  2.210633e+06
1   2020     Corporate Banking   -40130.758861           28  3.424433e+05
2   2020  Credit Card Services   -19102.153199           27  7.176778e+05
3   2020    Financial Advisory    59948.392446           24  8.953631e+05
4   2020             Insurance    -2479.154337           26  3.868427e+05
5   2020    Investment Banking    10981.017918           22  2.712867e+05
6   2020      Mortgage Lending    21382.984095           24  3.509252e+05
7   2020       Private Banking     -743.398458           22  3.436099e+05
8   2020                Retail   -10717.632820           24  2.752946e+05
9   2020     Wealth Management    11653.915328           24  3.079094e+05
10  2021      Asset Management    23902.592139          239  2.215250e+06
11  2021     Corporate Banking   104332.059493          227  1.960453e+06
12  2021  Credit Card Services    64097.767010          229  1.692992e+06
13  2021    Financial Advisory    92453.285740          235  1.664810e+06
14  2021             Insurance   -56338.188674          242  2.808985e+06
15  2021    Investment Banking    59747.839414          260  2.317055e+06
16  2021      Mortgage Lending   120428.231513          237  1.729269e+06
17  2021       Private Banking  -127233.219537          264  1.986027e+06
18  2021                Retail    89423.309646          250  2.045763e+06
19  2021     Wealth Management   -43933.644483          274  1.935515e+06
20  2022      Asset Management   104160.162264          242  1.941895e+06
21  2022     Corporate Banking    15693.276709          263  1.885439e+06
22  2022  Credit Card Services   -54662.521880          231  1.661523e+06
23  2022    Financial Advisory   -29448.777980          268  2.419414e+06
24  2022             Insurance  -102959.046941          275  2.162734e+06
25  2022    Investment Banking   -72021.282828          241  2.040052e+06
26  2022      Mortgage Lending    79257.233129          261  2.305613e+06
27  2022       Private Banking    84526.288510          253  2.103944e+06
28  2022                Retail   -75413.111807          231  1.726851e+06
29  2022     Wealth Management   -76709.453136          273  2.291304e+06
30  2023      Asset Management  -114082.425802          277  2.189543e+06
31  2023     Corporate Banking  -102269.305763          253  2.174113e+06
32  2023  Credit Card Services    58058.456839          256  1.789189e+06
33  2023    Financial Advisory   -70171.849669          252  2.164760e+06
34  2023             Insurance    85991.610519          276  2.335112e+06
35  2023    Investment Banking   104487.039565          273  2.106779e+06
36  2023      Mortgage Lending   -58219.421110          248  2.325215e+06
37  2023       Private Banking    60602.829217          222  1.997109e+06
38  2023                Retail   167625.455226          263  2.098411e+06
39  2023     Wealth Management   -24984.243399          250  2.209584e+06
40  2024      Asset Management   -64602.574478          217  1.754992e+06
41  2024     Corporate Banking    48260.215347          208  1.606545e+06
42  2024  Credit Card Services     -310.575814          219  1.606495e+06
43  2024    Financial Advisory   -19389.335429          199  1.542205e+06
44  2024             Insurance    73719.879894          218  1.628315e+06
45  2024    Investment Banking    67754.543240          227  1.804065e+06
46  2024      Mortgage Lending  -159099.219841          212  2.153467e+06
47  2024       Private Banking    69764.376290          245  1.760905e+06
48  2024                Retail    28249.709099          215  1.670541e+06
49  2024     Wealth Management   163799.903643          232  1.607923e+06
results_df
Year	Business Line	Total Net Loss	Event Count	VaR (99.9%)
0	2020	Asset Management	55519.391717	22	2.210633e+06
1	2020	Corporate Banking	-40130.758861	28	3.424433e+05
2	2020	Credit Card Services	-19102.153199	27	7.176778e+05
3	2020	Financial Advisory	59948.392446	24	8.953631e+05
4	2020	Insurance	-2479.154337	26	3.868427e+05
5	2020	Investment Banking	10981.017918	22	2.712867e+05
6	2020	Mortgage Lending	21382.984095	24	3.509252e+05
7	2020	Private Banking	-743.398458	22	3.436099e+05
8	2020	Retail	-10717.632820	24	2.752946e+05
9	2020	Wealth Management	11653.915328	24	3.079094e+05
10	2021	Asset Management	23902.592139	239	2.215250e+06
11	2021	Corporate Banking	104332.059493	227	1.960453e+06
12	2021	Credit Card Services	64097.767010	229	1.692992e+06
13	2021	Financial Advisory	92453.285740	235	1.664810e+06
14	2021	Insurance	-56338.188674	242	2.808985e+06
15	2021	Investment Banking	59747.839414	260	2.317055e+06
16	2021	Mortgage Lending	120428.231513	237	1.729269e+06
17	2021	Private Banking	-127233.219537	264	1.986027e+06
18	2021	Retail	89423.309646	250	2.045763e+06
19	2021	Wealth Management	-43933.644483	274	1.935515e+06
20	2022	Asset Management	104160.162264	242	1.941895e+06
21	2022	Corporate Banking	15693.276709	263	1.885439e+06
22	2022	Credit Card Services	-54662.521880	231	1.661523e+06
23	2022	Financial Advisory	-29448.777980	268	2.419414e+06
24	2022	Insurance	-102959.046941	275	2.162734e+06
25	2022	Investment Banking	-72021.282828	241	2.040052e+06
26	2022	Mortgage Lending	79257.233129	261	2.305613e+06
27	2022	Private Banking	84526.288510	253	2.103944e+06
28	2022	Retail	-75413.111807	231	1.726851e+06
29	2022	Wealth Management	-76709.453136	273	2.291304e+06
30	2023	Asset Management	-114082.425802	277	2.189543e+06
31	2023	Corporate Banking	-102269.305763	253	2.174113e+06
32	2023	Credit Card Services	58058.456839	256	1.789189e+06
33	2023	Financial Advisory	-70171.849669	252	2.164760e+06
34	2023	Insurance	85991.610519	276	2.335112e+06
35	2023	Investment Banking	104487.039565	273	2.106779e+06
36	2023	Mortgage Lending	-58219.421110	248	2.325215e+06
37	2023	Private Banking	60602.829217	222	1.997109e+06
38	2023	Retail	167625.455226	263	2.098411e+06
39	2023	Wealth Management	-24984.243399	250	2.209584e+06
40	2024	Asset Management	-64602.574478	217	1.754992e+06
41	2024	Corporate Banking	48260.215347	208	1.606545e+06
42	2024	Credit Card Services	-310.575814	219	1.606495e+06
43	2024	Financial Advisory	-19389.335429	199	1.542205e+06
44	2024	Insurance	73719.879894	218	1.628315e+06
45	2024	Investment Banking	67754.543240	227	1.804065e+06
46	2024	Mortgage Lending	-159099.219841	212	2.153467e+06
47	2024	Private Banking	69764.376290	245	1.760905e+06
48	2024	Retail	28249.709099	215	1.670541e+06
49	2024	Wealth Management	163799.903643	232	1.607923e+06
#prediction
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from pandas.tseries.offsets import DateOffset

# Assuming your data is structured with 'Date' as datetime and 'VaR (99.9%)' as the target column
# Let's simulate some data for demonstration, as we don't have actual data.

# Simulating quarterly data for VaR (99.9%) for demonstration (replace this with your actual DataFrame)
dates = pd.date_range(start="2020-01-01", end="2024-09-30", freq='Q')
np.random.seed(42)
var_data = np.random.lognormal(mean=0, sigma=1, size=len(dates))

# Create DataFrame for historical VaR data
results_df = pd.DataFrame({
    'Date': dates,
    'VaR (99.9%)': var_data
})

# Group by quarter (assuming you want to forecast by quarter)
results_df.set_index('Date', inplace=True)

# Visualize the data
plt.figure(figsize=(10, 6))
plt.plot(results_df.index, results_df['VaR (99.9%)'], marker='o', linestyle='-', color='b')
plt.title('Quarterly VaR (99.9%)')
plt.xlabel('Date')
plt.ylabel('VaR (99.9%)')
plt.grid(True)
plt.show()

# Step 1: Fit an ARIMA model (p, d, q are hyperparameters to tune, here we start with ARIMA(5,1,0))
model = ARIMA(results_df['VaR (99.9%)'], order=(5, 1, 0))

# Fit the model
model_fit = model.fit()

# Step 2: Forecast the next quarters (Q4 2024 to Q4 2025)
forecast_steps = 5  # We want to forecast for 5 quarters (Q4 2024, Q1-Q4 2025)
forecast = model_fit.get_forecast(steps=forecast_steps)
forecast_index = pd.date_range(start="2024-10-01", periods=forecast_steps, freq='Q')

# Step 3: Plot the forecast results
forecast_mean = forecast.predicted_mean
confidence_interval = forecast.conf_int()

plt.figure(figsize=(10, 6))
plt.plot(results_df.index, results_df['VaR (99.9%)'], label='Historical Data', color='blue')
plt.plot(forecast_index, forecast_mean, label='Forecasted VaR (99.9%)', color='red', linestyle='--')
plt.fill_between(forecast_index, confidence_interval.iloc[:, 0], confidence_interval.iloc[:, 1], color='red', alpha=0.3)
plt.title('VaR (99.9%) Forecast (Q4 2024 to Q4 2025)')
plt.xlabel('Date')
plt.ylabel('VaR (99.9%)')
plt.legend()
plt.grid(True)
plt.show()

# Step 4: Print the forecasted VaR values
forecasted_values = pd.DataFrame({
    'Date': forecast_index,
    'Forecasted VaR (99.9%)': forecast_mean,
    'Lower CI': confidence_interval.iloc[:, 0],
    'Upper CI': confidence_interval.iloc[:, 1]
})

print(forecasted_values)
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\3625918834.py:12: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  dates = pd.date_range(start="2020-01-01", end="2024-09-30", freq='Q')

C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QE-DEC will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QE-DEC will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency QE-DEC will be used.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\3625918834.py:43: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_steps, freq='Q')

                 Date  Forecasted VaR (99.9%)  Lower CI  Upper CI
2024-12-31 2024-12-31               -0.402898 -1.984195  1.178400
2025-03-31 2025-03-31                1.336610 -0.624302  3.297522
2025-06-30 2025-06-30                0.964327 -1.006220  2.934874
2025-09-30 2025-09-30               -0.105282 -2.382071  2.171508
2025-12-31 2025-12-31                0.771498 -2.008826  3.551822
forecasted_values
Date	Forecasted VaR (99.9%)	Lower CI	Upper CI
2024-12-31	2024-12-31	-0.402898	-1.984195	1.178400
2025-03-31	2025-03-31	1.336610	-0.624302	3.297522
2025-06-30	2025-06-30	0.964327	-1.006220	2.934874
2025-09-30	2025-09-30	-0.105282	-2.382071	2.171508
2025-12-31	2025-12-31	0.771498	-2.008826	3.551822
#business line
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from pandas.tseries.offsets import DateOffset

# Assuming 'results_df' is your dataframe with 'Date', 'VaR (99.9%)', and 'Business Line' columns
# Simulating some data (replace this with your actual DataFrame)
dates = pd.date_range(start="2020-01-01", end="2024-09-30", freq='Q')
business_lines = ["Retail", "Corporate Banking", "Investment Banking", "Insurance", "Wealth Management"]
np.random.seed(42)
var_data = np.random.lognormal(mean=0, sigma=1, size=len(dates))
business_line_data = np.random.choice(business_lines, len(dates))

# Create DataFrame for historical VaR data
results_df = pd.DataFrame({
    'Date': dates,
    'VaR (99.9%)': var_data,
    'Business Line': business_line_data
})

# Set Date column as index
results_df.set_index('Date', inplace=True)

# Visualize the data for all Business Lines (Optional)
plt.figure(figsize=(10, 6))
for business_line in business_lines:
    data_line = results_df[results_df['Business Line'] == business_line]
    plt.plot(data_line.index, data_line['VaR (99.9%)'], label=business_line)

plt.title('Quarterly VaR (99.9%) by Business Line')
plt.xlabel('Date')
plt.ylabel('VaR (99.9%)')
plt.legend()
plt.grid(True)
plt.show()

# Define a function to forecast VaR for each Business Line using ARIMA
def forecast_var_by_business_line(data, business_line, forecast_periods=5):
    # Filter data for the business line
    data_line = data[data['Business Line'] == business_line]
    
    # Fit the ARIMA model (you can adjust the (p, d, q) parameters as needed)
    model = ARIMA(data_line['VaR (99.9%)'], order=(5, 1, 0))  # Example ARIMA(5,1,0)
    model_fit = model.fit()
    
    # Forecast the next quarters (Q4 2024 to Q4 2025)
    forecast = model_fit.get_forecast(steps=forecast_periods)
    forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
    
    # Forecasted values and confidence intervals
    forecast_mean = forecast.predicted_mean
    confidence_interval = forecast.conf_int()

    return forecast_index, forecast_mean, confidence_interval

# Forecast for each business line
forecast_results = []

for business_line in business_lines:
    forecast_index, forecast_mean, confidence_interval = forecast_var_by_business_line(results_df, business_line)
    
    # Combine the results
    forecast_results.append(pd.DataFrame({
        'Business Line': business_line,
        'Date': forecast_index,
        'Forecasted VaR (99.9%)': forecast_mean,
        'Lower CI': confidence_interval.iloc[:, 0],
        'Upper CI': confidence_interval.iloc[:, 1]
    }))

# Combine all business line forecasts into a single DataFrame
final_forecast_df = pd.concat(forecast_results, ignore_index=True)

# Display the forecasted values
print(final_forecast_df)

# Optionally, save the forecast results to a CSV
final_forecast_df.to_csv("var_forecast_by_business_line.csv", index=False)

# Plot the forecasted VaR for each business line
plt.figure(figsize=(10, 6))
for business_line in business_lines:
    forecast_line = final_forecast_df[final_forecast_df['Business Line'] == business_line]
    plt.plot(forecast_line['Date'], forecast_line['Forecasted VaR (99.9%)'], label=f'{business_line} Forecast', linestyle='--')

plt.title('Forecasted VaR (99.9%) by Business Line (Q4 2024 to Q4 2025)')
plt.xlabel('Date')
plt.ylabel('VaR (99.9%)')
plt.legend()
plt.grid(True)
plt.show()
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1585265501.py:10: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  dates = pd.date_range(start="2020-01-01", end="2024-09-30", freq='Q')

C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1585265501.py:50: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1585265501.py:50: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1585265501.py:50: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1585265501.py:50: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1585265501.py:50: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
         Business Line       Date  Forecasted VaR (99.9%)  Lower CI   Upper CI
0               Retail 2024-12-31                0.570055 -3.910922   5.051033
1               Retail 2025-03-31                0.568800 -5.762754   6.900353
2               Retail 2025-06-30                0.568814 -7.240467   8.378094
3               Retail 2025-09-30                0.568786 -8.479862   9.617435
4               Retail 2025-12-31                0.568787 -9.569791  10.707364
5    Corporate Banking 2024-12-31                0.350790 -1.152230   1.853810
6    Corporate Banking 2025-03-31                0.336731 -1.650843   2.324306
7    Corporate Banking 2025-06-30                0.349020 -1.858349   2.556389
8    Corporate Banking 2025-09-30                0.354016 -2.054545   2.762577
9    Corporate Banking 2025-12-31                0.351197 -2.279830   2.982224
10  Investment Banking 2024-12-31                3.218935 -2.045144   8.483014
11  Investment Banking 2025-03-31                3.875323 -2.258133  10.008779
12  Investment Banking 2025-06-30                3.611363 -3.710539  10.933265
13  Investment Banking 2025-09-30                3.717518 -4.466686  11.901722
14  Investment Banking 2025-12-31                3.674832 -5.346126  12.695789
15           Insurance 2024-12-31                0.290202 -0.439350   1.019754
16           Insurance 2025-03-31                1.036770  0.179172   1.894368
17           Insurance 2025-06-30                0.185944 -1.113931   1.485819
18           Insurance 2025-09-30                0.940461 -0.497769   2.378692
19           Insurance 2025-12-31                0.162907 -1.621356   1.947170
20   Wealth Management 2024-12-31                2.048198  0.954298   3.142098
21   Wealth Management 2025-03-31                1.460892  0.357069   2.564714
22   Wealth Management 2025-06-30                1.968861  0.501988   3.435733
23   Wealth Management 2025-09-30                1.529463  0.040050   3.018877
24   Wealth Management 2025-12-31                1.909528  0.184424   3.634633

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from pandas.tseries.offsets import DateOffset

# Assuming 'results_df' is your dataframe with 'Date', 'VaR (99.9%)', and 'Business Line' columns
# Simulating some data (replace this with your actual DataFrame)
dates = pd.date_range(start="2020-01-01", end="2024-09-30", freq='Q')
business_lines = ["Retail", "Corporate Banking", "Investment Banking", "Insurance", "Wealth Management"]
np.random.seed(42)
var_data = np.random.lognormal(mean=0, sigma=1, size=len(dates))
business_line_data = np.random.choice(business_lines, len(dates))

# Create DataFrame for historical VaR data
results_df = pd.DataFrame({
    'Date': dates,
    'VaR (99.9%)': var_data,
    'Business Line': business_line_data
})

# Set Date column as index
results_df.set_index('Date', inplace=True)

# Visualize the data for all Business Lines (Optional)
plt.figure(figsize=(10, 6))
for business_line in business_lines:
    data_line = results_df[results_df['Business Line'] == business_line]
    plt.plot(data_line.index, data_line['VaR (99.9%)'], label=business_line)

plt.title('Quarterly VaR (99.9%) by Business Line')
plt.xlabel('Date')
plt.ylabel('VaR (99.9%)')
plt.legend()
plt.grid(True)
plt.show()

# Define a function to forecast VaR for each Business Line using ARIMA
def forecast_var_by_business_line(data, business_line, forecast_periods=5):
    # Filter data for the business line
    data_line = data[data['Business Line'] == business_line]
    
    # Fit the ARIMA model (you can adjust the (p, d, q) parameters as needed)
    model = ARIMA(data_line['VaR (99.9%)'], order=(5, 1, 0))  # Example ARIMA(5,1,0)
    model_fit = model.fit()
    
    # Forecast the next quarters (Q4 2024 to Q4 2025)
    forecast = model_fit.get_forecast(steps=forecast_periods)
    forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
    
    # Forecasted values and confidence intervals
    forecast_mean = forecast.predicted_mean
    confidence_interval = forecast.conf_int()

    return forecast_index, forecast_mean, confidence_interval

# Forecast for each business line
forecast_results = []

for business_line in business_lines:
    forecast_index, forecast_mean, confidence_interval = forecast_var_by_business_line(results_df, business_line)
    
    # Convert forecast_index to quarters (e.g., Q4 2024, Q1 2025, etc.)
    quarters = [f"Q{(x.month-1)//3 + 1} {x.year}" for x in forecast_index]
    
    # Combine the results
    forecast_results.append(pd.DataFrame({
        'Business Line': business_line,
        'Quarter': quarters,
        'Date': forecast_index,
        'Forecasted VaR (99.9%)': forecast_mean,
        'Lower CI': confidence_interval.iloc[:, 0],
        'Upper CI': confidence_interval.iloc[:, 1]
    }))

# Combine all business line forecasts into a single DataFrame
final_forecast_df = pd.concat(forecast_results, ignore_index=True)

# Display the forecasted values
print(final_forecast_df)

# Optionally, save the forecast results to a CSV
final_forecast_df.to_csv("var_forecast_by_business_line_with_quarter.csv", index=False)

# Plot the forecasted VaR for each business line
plt.figure(figsize=(10, 6))
for business_line in business_lines:
    forecast_line = final_forecast_df[final_forecast_df['Business Line'] == business_line]
    plt.plot(forecast_line['Date'], forecast_line['Forecasted VaR (99.9%)'], label=f'{business_line} Forecast', linestyle='--')

plt.title('Forecasted VaR (99.9%) by Business Line (Q4 2024 to Q4 2025)')
plt.xlabel('Date')
plt.ylabel('VaR (99.9%)')
plt.legend()
plt.grid(True)
plt.show()
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2319673010.py:9: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  dates = pd.date_range(start="2020-01-01", end="2024-09-30", freq='Q')

C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2319673010.py:49: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2319673010.py:49: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2319673010.py:49: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2319673010.py:49: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\statespace\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.
  warn('Too few observations to estimate starting parameters%s.'
         Business Line  Quarter       Date  Forecasted VaR (99.9%)  Lower CI  \
0               Retail  Q4 2024 2024-12-31                0.570055 -3.910922   
1               Retail  Q1 2025 2025-03-31                0.568800 -5.762754   
2               Retail  Q2 2025 2025-06-30                0.568814 -7.240467   
3               Retail  Q3 2025 2025-09-30                0.568786 -8.479862   
4               Retail  Q4 2025 2025-12-31                0.568787 -9.569791   
5    Corporate Banking  Q4 2024 2024-12-31                0.350790 -1.152230   
6    Corporate Banking  Q1 2025 2025-03-31                0.336731 -1.650843   
7    Corporate Banking  Q2 2025 2025-06-30                0.349020 -1.858349   
8    Corporate Banking  Q3 2025 2025-09-30                0.354016 -2.054545   
9    Corporate Banking  Q4 2025 2025-12-31                0.351197 -2.279830   
10  Investment Banking  Q4 2024 2024-12-31                3.218935 -2.045144   
11  Investment Banking  Q1 2025 2025-03-31                3.875323 -2.258133   
12  Investment Banking  Q2 2025 2025-06-30                3.611363 -3.710539   
13  Investment Banking  Q3 2025 2025-09-30                3.717518 -4.466686   
14  Investment Banking  Q4 2025 2025-12-31                3.674832 -5.346126   
15           Insurance  Q4 2024 2024-12-31                0.290202 -0.439350   
16           Insurance  Q1 2025 2025-03-31                1.036770  0.179172   
17           Insurance  Q2 2025 2025-06-30                0.185944 -1.113931   
18           Insurance  Q3 2025 2025-09-30                0.940461 -0.497769   
19           Insurance  Q4 2025 2025-12-31                0.162907 -1.621356   
20   Wealth Management  Q4 2024 2024-12-31                2.048198  0.954298   
21   Wealth Management  Q1 2025 2025-03-31                1.460892  0.357069   
22   Wealth Management  Q2 2025 2025-06-30                1.968861  0.501988   
23   Wealth Management  Q3 2025 2025-09-30                1.529463  0.040050   
24   Wealth Management  Q4 2025 2025-12-31                1.909528  0.184424   

     Upper CI  
0    5.051033  
1    6.900353  
2    8.378094  
3    9.617435  
4   10.707364  
5    1.853810  
6    2.324306  
7    2.556389  
8    2.762577  
9    2.982224  
10   8.483014  
11  10.008779  
12  10.933265  
13  11.901722  
14  12.695789  
15   1.019754  
16   1.894368  
17   1.485819  
18   2.378692  
19   1.947170  
20   3.142098  
21   2.564714  
22   3.435733  
23   3.018877  
24   3.634633  
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\statsmodels\tsa\base\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.
  return get_prediction_index(
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2319673010.py:49: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
  forecast_index = pd.date_range(start="2024-10-01", periods=forecast_periods, freq='Q')

# from statsmodels.tsa.statespace.sarimax import SARIMAX

# # Fit a SARIMA model (example with seasonal periods=4 for quarterly data)
# model = SARIMAX(results_df['VaR (99.9%)'], order=(5, 1, 0), seasonal_order=(1, 1, 0, 4))
# model_fit = model.fit()

# # Forecast the next 5 periods (quarters)
# forecast = model_fit.get_forecast(steps=5)
# forecasted_values = forecast.predicted_mean
# print(forecasted_values)
from fbprophet import Prophet

# Prepare data for Prophet
data_prophet = results_df.reset_index()[['Date', 'VaR (99.9%)']]
data_prophet.columns = ['ds', 'y']  # Prophet expects columns 'ds' for datetime and 'y' for the value

# Fit Prophet model
model = Prophet()
model.fit(data_prophet)

# Make future dataframe for forecasting
future = model.make_future_dataframe(data_prophet, periods=5, freq='Q')  # 5 quarters forecast

# Forecast the future
forecast = model.predict(future)

# Extract forecasted values and plot
forecasted_values = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
forecasted_values['Quarter'] = forecasted_values['ds'].dt.to_period('Q')

print(forecasted_values)
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Cell In[16], line 1
----> 1 from fbprophet import Prophet
      3 # Prepare data for Prophet
      4 data_prophet = results_df.reset_index()[['Date', 'VaR (99.9%)']]

ModuleNotFoundError: No module named 'fbprophet'
# !pip install fbprophet
import numpy as np

# Calculate outliers using IQR
def detect_outliers_iqr(data, column):
    q1 = data[column].quantile(0.25)  # First quartile
    q3 = data[column].quantile(0.75)  # Third quartile
    iqr = q3 - q1  # Interquartile range
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Calculate outliers using Z-score
def detect_outliers_zscore(data, column, threshold=3):
    mean = data[column].mean()
    std = data[column].std()
    data['Z-Score'] = (data[column] - mean) / std
    outliers = data[(data['Z-Score'] > threshold) | (data['Z-Score'] < -threshold)]
    return outliers

# Detect outliers in VaR column
iqr_outliers, iqr_lower, iqr_upper = detect_outliers_iqr(results_df, 'VaR (99.9%)')
zscore_outliers = detect_outliers_zscore(results_df, 'VaR (99.9%)')

# Display the results
print("Outliers using IQR:")
print(iqr_outliers)
print(f"IQR Lower Bound: {iqr_lower}, IQR Upper Bound: {iqr_upper}")

print("\nOutliers using Z-Score:")
print(zscore_outliers)

# Save outliers to CSV for reference
iqr_outliers.to_csv("var_outliers_iqr.csv", index=False)
zscore_outliers.to_csv("var_outliers_zscore.csv", index=False)
Outliers using IQR:
    Year         Business Line  Total Net Loss  Event Count   VaR (99.9%)
0   2020      Asset Management   -50961.533870           23  2.388394e+05
1   2020     Corporate Banking   -14840.512972           30  3.885621e+05
2   2020  Credit Card Services    11192.298485           29  3.170874e+05
3   2020    Financial Advisory     7666.418749           32  3.205742e+05
4   2020             Insurance   -42251.061396           36  6.410208e+05
5   2020    Investment Banking   -46892.504986           32  2.992099e+05
6   2020      Mortgage Lending    34340.110862           27  3.481474e+05
8   2020                Retail    11811.487730           30  3.987444e+05
9   2020     Wealth Management    11578.673597           37  5.612208e+05
16  2021      Mortgage Lending  -254428.454058          245  3.001495e+06
18  2021                Retail    49185.684787          255  3.406140e+06
IQR Lower Bound: 969968.7078125395, IQR Upper Bound: 2914269.0988309057

Outliers using Z-Score:
Empty DataFrame
Columns: [Year, Business Line, Total Net Loss, Event Count, VaR (99.9%), Z-Score]
Index: []
# Fixing the typo in the function and re-running the code
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Simulating results_df DataFrame for demonstration purposes
np.random.seed(42)
results_df = pd.DataFrame({
    'VaR (99.9%)': np.random.lognormal(mean=0, sigma=1, size=1000)  # log-normal distribution
})

# Function to detect outliers using IQR method
def detect_outliers_iqr(data, column):
    q1 = data[column].quantile(0.25)  # First quartile
    q3 = data[column].quantile(0.75)  # Third quartile
    iqr = q3 - q1  # Interquartile range
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Function to detect outliers using Z-score method
def detect_outliers_zscore(data, column, threshold=3):
    mean = data[column].mean()
    std = data[column].std()
    data['Z-Score'] = (data[column] - mean) / std
    outliers = data[(data['Z-Score'] > threshold) | (data['Z-Score'] < -threshold)]
    return outliers

# Detect outliers in VaR column using both IQR and Z-score methods
iqr_outliers, iqr_lower, iqr_upper = detect_outliers_iqr(results_df, 'VaR (99.9%)')
zscore_outliers = detect_outliers_zscore(results_df, 'VaR (99.9%)')

# Plotting the distribution and highlighting the outliers
plt.figure(figsize=(10, 6))
plt.hist(results_df['VaR (99.9%)'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)
plt.axvline(x=iqr_lower, color='red', linestyle='--', label=f'IQR Lower Bound ({iqr_lower:.2f})')
plt.axvline(x=iqr_upper, color='green', linestyle='--', label=f'IQR Upper Bound ({iqr_upper:.2f})')
plt.scatter(iqr_outliers['VaR (99.9%)'], np.zeros(len(iqr_outliers)), color='red', label='IQR Outliers', zorder=5)
plt.scatter(zscore_outliers['VaR (99.9%)'], np.zeros(len(zscore_outliers)), color='orange', label='Z-Score Outliers', zorder=5)
plt.title('Distribution of VaR (99.9%) and Outliers')
plt.xlabel('VaR (99.9%)')
plt.ylabel('Frequency')
plt.legend(loc='upper right')
plt.show()

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Business Line', 'Event Type']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Business Line', 'Event Type', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Business Line', 'Event Type'], how='left')

# --- Prediction for Q4 2024 to Q4 2025 ---
# Create future quarters data
future_years = [2024, 2025]
quarters = ['Q4']

future_data = []

for year in future_years:
    for quarter in quarters:
        for business_line in business_lines:
            for event_type in event_types:
                # Filter previous data for the specific business line and event type
                subset = final_result[(final_result['Year'] == year) & 
                                      (final_result['Business Line'] == business_line) & 
                                      (final_result['Event Type'] == event_type)]
                if not subset.empty:
                    var_99_9 = subset['VaR (99.9%)'].values[0]
                    total_loss = subset['Total_Loss_Amount'].values[0]
                    event_count = subset['Event_Count'].values[0]
                    future_data.append([year, business_line, event_type, quarter, var_99_9, total_loss, event_count])

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Business Line', 'Event Type', 'Quarter', 'PredVaR', 'Total_Loss_Amount', 'Event_Count'])

# Display future predictions for VaR
print(future_df)

# Optionally, save to a CSV file
future_df.to_csv('predicted_var_2024_2025.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: invalid value encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6423: RuntimeWarning: invalid value encountered in log
  lambda x, s: (-np.log(x)**2 / (2 * s**2)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6424: RuntimeWarning: invalid value encountered in log
  - np.log(s * x * np.sqrt(2 * np.pi))),
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: divide by zero encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
    Year       Business Line            Event Type Quarter       PredVaR  \
0   2024              Retail                 Fraud      Q4  3.013825e+17   
1   2024              Retail        System Failure      Q4  1.397580e+04   
2   2024              Retail                 Theft      Q4  1.671174e+20   
3   2024              Retail            Compliance      Q4  8.740680e+20   
4   2024              Retail      Natural Disaster      Q4  1.403693e+04   
..   ...                 ...                   ...     ...           ...   
95  2024  Financial Advisory          Cyber Attack      Q4  1.387748e+04   
96  2024  Financial Advisory           Market Risk      Q4  1.288398e+04   
97  2024  Financial Advisory     Operational Error      Q4  4.558177e+16   
98  2024  Financial Advisory           Vendor Risk      Q4  1.432755e+04   
99  2024  Financial Advisory  Regulatory Violation      Q4  2.094140e+15   

    Total_Loss_Amount  Event_Count  
0         9169.703688           24  
1       -78684.827257           23  
2        19892.058317           17  
3        46591.172171           24  
4       -21278.111125           27  
..                ...          ...  
95      -31659.253161           24  
96       11851.192437           36  
97      -25636.444448           20  
98      -36321.492063           30  
99      -43523.683548           23  

[100 rows x 7 columns]
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\3170249096.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Business Line', 'Event Type']).apply(
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Business Line', 'Event Type']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Business Line', 'Event Type', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Business Line', 'Event Type'], how='left')

# --- Prediction for Q4 2024 to Q4 2025 ---
# Create future quarters data
future_years = [2024, 2025]
quarters = ['Q4']

future_data = []

for year in future_years:
    for quarter in quarters:
        for business_line in business_lines:
            for event_type in event_types:
                # Filter previous data for the specific business line and event type
                subset = final_result[(final_result['Year'] == year) & 
                                      (final_result['Business Line'] == business_line) & 
                                      (final_result['Event Type'] == event_type)]
                if not subset.empty:
                    # Retrieve historical VaR, Event Count, and Total Loss
                    var_99_9 = subset['VaR (99.9%)'].values[0]
                    total_loss = subset['Total_Loss_Amount'].values[0]
                    event_count = subset['Event_Count'].values[0]

                    # Predict the event count for the future period (using historical mean)
                    predicted_event_count = event_count  # Assuming future counts are similar to the past

                    future_data.append([year, business_line, event_type, quarter, var_99_9, total_loss, event_count, predicted_event_count])

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Business Line', 'Event Type', 'Quarter', 'PredVaR', 'Total_Loss_Amount', 'Event_Count', 'Predicted_Event_Count'])

# Display future predictions for VaR and Event Counts
print(future_df)

# Optionally, save to a CSV file
future_df.to_csv('predicted_var_and_event_count_2024_2025.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: divide by zero encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6423: RuntimeWarning: invalid value encountered in log
  lambda x, s: (-np.log(x)**2 / (2 * s**2)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6424: RuntimeWarning: invalid value encountered in log
  - np.log(s * x * np.sqrt(2 * np.pi))),
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: invalid value encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
    Year       Business Line            Event Type Quarter       PredVaR  \
0   2024              Retail                 Fraud      Q4  1.244200e+04   
1   2024              Retail        System Failure      Q4  1.797124e+04   
2   2024              Retail                 Theft      Q4  1.479670e+15   
3   2024              Retail            Compliance      Q4  2.275254e+20   
4   2024              Retail      Natural Disaster      Q4  9.028397e+16   
..   ...                 ...                   ...     ...           ...   
95  2024  Financial Advisory          Cyber Attack      Q4  1.402470e+16   
96  2024  Financial Advisory           Market Risk      Q4  5.382466e+16   
97  2024  Financial Advisory     Operational Error      Q4  1.401166e+04   
98  2024  Financial Advisory           Vendor Risk      Q4  1.499460e+20   
99  2024  Financial Advisory  Regulatory Violation      Q4  1.005249e+17   

    Total_Loss_Amount  Event_Count  Predicted_Event_Count  
0        -9530.201888           26                     26  
1         8942.316461           31                     31  
2       -17394.004786           24                     24  
3        72620.589411           25                     25  
4       -33819.510505           16                     16  
..                ...          ...                    ...  
95       -1570.706710           18                     18  
96        3578.516576           21                     21  
97      -18976.453350           32                     32  
98        8278.974817           14                     14  
99       -8616.591890           24                     24  

[100 rows x 8 columns]
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\476894029.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Business Line', 'Event Type']).apply(
import pandas as pd
import numpy as np
import itertools
from datetime import datetime, timedelta
from scipy import stats

# Generate the existing data as before (skip for brevity)

# --- Prediction for Q4 2024 to Q4 2025 ---

# Define future years, quarters, business lines, and event types
future_years = [2024, 2025]
quarters = ['Q4']
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]
event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Create all combinations of future years, quarters, business lines, and event types
future_combinations = list(itertools.product(future_years, quarters, business_lines, event_types))

# Function to predict future event count based on historical data
def predict_event_count(year, business_line, event_type):
    # Filter previous data for the specific business line and event type
    subset = final_result[(final_result['Year'] == year) & 
                          (final_result['Business Line'] == business_line) & 
                          (final_result['Event Type'] == event_type)]
    if not subset.empty:
        # Retrieve historical VaR, Event Count, and Total Loss
        var_99_9 = subset['VaR (99.9%)'].values[0]
        total_loss = subset['Total_Loss_Amount'].values[0]
        event_count = subset['Event_Count'].values[0]
        
        # Predict the event count for the future period (using historical mean)
        predicted_event_count = event_count  # Assuming future counts are similar to the past
        
        return [year, business_line, event_type, 'Q4', var_99_9, total_loss, event_count, predicted_event_count]
    return None

# Generate future predictions
future_data = []

for combination in future_combinations:
    year, quarter, business_line, event_type = combination
    prediction = predict_event_count(year, business_line, event_type)
    if prediction:
        future_data.append(prediction)

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Business Line', 'Event Type', 'Quarter', 'PredVaR', 'Total_Loss_Amount', 'Event_Count', 'Predicted_Event_Count'])

# Display future predictions for VaR and Event Counts
print(future_df)

# Optionally, save to a CSV file
future_df.to_csv('predicted_var_and_event_count_2024_2025.csv', index=False)
    Year       Business Line            Event Type Quarter       PredVaR  \
0   2024              Retail                 Fraud      Q4  1.244200e+04   
1   2024              Retail        System Failure      Q4  1.797124e+04   
2   2024              Retail                 Theft      Q4  1.479670e+15   
3   2024              Retail            Compliance      Q4  2.275254e+20   
4   2024              Retail      Natural Disaster      Q4  9.028397e+16   
..   ...                 ...                   ...     ...           ...   
95  2024  Financial Advisory          Cyber Attack      Q4  1.402470e+16   
96  2024  Financial Advisory           Market Risk      Q4  5.382466e+16   
97  2024  Financial Advisory     Operational Error      Q4  1.401166e+04   
98  2024  Financial Advisory           Vendor Risk      Q4  1.499460e+20   
99  2024  Financial Advisory  Regulatory Violation      Q4  1.005249e+17   

    Total_Loss_Amount  Event_Count  Predicted_Event_Count  
0        -9530.201888           26                     26  
1         8942.316461           31                     31  
2       -17394.004786           24                     24  
3        72620.589411           25                     25  
4       -33819.510505           16                     16  
..                ...          ...                    ...  
95       -1570.706710           18                     18  
96        3578.516576           21                     21  
97      -18976.453350           32                     32  
98        8278.974817           14                     14  
99       -8616.591890           24                     24  

[100 rows x 8 columns]
import pandas as pd
import numpy as np
import itertools
from scipy import stats

# Function to generate random dates (as an example)
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Business Line']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Business Line', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Business Line']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Business Line'], how='left')

# --- Prediction for Q4 2024 to Q4 2025 ---
# Define future years, quarters, and business lines (remove event types)
future_years = [2024, 2025]
quarters = ['Q4']
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

# Create all combinations of future years, quarters, and business lines
future_combinations = list(itertools.product(future_years, quarters, business_lines))

# Function to predict future event count based on historical data
def predict_event_count(year, business_line):
    # Filter previous data for the specific business line
    subset = final_result[(final_result['Year'] == year) & 
                          (final_result['Business Line'] == business_line)]
    if not subset.empty:
        # Retrieve historical VaR, Event Count, and Total Loss
        var_99_9 = subset['VaR (99.9%)'].values[0]
        total_loss = subset['Total_Loss_Amount'].values[0]
        event_count = subset['Event_Count'].values[0]
        
        # Predict the event count for the future period (using historical mean)
        predicted_event_count = event_count  # Assuming future counts are similar to the past
        
        return [year, business_line, 'Q4', var_99_9, total_loss, event_count, predicted_event_count]
    return None

# Generate future predictions
future_data = []

for combination in future_combinations:
    year, quarter, business_line = combination
    prediction = predict_event_count(year, business_line)
    if prediction:
        future_data.append(prediction)

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Business Line', 'Quarter', 'PredVaR', 'Total_Loss_Amount', 'Event_Count', 'Predicted_Event_Count'])

# Display future predictions for VaR and Event Counts
print(future_df)

# Optionally, save to a CSV file
future_df.to_csv('predicted_var_and_event_count_2024_2025_no_event_type.csv', index=False)
   Year         Business Line Quarter       PredVaR  Total_Loss_Amount  \
0  2024                Retail      Q4  14204.196237       81879.227364   
1  2024     Corporate Banking      Q4  13710.369658      143305.288100   
2  2024    Investment Banking      Q4  13365.062669      -86349.024325   
3  2024             Insurance      Q4  14799.205063      -65888.259264   
4  2024     Wealth Management      Q4  13209.259106      120339.807615   
5  2024      Asset Management      Q4  13552.764481        7452.185387   
6  2024       Private Banking      Q4  15092.995938        9882.079577   
7  2024  Credit Card Services      Q4  14850.455813       12034.090553   
8  2024      Mortgage Lending      Q4  13818.372100      -21104.504314   
9  2024    Financial Advisory      Q4  13839.213563       41685.509183   

   Event_Count  Predicted_Event_Count  
0          219                    219  
1          216                    216  
2          221                    221  
3          211                    211  
4          209                    209  
5          229                    229  
6          223                    223  
7          218                    218  
8          209                    209  
9          237                    237  
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\4088505600.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Business Line']).apply(
future_df
Year	Business Line	Quarter	PredVaR	Total_Loss_Amount	Event_Count	Predicted_Event_Count
0	2024	Retail	Q4	14204.196237	81879.227364	219	219
1	2024	Corporate Banking	Q4	13710.369658	143305.288100	216	216
2	2024	Investment Banking	Q4	13365.062669	-86349.024325	221	221
3	2024	Insurance	Q4	14799.205063	-65888.259264	211	211
4	2024	Wealth Management	Q4	13209.259106	120339.807615	209	209
5	2024	Asset Management	Q4	13552.764481	7452.185387	229	229
6	2024	Private Banking	Q4	15092.995938	9882.079577	223	223
7	2024	Credit Card Services	Q4	14850.455813	12034.090553	218	218
8	2024	Mortgage Lending	Q4	13818.372100	-21104.504314	209	209
9	2024	Financial Advisory	Q4	13839.213563	41685.509183	237	237
import pandas as pd
import numpy as np
import itertools
from scipy import stats
from datetime import datetime, timedelta

# Function to generate random dates (as an example)
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Business Line']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Business Line', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Business Line']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Business Line'], how='left')

# --- Prediction for Q4 2024 to Q4 2025 ---
# Define future years, quarters, and business lines (no event types)
future_years = [2024, 2025]
quarters = ['Q4', 'Q1', 'Q2', 'Q3', 'Q4']  # Full quarters for 2025
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

# Create all combinations of future years, quarters, and business lines
future_combinations = list(itertools.product(future_years, quarters, business_lines))

# Function to predict future event count based on historical data
def predict_event_count(year, business_line, quarter):
    # Filter previous data for the specific business line and year
    subset = final_result[(final_result['Year'] == year) & 
                          (final_result['Business Line'] == business_line)]
    if not subset.empty:
        # Retrieve historical VaR, Event Count, and Total Loss
        var_99_9 = subset['VaR (99.9%)'].values[0]
        total_loss = subset['Total_Loss_Amount'].values[0]
        event_count = subset['Event_Count'].values[0]
        
        # Predict the event count for the future period (using historical mean)
        predicted_event_count = event_count  # Assuming future counts are similar to the past
        
        return [year, business_line, quarter, var_99_9, total_loss, event_count, predicted_event_count]
    return None

# Generate future predictions
future_data = []

for combination in future_combinations:
    year, quarter, business_line = combination
    prediction = predict_event_count(year, business_line, quarter)
    if prediction:
        future_data.append(prediction)

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Business Line', 'Quarter', 'PredVaR', 'Total_Loss_Amount', 'Event_Count', 'Predicted_Event_Count'])

# Filter for the required periods: Q4 2024, Q1-Q4 2025
required_periods = ['Q4', 'Q1', 'Q2', 'Q3', 'Q4']
future_df_filtered = future_df[future_df['Quarter'].isin(required_periods)]

# Display filtered predictions for VaR and Event Counts
print(future_df_filtered)

# Optionally, save to a CSV file
future_df_filtered.to_csv('predicted_var_and_event_count_2024_2025_filtered.csv', index=False)
    Year         Business Line Quarter       PredVaR  Total_Loss_Amount  \
0   2024                Retail      Q4  14141.125619       41587.408274   
1   2024     Corporate Banking      Q4  14163.131695     -260625.088723   
2   2024    Investment Banking      Q4  16256.022330       68033.250982   
3   2024             Insurance      Q4  17141.349440      -50113.395498   
4   2024     Wealth Management      Q4  13916.381944     -132879.665828   
5   2024      Asset Management      Q4  14874.367779     -134068.145535   
6   2024       Private Banking      Q4  14112.166923      -48717.104563   
7   2024  Credit Card Services      Q4  14403.780221     -165808.715621   
8   2024      Mortgage Lending      Q4  14290.153021      -15544.565281   
9   2024    Financial Advisory      Q4  14875.525907      -93035.498970   
10  2024                Retail      Q1  14141.125619       41587.408274   
11  2024     Corporate Banking      Q1  14163.131695     -260625.088723   
12  2024    Investment Banking      Q1  16256.022330       68033.250982   
13  2024             Insurance      Q1  17141.349440      -50113.395498   
14  2024     Wealth Management      Q1  13916.381944     -132879.665828   
15  2024      Asset Management      Q1  14874.367779     -134068.145535   
16  2024       Private Banking      Q1  14112.166923      -48717.104563   
17  2024  Credit Card Services      Q1  14403.780221     -165808.715621   
18  2024      Mortgage Lending      Q1  14290.153021      -15544.565281   
19  2024    Financial Advisory      Q1  14875.525907      -93035.498970   
20  2024                Retail      Q2  14141.125619       41587.408274   
21  2024     Corporate Banking      Q2  14163.131695     -260625.088723   
22  2024    Investment Banking      Q2  16256.022330       68033.250982   
23  2024             Insurance      Q2  17141.349440      -50113.395498   
24  2024     Wealth Management      Q2  13916.381944     -132879.665828   
25  2024      Asset Management      Q2  14874.367779     -134068.145535   
26  2024       Private Banking      Q2  14112.166923      -48717.104563   
27  2024  Credit Card Services      Q2  14403.780221     -165808.715621   
28  2024      Mortgage Lending      Q2  14290.153021      -15544.565281   
29  2024    Financial Advisory      Q2  14875.525907      -93035.498970   
30  2024                Retail      Q3  14141.125619       41587.408274   
31  2024     Corporate Banking      Q3  14163.131695     -260625.088723   
32  2024    Investment Banking      Q3  16256.022330       68033.250982   
33  2024             Insurance      Q3  17141.349440      -50113.395498   
34  2024     Wealth Management      Q3  13916.381944     -132879.665828   
35  2024      Asset Management      Q3  14874.367779     -134068.145535   
36  2024       Private Banking      Q3  14112.166923      -48717.104563   
37  2024  Credit Card Services      Q3  14403.780221     -165808.715621   
38  2024      Mortgage Lending      Q3  14290.153021      -15544.565281   
39  2024    Financial Advisory      Q3  14875.525907      -93035.498970   
40  2024                Retail      Q4  14141.125619       41587.408274   
41  2024     Corporate Banking      Q4  14163.131695     -260625.088723   
42  2024    Investment Banking      Q4  16256.022330       68033.250982   
43  2024             Insurance      Q4  17141.349440      -50113.395498   
44  2024     Wealth Management      Q4  13916.381944     -132879.665828   
45  2024      Asset Management      Q4  14874.367779     -134068.145535   
46  2024       Private Banking      Q4  14112.166923      -48717.104563   
47  2024  Credit Card Services      Q4  14403.780221     -165808.715621   
48  2024      Mortgage Lending      Q4  14290.153021      -15544.565281   
49  2024    Financial Advisory      Q4  14875.525907      -93035.498970   

    Event_Count  Predicted_Event_Count  
0           237                    237  
1           207                    207  
2           208                    208  
3           232                    232  
4           224                    224  
5           243                    243  
6           186                    186  
7           216                    216  
8           228                    228  
9           256                    256  
10          237                    237  
11          207                    207  
12          208                    208  
13          232                    232  
14          224                    224  
15          243                    243  
16          186                    186  
17          216                    216  
18          228                    228  
19          256                    256  
20          237                    237  
21          207                    207  
22          208                    208  
23          232                    232  
24          224                    224  
25          243                    243  
26          186                    186  
27          216                    216  
28          228                    228  
29          256                    256  
30          237                    237  
31          207                    207  
32          208                    208  
33          232                    232  
34          224                    224  
35          243                    243  
36          186                    186  
37          216                    216  
38          228                    228  
39          256                    256  
40          237                    237  
41          207                    207  
42          208                    208  
43          232                    232  
44          224                    224  
45          243                    243  
46          186                    186  
47          216                    216  
48          228                    228  
49          256                    256  
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\661819880.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Business Line']).apply(
future_df
Year	Business Line	Quarter	PredVaR	Total_Loss_Amount	Event_Count	Predicted_Event_Count
0	2024	Retail	Q4	14141.125619	41587.408274	237	237
1	2024	Corporate Banking	Q4	14163.131695	-260625.088723	207	207
2	2024	Investment Banking	Q4	16256.022330	68033.250982	208	208
3	2024	Insurance	Q4	17141.349440	-50113.395498	232	232
4	2024	Wealth Management	Q4	13916.381944	-132879.665828	224	224
5	2024	Asset Management	Q4	14874.367779	-134068.145535	243	243
6	2024	Private Banking	Q4	14112.166923	-48717.104563	186	186
7	2024	Credit Card Services	Q4	14403.780221	-165808.715621	216	216
8	2024	Mortgage Lending	Q4	14290.153021	-15544.565281	228	228
9	2024	Financial Advisory	Q4	14875.525907	-93035.498970	256	256
10	2024	Retail	Q1	14141.125619	41587.408274	237	237
11	2024	Corporate Banking	Q1	14163.131695	-260625.088723	207	207
12	2024	Investment Banking	Q1	16256.022330	68033.250982	208	208
13	2024	Insurance	Q1	17141.349440	-50113.395498	232	232
14	2024	Wealth Management	Q1	13916.381944	-132879.665828	224	224
15	2024	Asset Management	Q1	14874.367779	-134068.145535	243	243
16	2024	Private Banking	Q1	14112.166923	-48717.104563	186	186
17	2024	Credit Card Services	Q1	14403.780221	-165808.715621	216	216
18	2024	Mortgage Lending	Q1	14290.153021	-15544.565281	228	228
19	2024	Financial Advisory	Q1	14875.525907	-93035.498970	256	256
20	2024	Retail	Q2	14141.125619	41587.408274	237	237
21	2024	Corporate Banking	Q2	14163.131695	-260625.088723	207	207
22	2024	Investment Banking	Q2	16256.022330	68033.250982	208	208
23	2024	Insurance	Q2	17141.349440	-50113.395498	232	232
24	2024	Wealth Management	Q2	13916.381944	-132879.665828	224	224
25	2024	Asset Management	Q2	14874.367779	-134068.145535	243	243
26	2024	Private Banking	Q2	14112.166923	-48717.104563	186	186
27	2024	Credit Card Services	Q2	14403.780221	-165808.715621	216	216
28	2024	Mortgage Lending	Q2	14290.153021	-15544.565281	228	228
29	2024	Financial Advisory	Q2	14875.525907	-93035.498970	256	256
30	2024	Retail	Q3	14141.125619	41587.408274	237	237
31	2024	Corporate Banking	Q3	14163.131695	-260625.088723	207	207
32	2024	Investment Banking	Q3	16256.022330	68033.250982	208	208
33	2024	Insurance	Q3	17141.349440	-50113.395498	232	232
34	2024	Wealth Management	Q3	13916.381944	-132879.665828	224	224
35	2024	Asset Management	Q3	14874.367779	-134068.145535	243	243
36	2024	Private Banking	Q3	14112.166923	-48717.104563	186	186
37	2024	Credit Card Services	Q3	14403.780221	-165808.715621	216	216
38	2024	Mortgage Lending	Q3	14290.153021	-15544.565281	228	228
39	2024	Financial Advisory	Q3	14875.525907	-93035.498970	256	256
40	2024	Retail	Q4	14141.125619	41587.408274	237	237
41	2024	Corporate Banking	Q4	14163.131695	-260625.088723	207	207
42	2024	Investment Banking	Q4	16256.022330	68033.250982	208	208
43	2024	Insurance	Q4	17141.349440	-50113.395498	232	232
44	2024	Wealth Management	Q4	13916.381944	-132879.665828	224	224
45	2024	Asset Management	Q4	14874.367779	-134068.145535	243	243
46	2024	Private Banking	Q4	14112.166923	-48717.104563	186	186
47	2024	Credit Card Services	Q4	14403.780221	-165808.715621	216	216
48	2024	Mortgage Lending	Q4	14290.153021	-15544.565281	228	228
49	2024	Financial Advisory	Q4	14875.525907	-93035.498970	256	256
# --- Prediction for Q4 2024 to Q4 2025 ---
future_years = [2024, 2025]
quarters = ['Q4', 'Q1', 'Q2', 'Q3', 'Q4']  # Full quarters for 2025
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

# Create all combinations of future years, quarters, and business lines
future_combinations = list(itertools.product(future_years, quarters, business_lines))

# Function to predict future event count based on historical data
def predict_event_count(year, business_line, quarter):
    # Filter previous data for the specific business line and year
    subset = final_result[(final_result['Year'] == year) & 
                          (final_result['Business Line'] == business_line)]
    
    if not subset.empty:
        # Retrieve historical VaR, Event Count, and Total Loss
        var_99_9 = subset['VaR (99.9%)'].values[0]
        total_loss = subset['Total_Loss_Amount'].values[0]
        event_count = subset['Event_Count'].values[0]
        
        # Predict the event count for the future period (assuming future counts are similar to the past)
        predicted_event_count = event_count  # Assuming future counts are similar to the past
        
        return [year, business_line, quarter, var_99_9, total_loss, event_count, predicted_event_count]
    return None

# Generate future predictions
future_data = []

for combination in future_combinations:
    year, quarter, business_line = combination
    prediction = predict_event_count(year, business_line, quarter)
    if prediction:
        future_data.append(prediction)

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Business Line', 'Quarter', 'PredVaR', 'Total_Loss_Amount', 'Event_Count', 'Predicted_Event_Count'])

# Filter for the required periods: Q4 2024, Q1-Q4 2025
required_periods = ['Q4', 'Q1', 'Q2', 'Q3', 'Q4']
future_df_filtered = future_df[future_df['Quarter'].isin(required_periods)]

# Display filtered predictions for VaR and Event Counts
print(future_df_filtered)
    Year         Business Line Quarter       PredVaR  Total_Loss_Amount  \
0   2024                Retail      Q4  14141.125619       41587.408274   
1   2024     Corporate Banking      Q4  14163.131695     -260625.088723   
2   2024    Investment Banking      Q4  16256.022330       68033.250982   
3   2024             Insurance      Q4  17141.349440      -50113.395498   
4   2024     Wealth Management      Q4  13916.381944     -132879.665828   
5   2024      Asset Management      Q4  14874.367779     -134068.145535   
6   2024       Private Banking      Q4  14112.166923      -48717.104563   
7   2024  Credit Card Services      Q4  14403.780221     -165808.715621   
8   2024      Mortgage Lending      Q4  14290.153021      -15544.565281   
9   2024    Financial Advisory      Q4  14875.525907      -93035.498970   
10  2024                Retail      Q1  14141.125619       41587.408274   
11  2024     Corporate Banking      Q1  14163.131695     -260625.088723   
12  2024    Investment Banking      Q1  16256.022330       68033.250982   
13  2024             Insurance      Q1  17141.349440      -50113.395498   
14  2024     Wealth Management      Q1  13916.381944     -132879.665828   
15  2024      Asset Management      Q1  14874.367779     -134068.145535   
16  2024       Private Banking      Q1  14112.166923      -48717.104563   
17  2024  Credit Card Services      Q1  14403.780221     -165808.715621   
18  2024      Mortgage Lending      Q1  14290.153021      -15544.565281   
19  2024    Financial Advisory      Q1  14875.525907      -93035.498970   
20  2024                Retail      Q2  14141.125619       41587.408274   
21  2024     Corporate Banking      Q2  14163.131695     -260625.088723   
22  2024    Investment Banking      Q2  16256.022330       68033.250982   
23  2024             Insurance      Q2  17141.349440      -50113.395498   
24  2024     Wealth Management      Q2  13916.381944     -132879.665828   
25  2024      Asset Management      Q2  14874.367779     -134068.145535   
26  2024       Private Banking      Q2  14112.166923      -48717.104563   
27  2024  Credit Card Services      Q2  14403.780221     -165808.715621   
28  2024      Mortgage Lending      Q2  14290.153021      -15544.565281   
29  2024    Financial Advisory      Q2  14875.525907      -93035.498970   
30  2024                Retail      Q3  14141.125619       41587.408274   
31  2024     Corporate Banking      Q3  14163.131695     -260625.088723   
32  2024    Investment Banking      Q3  16256.022330       68033.250982   
33  2024             Insurance      Q3  17141.349440      -50113.395498   
34  2024     Wealth Management      Q3  13916.381944     -132879.665828   
35  2024      Asset Management      Q3  14874.367779     -134068.145535   
36  2024       Private Banking      Q3  14112.166923      -48717.104563   
37  2024  Credit Card Services      Q3  14403.780221     -165808.715621   
38  2024      Mortgage Lending      Q3  14290.153021      -15544.565281   
39  2024    Financial Advisory      Q3  14875.525907      -93035.498970   
40  2024                Retail      Q4  14141.125619       41587.408274   
41  2024     Corporate Banking      Q4  14163.131695     -260625.088723   
42  2024    Investment Banking      Q4  16256.022330       68033.250982   
43  2024             Insurance      Q4  17141.349440      -50113.395498   
44  2024     Wealth Management      Q4  13916.381944     -132879.665828   
45  2024      Asset Management      Q4  14874.367779     -134068.145535   
46  2024       Private Banking      Q4  14112.166923      -48717.104563   
47  2024  Credit Card Services      Q4  14403.780221     -165808.715621   
48  2024      Mortgage Lending      Q4  14290.153021      -15544.565281   
49  2024    Financial Advisory      Q4  14875.525907      -93035.498970   

    Event_Count  Predicted_Event_Count  
0           237                    237  
1           207                    207  
2           208                    208  
3           232                    232  
4           224                    224  
5           243                    243  
6           186                    186  
7           216                    216  
8           228                    228  
9           256                    256  
10          237                    237  
11          207                    207  
12          208                    208  
13          232                    232  
14          224                    224  
15          243                    243  
16          186                    186  
17          216                    216  
18          228                    228  
19          256                    256  
20          237                    237  
21          207                    207  
22          208                    208  
23          232                    232  
24          224                    224  
25          243                    243  
26          186                    186  
27          216                    216  
28          228                    228  
29          256                    256  
30          237                    237  
31          207                    207  
32          208                    208  
33          232                    232  
34          224                    224  
35          243                    243  
36          186                    186  
37          216                    216  
38          228                    228  
39          256                    256  
40          237                    237  
41          207                    207  
42          208                    208  
43          232                    232  
44          224                    224  
45          243                    243  
46          186                    186  
47          216                    216  
48          228                    228  
49          256                    256  
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy import stats
import itertools

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year column
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.to_period('Q')

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Quarter, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Quarter', 'Business Line', 'Event Type', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Quarter', 'Business Line', 'Event Type'], how='left')

# Filter data for Q4 2024
q4_2024_data = final_result[(final_result['Year'] == 2024) & (final_result['Quarter'] == '2024Q4')]

# Function to predict event count and total loss for Q4 2024
def predict_q4_2024(year, quarter, business_line):
    # Retrieve historical data for Q4 and business line
    subset = final_result[(final_result['Year'] == year) & 
                          (final_result['Quarter'] == quarter) &
                          (final_result['Business Line'] == business_line)]
    
    if not subset.empty:
        # Retrieve historical VaR, Event Count, and Total Loss
        var_99_9 = subset['VaR (99.9%)'].values[0]
        total_loss = subset['Total_Loss_Amount'].values[0]
        event_count = subset['Event_Count'].values[0]
        
        # Predict the event count for the future period (using historical mean)
        predicted_event_count = event_count  # Assuming future counts are similar to the past
        predicted_total_loss = total_loss  # Assuming future loss amounts remain similar
        
        return [year, quarter, business_line, var_99_9, total_loss, event_count, predicted_event_count, predicted_total_loss]
    return None

# Generate future predictions for Q4 2024
future_data = []
for business_line in business_lines:
    prediction = predict_q4_2024(2024, '2024Q4', business_line)
    if prediction:
        future_data.append(prediction)

# Convert future data to DataFrame
future_df = pd.DataFrame(future_data, columns=['Year', 'Quarter', 'Business Line', 'PredVaR', 'Total_Loss_Amount', 'Event_Count', 'Predicted_Event_Count', 'Predicted_Total_Loss'])

# Display filtered predictions for Q4 2024
print(future_df)

# Optionally, save results to a CSV
future_df.to_csv('predicted_var_and_event_count_q4_2024.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: invalid value encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6423: RuntimeWarning: invalid value encountered in log
  lambda x, s: (-np.log(x)**2 / (2 * s**2)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6424: RuntimeWarning: invalid value encountered in log
  - np.log(s * x * np.sqrt(2 * np.pi))),
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: divide by zero encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
   Year Quarter         Business Line       PredVaR  Total_Loss_Amount  \
0  2024  2024Q4                Retail  2.212802e+21        1324.070840   
1  2024  2024Q4     Corporate Banking  8.728747e+18      -10310.228826   
2  2024  2024Q4    Investment Banking           NaN       13947.627441   
3  2024  2024Q4             Insurance  1.308520e+20        1786.074820   
4  2024  2024Q4     Wealth Management  1.651654e+20        1296.248746   
5  2024  2024Q4      Asset Management  8.393238e+21      -11165.158935   
6  2024  2024Q4       Private Banking  3.056358e+07       -3984.695186   
7  2024  2024Q4  Credit Card Services  1.981584e+20      -18497.870984   
8  2024  2024Q4      Mortgage Lending  2.151886e+20        1672.644812   
9  2024  2024Q4    Financial Advisory  3.139647e+21      -13711.032692   

   Event_Count  Predicted_Event_Count  Predicted_Total_Loss  
0            5                      5           1324.070840  
1            7                      7         -10310.228826  
2            3                      3          13947.627441  
3            6                      6           1786.074820  
4            4                      4           1296.248746  
5            4                      4         -11165.158935  
6            1                      1          -3984.695186  
7            7                      7         -18497.870984  
8            4                      4           1672.644812  
9            4                      4         -13711.032692  
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1129957418.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).apply(
import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime, timedelta

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year and Quarter column
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.to_period('Q')

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Quarter, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Quarter', 'Business Line', 'Event Type', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Quarter', 'Business Line', 'Event Type'], how='left')

# Predict for 2025 for all quarters
predictions = []

# Function to predict event count and total loss for 2025 using historical data
def predict_for_2025(business_line, quarter, final_result):
    # Filter data for 2024 for the given business line and quarter
    subset_2024 = final_result[(final_result['Year'] == 2024) & (final_result['Business Line'] == business_line) & (final_result['Quarter'] == quarter)]
    
    if not subset_2024.empty:
        # Use 2024 data for prediction
        predicted_event_count = subset_2024['Event_Count'].values[0]
        predicted_total_loss = subset_2024['Total_Loss_Amount'].values[0]
        predicted_var = subset_2024['VaR (99.9%)'].values[0]
        
        # For simplicity, predict the same values for 2025
        prediction = [2025, quarter, business_line, predicted_var, predicted_total_loss, predicted_event_count]
        predictions.append(prediction)

# Generate predictions for all quarters in 2025
for business_line in business_lines:
    for quarter in ['2025Q1', '2025Q2', '2025Q3', '2025Q4']:
        predict_for_2025(business_line, quarter, final_result)

# Create DataFrame for predictions
predictions_df = pd.DataFrame(predictions, columns=['Year', 'Quarter', 'Business Line', 'PredVaR', 'Predicted_Total_Loss', 'Predicted_Event_Count'])

# Display predictions for 2025
print(predictions_df)

# Optionally, save to a CSV file
predictions_df.to_csv('predictions_2025.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: divide by zero encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6423: RuntimeWarning: invalid value encountered in log
  lambda x, s: (-np.log(x)**2 / (2 * s**2)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6424: RuntimeWarning: invalid value encountered in log
  - np.log(s * x * np.sqrt(2 * np.pi))),
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: invalid value encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
Empty DataFrame
Columns: [Year, Quarter, Business Line, PredVaR, Predicted_Total_Loss, Predicted_Event_Count]
Index: []
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1855192767.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).apply(
import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime, timedelta

# Function to generate random dates
def random_dates(start, end, n=10):
    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]

# Parameters
num_records = 10000
start_date = datetime.now() - timedelta(days=4*365)
end_date = datetime.now()

# Expanded categories
business_lines = [
    "Retail", "Corporate Banking", "Investment Banking", "Insurance",
    "Wealth Management", "Asset Management", "Private Banking",
    "Credit Card Services", "Mortgage Lending", "Financial Advisory"
]

event_types = [
    "Fraud", "System Failure", "Theft", "Compliance", "Natural Disaster",
    "Cyber Attack", "Market Risk", "Operational Error", "Vendor Risk", "Regulatory Violation"
]

# Generate data
data = {
    "Date": random_dates(start_date, end_date, num_records),
    "Unique Event ID": [f"EID{str(i).zfill(5)}" for i in range(num_records)],
    "Event Type": np.random.choice(event_types, num_records),
    "Business Line": np.random.choice(business_lines, num_records),
    "Event Description": np.random.choice(
        [
            "Unauthorized transaction", "Server downtime", "Lost assets", 
            "Regulatory fines", "Data breach", "Network failure", 
            "Inadequate compliance", "Financial misstatement", 
            "Supplier issues", "Internal fraud"
        ],
        num_records
    ),
    "Net Loss Amount": np.random.choice(
        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + 
        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],
        num_records
    )
}

# Create DataFrame
df = pd.DataFrame(data)

# Add a Year and Quarter column
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.to_period('Q')

# Function to fit distribution and calculate VaR for the specified confidence level
def fit_distribution_and_calculate_var(data):
    confidence = 0.999  # Set to 99.9%
    var_result = {}
    # Filter only negative net loss amounts for fitting
    negative_losses = data[data['Net Loss Amount'] < 0]
    
    if negative_losses.empty:
        return {confidence: np.nan}  # Return NaN if no data

    # Fit a log-normal distribution to the net loss amounts (inverted for fitting)
    shape, loc, scale = stats.lognorm.fit(negative_losses['Net Loss Amount'] * -1)  
    # Calculate VaR for the specified confidence level
    var = stats.lognorm.ppf(confidence, shape, loc=loc, scale=scale)
    var_result[confidence] = var
    return var_result

# Group by Year, Quarter, Business Line, and Event Type, then apply the VaR calculation
result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).apply(
    lambda x: fit_distribution_and_calculate_var(x)[0.999]
).reset_index()

result.columns = ['Year', 'Quarter', 'Business Line', 'Event Type', 'VaR (99.9%)']

# Calculate total loss amount and count of event IDs for each group
agg_result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count')
).reset_index()

# Merge VaR results with aggregated results
final_result = pd.merge(agg_result, result, on=['Year', 'Quarter', 'Business Line', 'Event Type'], how='left')

# Check if the data for 2024 is available
print("Checking data availability for 2024...")
print(final_result[final_result['Year'] == 2024].head())

# Predict for 2025 for all quarters
predictions = []

# Function to predict event count and total loss for 2025 using historical data
def predict_for_2025(business_line, quarter, final_result):
    # Filter data for 2024 for the given business line and quarter
    subset_2024 = final_result[(final_result['Year'] == 2024) & (final_result['Business Line'] == business_line) & (final_result['Quarter'] == quarter)]
    
    if not subset_2024.empty:
        # Use 2024 data for prediction
        predicted_event_count = subset_2024['Event_Count'].values[0]
        predicted_total_loss = subset_2024['Total_Loss_Amount'].values[0]
        predicted_var = subset_2024['VaR (99.9%)'].values[0]
        
        # For simplicity, predict the same values for 2025
        prediction = [2025, quarter, business_line, predicted_var, predicted_total_loss, predicted_event_count]
        predictions.append(prediction)
    else:
        print(f"No data found for {business_line} in {quarter} of 2024.")  # Debugging statement

# Generate predictions for all quarters in 2025
for business_line in business_lines:
    for quarter in ['2025Q1', '2025Q2', '2025Q3', '2025Q4']:
        predict_for_2025(business_line, quarter, final_result)

# Create DataFrame for predictions
predictions_df = pd.DataFrame(predictions, columns=['Year', 'Quarter', 'Business Line', 'PredVaR', 'Predicted_Total_Loss', 'Predicted_Event_Count'])

# Display predictions for 2025
print("Predictions for 2025:")
print(predictions_df)

# Optionally, save to a CSV file
predictions_df.to_csv('predictions_2025.csv', index=False)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: invalid value encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6423: RuntimeWarning: invalid value encountered in log
  lambda x, s: (-np.log(x)**2 / (2 * s**2)
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6424: RuntimeWarning: invalid value encountered in log
  - np.log(s * x * np.sqrt(2 * np.pi))),
C:\Users\Himanshu Singh\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\stats\_continuous_distns.py:6554: RuntimeWarning: divide by zero encountered in divide
  return np.sum((1 + np.log(shifted/scale)/shape**2)/shifted)
Checking data availability for 2024...
      Year Quarter     Business Line        Event Type  Total_Loss_Amount  \
1295  2024  2024Q1  Asset Management        Compliance       19661.236536   
1296  2024  2024Q1  Asset Management      Cyber Attack      -13525.298459   
1297  2024  2024Q1  Asset Management             Fraud       -6314.757101   
1298  2024  2024Q1  Asset Management       Market Risk       23006.280192   
1299  2024  2024Q1  Asset Management  Natural Disaster      -24646.346847   

      Event_Count   VaR (99.9%)  
1295            5  2.702700e+03  
1296            2  4.728944e+19  
1297            1  6.314757e+03  
1298            4           NaN  
1299            5  2.170474e+22  
No data found for Retail in 2025Q1 of 2024.
No data found for Retail in 2025Q2 of 2024.
No data found for Retail in 2025Q3 of 2024.
No data found for Retail in 2025Q4 of 2024.
No data found for Corporate Banking in 2025Q1 of 2024.
No data found for Corporate Banking in 2025Q2 of 2024.
No data found for Corporate Banking in 2025Q3 of 2024.
No data found for Corporate Banking in 2025Q4 of 2024.
No data found for Investment Banking in 2025Q1 of 2024.
No data found for Investment Banking in 2025Q2 of 2024.
No data found for Investment Banking in 2025Q3 of 2024.
No data found for Investment Banking in 2025Q4 of 2024.
No data found for Insurance in 2025Q1 of 2024.
No data found for Insurance in 2025Q2 of 2024.
No data found for Insurance in 2025Q3 of 2024.
No data found for Insurance in 2025Q4 of 2024.
No data found for Wealth Management in 2025Q1 of 2024.
No data found for Wealth Management in 2025Q2 of 2024.
No data found for Wealth Management in 2025Q3 of 2024.
No data found for Wealth Management in 2025Q4 of 2024.
No data found for Asset Management in 2025Q1 of 2024.
No data found for Asset Management in 2025Q2 of 2024.
No data found for Asset Management in 2025Q3 of 2024.
No data found for Asset Management in 2025Q4 of 2024.
No data found for Private Banking in 2025Q1 of 2024.
No data found for Private Banking in 2025Q2 of 2024.
No data found for Private Banking in 2025Q3 of 2024.
No data found for Private Banking in 2025Q4 of 2024.
No data found for Credit Card Services in 2025Q1 of 2024.
No data found for Credit Card Services in 2025Q2 of 2024.
No data found for Credit Card Services in 2025Q3 of 2024.
No data found for Credit Card Services in 2025Q4 of 2024.
No data found for Mortgage Lending in 2025Q1 of 2024.
No data found for Mortgage Lending in 2025Q2 of 2024.
No data found for Mortgage Lending in 2025Q3 of 2024.
No data found for Mortgage Lending in 2025Q4 of 2024.
No data found for Financial Advisory in 2025Q1 of 2024.
No data found for Financial Advisory in 2025Q2 of 2024.
No data found for Financial Advisory in 2025Q3 of 2024.
No data found for Financial Advisory in 2025Q4 of 2024.
Predictions for 2025:
Empty DataFrame
Columns: [Year, Quarter, Business Line, PredVaR, Predicted_Total_Loss, Predicted_Event_Count]
Index: []
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\1816655037.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  result = df.groupby(['Year', 'Quarter', 'Business Line', 'Event Type']).apply(
import pandas as pd
import numpy as np
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Assuming 'final_result' has the necessary data aggregated by quarter (similar to your earlier dataset)

# Add a Quarter column
df['Quarter'] = df['Date'].dt.to_period('Q')

# Grouping by Year, Quarter, Business Line, and Event Type to calculate totals
quarterly_agg = df.groupby(['Year', 'Quarter', 'Business Line']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count'),
    VaR_99_9=('Net Loss Amount', lambda x: fit_distribution_and_calculate_var(x)[0.999])
).reset_index()

# Now, we'll proceed with time series predictions (for simplicity, we use Exponential Smoothing)
predictions = []

for business_line in quarterly_agg['Business Line'].unique():
    for event_type in quarterly_agg['Event Type'].unique():
        quarterly_data = quarterly_agg[(quarterly_agg['Business Line'] == business_line) & 
                                       (quarterly_agg['Event Type'] == event_type)]
        
        # Create a time series for Event Count, VaR, and Total Loss Amount
        ts_event_count = quarterly_data['Event_Count'].values
        ts_var = quarterly_data['VaR_99_9'].values
        ts_loss = quarterly_data['Total_Loss_Amount'].values
        
        # Exponential Smoothing for predictions (simple model)
        model_event_count = ExponentialSmoothing(ts_event_count, trend='add', seasonal='add', seasonal_periods=4).fit()
        model_var = ExponentialSmoothing(ts_var, trend='add', seasonal='add', seasonal_periods=4).fit()
        model_loss = ExponentialSmoothing(ts_loss, trend='add', seasonal='add', seasonal_periods=4).fit()
        
        # Predict for 2025 (all quarters)
        pred_event_count = model_event_count.forecast(4)  # Predict for 4 quarters
        pred_var = model_var.forecast(4)
        pred_loss = model_loss.forecast(4)
        
        # Add predictions to list
        for i, quarter in enumerate(['Q1', 'Q2', 'Q3', 'Q4']):
            predictions.append({
                'Year': 2025,
                'Quarter': quarter,
                'Business Line': business_line,
                'Predicted_Event_Count': pred_event_count[i],
                'PredVaR': pred_var[i],
                'Predicted_Total_Loss': pred_loss[i]
            })

# Convert predictions to DataFrame
predicted_df = pd.DataFrame(predictions)

# Display predictions for 2025 (all quarters)
print(predicted_df)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\indexes\base.py:3805, in Index.get_loc(self, key)
   3804 try:
-> 3805     return self._engine.get_loc(casted_key)
   3806 except KeyError as err:

File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()

File index.pyx:175, in pandas._libs.index.IndexEngine.get_loc()

File pandas\\_libs\\index_class_helper.pxi:70, in pandas._libs.index.Int64Engine._check_type()

KeyError: 'Net Loss Amount'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
Cell In[45], line 11
      8 df['Quarter'] = df['Date'].dt.to_period('Q')
     10 # Grouping by Year, Quarter, Business Line, and Event Type to calculate totals
---> 11 quarterly_agg = df.groupby(['Year', 'Quarter', 'Business Line']).agg(
     12     Total_Loss_Amount=('Net Loss Amount', 'sum'),
     13     Event_Count=('Unique Event ID', 'count'),
     14     VaR_99_9=('Net Loss Amount', lambda x: fit_distribution_and_calculate_var(x)[0.999])
     15 ).reset_index()
     17 # Now, we'll proceed with time series predictions (for simplicity, we use Exponential Smoothing)
     18 predictions = []

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:1432, in DataFrameGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
   1429     kwargs["engine_kwargs"] = engine_kwargs
   1431 op = GroupByApply(self, func, args=args, kwargs=kwargs)
-> 1432 result = op.agg()
   1433 if not is_dict_like(func) and result is not None:
   1434     # GH #52849
   1435     if not self.as_index and is_list_like(func):

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:190, in Apply.agg(self)
    187     return self.apply_str()
    189 if is_dict_like(func):
--> 190     return self.agg_dict_like()
    191 elif is_list_like(func):
    192     # we require a list, but not a 'str'
    193     return self.agg_list_like()

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:423, in Apply.agg_dict_like(self)
    415 def agg_dict_like(self) -> DataFrame | Series:
    416     """
    417     Compute aggregation in the case of a dict-like argument.
    418 
   (...)
    421     Result of aggregation.
    422     """
--> 423     return self.agg_or_apply_dict_like(op_name="agg")

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:1608, in GroupByApply.agg_or_apply_dict_like(self, op_name)
   1603     kwargs.update({"engine": engine, "engine_kwargs": engine_kwargs})
   1605 with com.temp_setattr(
   1606     obj, "as_index", True, condition=hasattr(obj, "as_index")
   1607 ):
-> 1608     result_index, result_data = self.compute_dict_like(
   1609         op_name, selected_obj, selection, kwargs
   1610     )
   1611 result = self.wrap_results_dict_like(selected_obj, result_index, result_data)
   1612 return result

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\apply.py:497, in Apply.compute_dict_like(self, op_name, selected_obj, selection, kwargs)
    493         results += key_data
    494 else:
    495     # key used for column selection and output
    496     results = [
--> 497         getattr(obj._gotitem(key, ndim=1), op_name)(how, **kwargs)
    498         for key, how in func.items()
    499     ]
    500     keys = list(func.keys())
    502 return keys, results

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:257, in SeriesGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    255 kwargs["engine"] = engine
    256 kwargs["engine_kwargs"] = engine_kwargs
--> 257 ret = self._aggregate_multiple_funcs(func, *args, **kwargs)
    258 if relabeling:
    259     # columns is not narrowed by mypy from relabeling flag
    260     assert columns is not None  # for mypy

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:362, in SeriesGroupBy._aggregate_multiple_funcs(self, arg, *args, **kwargs)
    360     for idx, (name, func) in enumerate(arg):
    361         key = base.OutputKey(label=name, position=idx)
--> 362         results[key] = self.aggregate(func, *args, **kwargs)
    364 if any(isinstance(x, DataFrame) for x in results.values()):
    365     from pandas import concat

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:291, in SeriesGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    283     return self.obj._constructor(
    284         [],
    285         name=self.obj.name,
    286         index=self._grouper.result_index,
    287         dtype=obj.dtype,
    288     )
    290 if self._grouper.nkeys > 1:
--> 291     return self._python_agg_general(func, *args, **kwargs)
    293 try:
    294     return self._python_agg_general(func, *args, **kwargs)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:327, in SeriesGroupBy._python_agg_general(self, func, *args, **kwargs)
    324 f = lambda x: func(x, *args, **kwargs)
    326 obj = self._obj_with_exclusions
--> 327 result = self._grouper.agg_series(obj, f)
    328 res = obj._constructor(result, name=obj.name)
    329 return self._wrap_aggregated_output(res)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\ops.py:864, in BaseGrouper.agg_series(self, obj, func, preserve_dtype)
    857 if not isinstance(obj._values, np.ndarray):
    858     # we can preserve a little bit more aggressively with EA dtype
    859     #  because maybe_cast_pointwise_result will do a try/except
    860     #  with _from_sequence.  NB we are assuming here that _from_sequence
    861     #  is sufficiently strict that it casts appropriately.
    862     preserve_dtype = True
--> 864 result = self._aggregate_series_pure_python(obj, func)
    866 npvalues = lib.maybe_convert_objects(result, try_float=False)
    867 if preserve_dtype:

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\ops.py:885, in BaseGrouper._aggregate_series_pure_python(self, obj, func)
    882 splitter = self._get_splitter(obj, axis=0)
    884 for i, group in enumerate(splitter):
--> 885     res = func(group)
    886     res = extract_result(res)
    888     if not initialized:
    889         # We only do this validation on the first iteration

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\groupby\generic.py:324, in SeriesGroupBy._python_agg_general.<locals>.<lambda>(x)
    322     alias = com._builtin_table_alias[func]
    323     warn_alias_replacement(self, orig_func, alias)
--> 324 f = lambda x: func(x, *args, **kwargs)
    326 obj = self._obj_with_exclusions
    327 result = self._grouper.agg_series(obj, f)

Cell In[45], line 14, in <lambda>(x)
      8 df['Quarter'] = df['Date'].dt.to_period('Q')
     10 # Grouping by Year, Quarter, Business Line, and Event Type to calculate totals
     11 quarterly_agg = df.groupby(['Year', 'Quarter', 'Business Line']).agg(
     12     Total_Loss_Amount=('Net Loss Amount', 'sum'),
     13     Event_Count=('Unique Event ID', 'count'),
---> 14     VaR_99_9=('Net Loss Amount', lambda x: fit_distribution_and_calculate_var(x)[0.999])
     15 ).reset_index()
     17 # Now, we'll proceed with time series predictions (for simplicity, we use Exponential Smoothing)
     18 predictions = []

Cell In[42], line 60, in fit_distribution_and_calculate_var(data)
     58 var_result = {}
     59 # Filter only negative net loss amounts for fitting
---> 60 negative_losses = data[data['Net Loss Amount'] < 0]
     62 if negative_losses.empty:
     63     return {confidence: np.nan}  # Return NaN if no data

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\series.py:1121, in Series.__getitem__(self, key)
   1118     return self._values[key]
   1120 elif key_is_scalar:
-> 1121     return self._get_value(key)
   1123 # Convert generator to list before going through hashable part
   1124 # (We will iterate through the generator there to check for slices)
   1125 if is_iterator(key):

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\series.py:1237, in Series._get_value(self, label, takeable)
   1234     return self._values[label]
   1236 # Similar to Index.get_value, but we do not fall back to positional
-> 1237 loc = self.index.get_loc(label)
   1239 if is_integer(loc):
   1240     return self._values[loc]

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\indexes\base.py:3812, in Index.get_loc(self, key)
   3807     if isinstance(casted_key, slice) or (
   3808         isinstance(casted_key, abc.Iterable)
   3809         and any(isinstance(x, slice) for x in casted_key)
   3810     ):
   3811         raise InvalidIndexError(key)
-> 3812     raise KeyError(key) from err
   3813 except TypeError:
   3814     # If we have a listlike key, _check_indexing_error will raise
   3815     #  InvalidIndexError. Otherwise we fall through and re-raise
   3816     #  the TypeError.
   3817     self._check_indexing_error(key)

KeyError: 'Net Loss Amount'
import numpy as np
import pandas as pd
from scipy.stats import lognorm
import matplotlib.pyplot as plt

# Load your dataframe (assuming df is already loaded with relevant columns)
# Example: df = pd.read_csv('your_data.csv')

# Ensure the 'Date' column is of datetime type
df['Date'] = pd.to_datetime(df['Date'])

# Add 'Year' and 'Quarter' columns
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.to_period('Q')

# Check for missing or incorrect column names
print(df.columns)  # Ensure 'Net Loss Amount' exists in your data

# Define function to fit Lognormal and calculate VaR
def fit_lognormal_and_calculate_var(data, confidence=0.999):
    """
    Fits a Lognormal distribution to the data and calculates the specified VaR at the given confidence level.
    """
    # Remove any rows with non-positive loss amounts
    data = data[data > 0]
    
    if len(data) == 0:
        return np.nan
    
    # Fit Lognormal distribution to the data (assume loss amounts are positive)
    shape, loc, scale = lognorm.fit(data, floc=0)
    
    # Calculate the Value at Risk at the given confidence level (VaR at 99.9% for example)
    var_value = lognorm.ppf(1 - confidence, shape, loc, scale)
    return var_value

# Group by Year, Quarter, Business Line, and Event Type to calculate totals
quarterly_agg = df.groupby(['Year', 'Quarter', 'Business Line']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count'),
    VaR_99_9=('Net Loss Amount', lambda x: fit_lognormal_and_calculate_var(x, 0.999))
).reset_index()

# Function to predict VaR and Event Count for 2025 (next year)
def predict_var_and_event_count_for_2025(quarterly_agg):
    """
    Predicts VaR and Event Count for the year 2025 using historical data and Lognormal fitting.
    """
    # Filter data for 2024 to predict for 2025
    data_2024 = quarterly_agg[quarterly_agg['Year'] == 2024]
    
    # Generate predictions for each quarter of 2025
    predictions_2025 = []
    for quarter in ['2025Q1', '2025Q2', '2025Q3', '2025Q4']:
        for business_line in data_2024['Business Line'].unique():
            # Get the latest data for the business line
            business_line_data = data_2024[data_2024['Business Line'] == business_line]
            
            # Calculate the predicted VaR for 2025
            var_2025 = business_line_data['VaR_99_9'].values[0]
            
            # Predict the Event Count for 2025 as the average from 2024
            avg_event_count_2024 = business_line_data['Event_Count'].mean()
            
            predictions_2025.append({
                'Year': 2025,
                'Quarter': quarter,
                'Business Line': business_line,
                'Predicted_VaR_99_9': var_2025,
                'Predicted_Event_Count': avg_event_count_2024
            })
    
    return pd.DataFrame(predictions_2025)

# Predict VaR and Event Count for 2025
predictions_2025 = predict_var_and_event_count_for_2025(quarterly_agg)

# Combine the actual 2024 data and the predictions for 2025
final_agg = pd.concat([quarterly_agg, predictions_2025], ignore_index=True)

# Display the results
print(final_agg)

# Optionally, save the predictions to a CSV file
final_agg.to_csv('predicted_var_event_count_2025.csv', index=False)
Index(['Date', 'Unique Event ID', 'Event Type', 'Business Line',
       'Event Description', 'Net Loss Amount', 'Year', 'Quarter'],
      dtype='object')
     Year Quarter         Business Line  Total_Loss_Amount  Event_Count  \
0    2020  2020Q4      Asset Management      -35398.023607         26.0   
1    2020  2020Q4     Corporate Banking      -29876.943111         31.0   
2    2020  2020Q4  Credit Card Services      -23300.982441         34.0   
3    2020  2020Q4    Financial Advisory        4970.124313         29.0   
4    2020  2020Q4             Insurance      -59994.646320         23.0   
..    ...     ...                   ...                ...          ...   
205  2025  2025Q4    Investment Banking                NaN          NaN   
206  2025  2025Q4      Mortgage Lending                NaN          NaN   
207  2025  2025Q4       Private Banking                NaN          NaN   
208  2025  2025Q4                Retail                NaN          NaN   
209  2025  2025Q4     Wealth Management                NaN          NaN   

        VaR_99_9  Predicted_VaR_99_9  Predicted_Event_Count  
0    1277.437682                 NaN                    NaN  
1     162.874207                 NaN                    NaN  
2     196.365951                 NaN                    NaN  
3     279.303354                 NaN                    NaN  
4      43.402228                 NaN                    NaN  
..           ...                 ...                    ...  
205          NaN          698.911233                   55.0  
206          NaN          117.949306                   57.0  
207          NaN          204.648688                   58.0  
208          NaN          123.762345                   55.5  
209          NaN          118.322741                   52.5  

[210 rows x 8 columns]
final_agg
Year	Quarter	Business Line	Total_Loss_Amount	Event_Count	VaR_99_9	Predicted_VaR_99_9	Predicted_Event_Count
0	2020	2020Q4	Asset Management	-35398.023607	26.0	1277.437682	NaN	NaN
1	2020	2020Q4	Corporate Banking	-29876.943111	31.0	162.874207	NaN	NaN
2	2020	2020Q4	Credit Card Services	-23300.982441	34.0	196.365951	NaN	NaN
3	2020	2020Q4	Financial Advisory	4970.124313	29.0	279.303354	NaN	NaN
4	2020	2020Q4	Insurance	-59994.646320	23.0	43.402228	NaN	NaN
...	...	...	...	...	...	...	...	...
205	2025	2025Q4	Investment Banking	NaN	NaN	NaN	698.911233	55.0
206	2025	2025Q4	Mortgage Lending	NaN	NaN	NaN	117.949306	57.0
207	2025	2025Q4	Private Banking	NaN	NaN	NaN	204.648688	58.0
208	2025	2025Q4	Retail	NaN	NaN	NaN	123.762345	55.5
209	2025	2025Q4	Wealth Management	NaN	NaN	NaN	118.322741	52.5
210 rows Ã— 8 columns

import numpy as np
import pandas as pd
from scipy.stats import lognorm
import matplotlib.pyplot as plt

# Load your dataframe (assuming df is already loaded with relevant columns)
# Example: df = pd.read_csv('your_data.csv')

# Ensure the 'Date' column is of datetime type
df['Date'] = pd.to_datetime(df['Date'])

# Add 'Year' and 'Quarter' columns
df['Year'] = df['Date'].dt.year
df['Quarter'] = df['Date'].dt.to_period('Q')

# Check for missing or incorrect column names
print(df.columns)  # Ensure 'Net Loss Amount' exists in your data

# Define function to fit Lognormal and calculate VaR
def fit_lognormal_and_calculate_var(data, confidence=0.999):
    """
    Fits a Lognormal distribution to the data and calculates the specified VaR at the given confidence level.
    """
    # Remove any rows with non-positive loss amounts
    data = data[data > 0]
    
    if len(data) == 0:
        return np.nan
    
    # Fit Lognormal distribution to the data (assume loss amounts are positive)
    shape, loc, scale = lognorm.fit(data, floc=0)
    
    # Calculate the Value at Risk at the given confidence level (VaR at 99.9% for example)
    var_value = lognorm.ppf(1 - confidence, shape, loc, scale)
    return var_value

# Group by Year, Quarter, Business Line, and Event Type to calculate totals
quarterly_agg = df.groupby(['Year', 'Quarter', 'Business Line']).agg(
    Total_Loss_Amount=('Net Loss Amount', 'sum'),
    Event_Count=('Unique Event ID', 'count'),
    VaR_99_9=('Net Loss Amount', lambda x: fit_lognormal_and_calculate_var(x, 0.999))
).reset_index()

# Function to predict VaR and Event Count for specified quarters (2024Q4 and 2025 quarters)
def predict_var_and_event_count_for_specified_quarters(quarterly_agg, quarters_to_predict):
    """
    Predicts VaR and Event Count for specified quarters.
    """
    # Filter data for the most recent year (2024)
    data_2024 = quarterly_agg[quarterly_agg['Year'] == 2024]
    
    # Generate predictions for each specified quarter
    predictions = []
    for quarter in quarters_to_predict:
        for business_line in data_2024['Business Line'].unique():
            # Get the latest data for the business line
            business_line_data = data_2024[data_2024['Business Line'] == business_line]
            
            # Calculate the predicted VaR for the quarter
            var_2025 = business_line_data['VaR_99_9'].values[0]
            
            # Predict the Event Count for the quarter as the average from 2024
            avg_event_count_2024 = business_line_data['Event_Count'].mean()
            
            predictions.append({
                'Year': quarter[:4],
                'Quarter': quarter,
                'Business Line': business_line,
                'Predicted_VaR_99_9': var_2025,
                'Predicted_Event_Count': avg_event_count_2024
            })
    
    return pd.DataFrame(predictions)

# Specify the quarters to predict: 2024Q4, 2025Q1, 2025Q2, 2025Q3, 2025Q5
quarters_to_predict = ['2024Q4', '2025Q1', '2025Q2', '2025Q3', '2025Q5']

# Predict VaR and Event Count for specified quarters
predictions = predict_var_and_event_count_for_specified_quarters(quarterly_agg, quarters_to_predict)

# Combine the actual 2024 data and the predictions for specified quarters
final_agg = pd.concat([quarterly_agg, predictions], ignore_index=True)

# Display the results
print(final_agg)

# Optionally, save the predictions to a CSV file
final_agg.to_csv('predicted_var_event_count_specified_quarters.csv', index=False)
Index(['Date', 'Unique Event ID', 'Event Type', 'Business Line',
       'Event Description', 'Net Loss Amount', 'Year', 'Quarter'],
      dtype='object')
     Year Quarter         Business Line  Total_Loss_Amount  Event_Count  \
0    2020  2020Q4      Asset Management      -35398.023607         26.0   
1    2020  2020Q4     Corporate Banking      -29876.943111         31.0   
2    2020  2020Q4  Credit Card Services      -23300.982441         34.0   
3    2020  2020Q4    Financial Advisory        4970.124313         29.0   
4    2020  2020Q4             Insurance      -59994.646320         23.0   
..    ...     ...                   ...                ...          ...   
215  2025  2025Q5    Investment Banking                NaN          NaN   
216  2025  2025Q5      Mortgage Lending                NaN          NaN   
217  2025  2025Q5       Private Banking                NaN          NaN   
218  2025  2025Q5                Retail                NaN          NaN   
219  2025  2025Q5     Wealth Management                NaN          NaN   

        VaR_99_9  Predicted_VaR_99_9  Predicted_Event_Count  
0    1277.437682                 NaN                    NaN  
1     162.874207                 NaN                    NaN  
2     196.365951                 NaN                    NaN  
3     279.303354                 NaN                    NaN  
4      43.402228                 NaN                    NaN  
..           ...                 ...                    ...  
215          NaN          698.911233                   55.0  
216          NaN          117.949306                   57.0  
217          NaN          204.648688                   58.0  
218          NaN          123.762345                   55.5  
219          NaN          118.322741                   52.5  

[220 rows x 8 columns]
final_agg
Year	Quarter	Business Line	Total_Loss_Amount	Event_Count	VaR_99_9	Predicted_VaR_99_9	Predicted_Event_Count
0	2020	2020Q4	Asset Management	-35398.023607	26.0	1277.437682	NaN	NaN
1	2020	2020Q4	Corporate Banking	-29876.943111	31.0	162.874207	NaN	NaN
2	2020	2020Q4	Credit Card Services	-23300.982441	34.0	196.365951	NaN	NaN
3	2020	2020Q4	Financial Advisory	4970.124313	29.0	279.303354	NaN	NaN
4	2020	2020Q4	Insurance	-59994.646320	23.0	43.402228	NaN	NaN
...	...	...	...	...	...	...	...	...
215	2025	2025Q5	Investment Banking	NaN	NaN	NaN	698.911233	55.0
216	2025	2025Q5	Mortgage Lending	NaN	NaN	NaN	117.949306	57.0
217	2025	2025Q5	Private Banking	NaN	NaN	NaN	204.648688	58.0
218	2025	2025Q5	Retail	NaN	NaN	NaN	123.762345	55.5
219	2025	2025Q5	Wealth Management	NaN	NaN	NaN	118.322741	52.5
220 rows Ã— 8 columns

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the seaborn style
sns.set(style="whitegrid")

# Convert 'Year' and 'Quarter' to a string for easier plotting
final_agg['Quarter_Str'] = final_agg['Year'].astype(str) + ' ' + final_agg['Quarter'].astype(str)

# Plot Event Count over time
plt.figure(figsize=(14, 7))
plt.subplot(2, 1, 1)
sns.lineplot(x='Quarter_Str', y='Event_Count', hue='Business Line', data=final_agg, marker='o', palette='tab10')
plt.title('Event Count Over Time')
plt.xticks(rotation=45)
plt.xlabel('Quarter')
plt.ylabel('Event Count')

# Plot VaR (99.9%) over time
plt.subplot(2, 1, 2)
sns.lineplot(x='Quarter_Str', y='VaR_99_9', hue='Business Line', data=final_agg, marker='o', palette='tab10')
plt.title('VaR (99.9%) Over Time')
plt.xticks(rotation=45)
plt.xlabel('Quarter')
plt.ylabel('VaR (99.9%)')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

# Filter for Q4 2024
q4_2024_data = final_agg[final_agg['Quarter'] == '2024Q4']

# Plot Event Count for Q4 2024
plt.figure(figsize=(14, 7))
plt.subplot(2, 1, 1)
sns.barplot(x='Business Line', y='Event_Count', data=q4_2024_data, palette='tab10')
plt.title('Event Count for Q4 2024')
plt.xlabel('Business Line')
plt.ylabel('Event Count')

# Plot VaR (99.9%) for Q4 2024
plt.subplot(2, 1, 2)
sns.barplot(x='Business Line', y='VaR_99_9', data=q4_2024_data, palette='tab10')
plt.title('VaR (99.9%) for Q4 2024')
plt.xlabel('Business Line')
plt.ylabel('VaR (99.9%)')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2263360066.py:7: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x='Business Line', y='Event_Count', data=q4_2024_data, palette='tab10')
C:\Users\Himanshu Singh\AppData\Local\Temp\ipykernel_8368\2263360066.py:14: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x='Business Line', y='VaR_99_9', data=q4_2024_data, palette='tab10')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import poisson, lognorm

# Assuming the 'final_agg' DataFrame has columns: 'Business Line', 'Event Count', 'Total Loss Amount'

# Function to generate synthetic data based on frequency and severity
def monte_carlo_simulation(business_line, freq_mean, severity_params, num_simulations=10000, confidence_level=0.999):
    # Frequency: Poisson distribution (events per period)
    frequency_samples = poisson.rvs(mu=freq_mean, size=num_simulations)
    
    # Severity: Lognormal distribution (loss per event)
    severity_samples = lognorm.rvs(sigma=severity_params[0], scale=np.exp(severity_params[1]), size=num_simulations)
    
    # Calculate total losses per simulation
    total_losses = frequency_samples * severity_samples
    
    # Calculate VaR at the given confidence level
    var_at_confidence = np.percentile(total_losses, (1 - confidence_level) * 100)
    
    return frequency_samples, severity_samples, total_losses, var_at_confidence

# Function to aggregate data and apply Monte Carlo simulation for each business line
def simulate_lda(final_agg, num_simulations=10000, confidence_level=0.999):
    results = []
    
    # Loop over each business line
    for business_line in final_agg['Business Line'].unique():
        # Filter the data for the current business line
        business_data = final_agg[final_agg['Business Line'] == business_line]
        
        # Frequency: mean event count (Poisson)
        freq_mean = business_data['Event_Count'].mean()
        
        # Severity: Fit Lognormal distribution to 'Net Loss Amount' data
        severity_params = np.log(business_data['Total_Loss_Amount'][business_data['Total_Loss_Amount'] > 0]).agg(['std', 'mean'])
        
        # Apply Monte Carlo simulation
        freq_samples, sev_samples, total_losses, var_at_confidence = monte_carlo_simulation(
            business_line, freq_mean, severity_params, num_simulations, confidence_level)
        
        # Store the results
        results.append({
            'Business Line': business_line,
            'Simulated_Frequency': np.mean(freq_samples),
            'Simulated_Severity': np.mean(sev_samples),
            'Simulated_VaR_99_9': var_at_confidence
        })
    
    # Convert the results to a DataFrame
    results_df = pd.DataFrame(results)
    return results_df

# Run the simulation for LDA and Monte Carlo for 2024Q4
q4_2024_data = final_agg[final_agg['Quarter'] == '2024Q4']
lda_results = simulate_lda(q4_2024_data)

# Plot the results for Q4 2024
plt.figure(figsize=(14, 7))

# Plot the simulated VaR for each business line
sns.barplot(x='Business Line', y='Simulated_VaR_99_9', data=lda_results, palette='tab10')
plt.title('Simulated VaR (99.9%) for Q4 2024 Using Monte Carlo LDA')
plt.xlabel('Business Line')
plt.ylabel('Simulated VaR (99.9%)')

plt.tight_layout()
plt.show()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[54], line 58
